# typed: true

# DO NOT EDIT MANUALLY
# This is an autogenerated file for types exported from the `karafka-rdkafka` gem.
# Please instead update this file by running `bin/tapioca gem karafka-rdkafka`.


# Main Rdkafka namespace of this gem
#
# source://karafka-rdkafka//lib/rdkafka/version.rb#3
module Rdkafka; end

# This class serves as an abstract base class to represent handles within the Rdkafka module.
# As a subclass of `FFI::Struct`, this class provides a blueprint for other specific handle
# classes to inherit from, ensuring they adhere to a particular structure and behavior.
#
# Subclasses must define their own layout, and the layout must start with:
#
# layout :pending, :bool,
#        :response, :int
#
# source://karafka-rdkafka//lib/rdkafka/abstract_handle.rb#12
class Rdkafka::AbstractHandle < ::FFI::Struct
  include ::Rdkafka::Helpers::Time

  # @return [AbstractHandle] a new instance of AbstractHandle
  #
  # source://karafka-rdkafka//lib/rdkafka/abstract_handle.rb#42
  def initialize; end

  # @return [Object] operation-specific result
  #
  # source://karafka-rdkafka//lib/rdkafka/abstract_handle.rb#109
  def create_result; end

  # @return [String] the name of the operation (e.g. "delivery")
  #
  # source://karafka-rdkafka//lib/rdkafka/abstract_handle.rb#104
  def operation_name; end

  # Whether the handle is still pending.
  #
  # @return [Boolean]
  #
  # source://karafka-rdkafka//lib/rdkafka/abstract_handle.rb#52
  def pending?; end

  # Allow subclasses to override
  #
  # source://karafka-rdkafka//lib/rdkafka/abstract_handle.rb#114
  def raise_error; end

  # Unlock the resources
  #
  # source://karafka-rdkafka//lib/rdkafka/abstract_handle.rb#96
  def unlock; end

  # Wait for the operation to complete or raise an error if this takes longer than the timeout.
  # If there is a timeout this does not mean the operation failed, rdkafka might still be working
  # on the operation. In this case it is possible to call wait again.
  #
  # @param max_wait_timeout [Numeric, nil] Amount of time to wait before timing out.
  #   If this is nil we will wait forever
  # @param wait_timeout [nil] deprecated
  # @param raise_response_error [Boolean] should we raise error when waiting finishes
  # @raise [RdkafkaError] When the operation failed
  # @raise [WaitTimeoutError] When the timeout has been reached and the handle is still pending
  # @return [Object] Operation-specific result
  #
  # source://karafka-rdkafka//lib/rdkafka/abstract_handle.rb#69
  def wait(max_wait_timeout: T.unsafe(nil), wait_timeout: T.unsafe(nil), raise_response_error: T.unsafe(nil)); end

  class << self
    # Adds handle to the register
    #
    # @param handle [AbstractHandle] any handle we want to register
    #
    # source://karafka-rdkafka//lib/rdkafka/abstract_handle.rb#29
    def register(handle); end

    # Removes handle from the register based on the handle address
    #
    # @param address [Integer] address of the registered handle we want to remove
    #
    # source://karafka-rdkafka//lib/rdkafka/abstract_handle.rb#37
    def remove(address); end
  end
end

# Default wait timeout is 31 years
#
# source://karafka-rdkafka//lib/rdkafka/abstract_handle.rb#18
Rdkafka::AbstractHandle::MAX_WAIT_TIMEOUT_FOREVER = T.let(T.unsafe(nil), Integer)

# Registry for registering all the handles.
#
# source://karafka-rdkafka//lib/rdkafka/abstract_handle.rb#16
Rdkafka::AbstractHandle::REGISTRY = T.let(T.unsafe(nil), Hash)

# Deprecation message for wait_timeout argument in wait method
#
# source://karafka-rdkafka//lib/rdkafka/abstract_handle.rb#20
Rdkafka::AbstractHandle::WAIT_TIMEOUT_DEPRECATION_MESSAGE = T.let(T.unsafe(nil), String)

# Error that is raised when waiting for the handle to complete
# takes longer than the specified timeout.
#
# source://karafka-rdkafka//lib/rdkafka/abstract_handle.rb#120
class Rdkafka::AbstractHandle::WaitTimeoutError < ::RuntimeError; end

# source://karafka-rdkafka//lib/rdkafka/admin.rb#4
class Rdkafka::Admin
  include ::Rdkafka::Helpers::OAuth

  # @private
  # @return [Admin] a new instance of Admin
  #
  # source://karafka-rdkafka//lib/rdkafka/admin.rb#52
  def initialize(native_kafka); end

  # Close this admin instance
  #
  # source://karafka-rdkafka//lib/rdkafka/admin.rb#90
  def close; end

  # Whether this admin has closed
  #
  # @return [Boolean]
  #
  # source://karafka-rdkafka//lib/rdkafka/admin.rb#97
  def closed?; end

  # Create acl
  #
  # @param resource_type - values of type rd_kafka_ResourceType_t
  #   https://github.com/confluentinc/librdkafka/blob/292d2a66b9921b783f08147807992e603c7af059/src/rdkafka.h#L7307
  #   valid values are:
  #   RD_KAFKA_RESOURCE_TOPIC   = 2
  #   RD_KAFKA_RESOURCE_GROUP   = 3
  #   RD_KAFKA_RESOURCE_BROKER  = 4
  # @param resource_pattern_type - values of type rd_kafka_ResourcePatternType_t
  #   https://github.com/confluentinc/librdkafka/blob/292d2a66b9921b783f08147807992e603c7af059/src/rdkafka.h#L7320
  #   valid values are:
  #   RD_KAFKA_RESOURCE_PATTERN_MATCH    = 2
  #   RD_KAFKA_RESOURCE_PATTERN_LITERAL  = 3
  #   RD_KAFKA_RESOURCE_PATTERN_PREFIXED = 4
  # @param operation - values of type rd_kafka_AclOperation_t
  #   https://github.com/confluentinc/librdkafka/blob/292d2a66b9921b783f08147807992e603c7af059/src/rdkafka.h#L8403
  #   valid values are:
  #   RD_KAFKA_ACL_OPERATION_ALL              = 2
  #   RD_KAFKA_ACL_OPERATION_READ             = 3
  #   RD_KAFKA_ACL_OPERATION_WRITE            = 4
  #   RD_KAFKA_ACL_OPERATION_CREATE           = 5
  #   RD_KAFKA_ACL_OPERATION_DELETE           = 6
  #   RD_KAFKA_ACL_OPERATION_ALTER            = 7
  #   RD_KAFKA_ACL_OPERATION_DESCRIBE         = 8
  #   RD_KAFKA_ACL_OPERATION_CLUSTER_ACTION   = 9
  #   RD_KAFKA_ACL_OPERATION_DESCRIBE_CONFIGS = 10
  #   RD_KAFKA_ACL_OPERATION_ALTER_CONFIGS    = 11
  #   RD_KAFKA_ACL_OPERATION_IDEMPOTENT_WRITE = 12
  # @param permission_type - values of type rd_kafka_AclPermissionType_t
  #   https://github.com/confluentinc/librdkafka/blob/292d2a66b9921b783f08147807992e603c7af059/src/rdkafka.h#L8435
  #   valid values are:
  #   RD_KAFKA_ACL_PERMISSION_TYPE_DENY  = 2
  #   RD_KAFKA_ACL_PERMISSION_TYPE_ALLOW = 3
  # @raise [RdkafkaError]
  # @return [CreateAclHandle] Create acl handle that can be used to wait for the result of creating the acl
  #
  # source://karafka-rdkafka//lib/rdkafka/admin.rb#392
  def create_acl(resource_type:, resource_name:, resource_pattern_type:, principal:, host:, operation:, permission_type:); end

  # Creates extra partitions for a given topic
  #
  # @param topic_name [String]
  # @param partition_count [Integer] how many partitions we want to end up with for given topic
  # @raise [ConfigError] When the partition count or replication factor are out of valid range
  # @raise [RdkafkaError] When the topic name is invalid or the topic already exists
  # @raise [RdkafkaError] When the topic configuration is invalid
  # @return [CreateTopicHandle] Create topic handle that can be used to wait for the result of creating the topic
  #
  # source://karafka-rdkafka//lib/rdkafka/admin.rb#301
  def create_partitions(topic_name, partition_count); end

  # Create a topic with the given partition count and replication factor
  #
  # @raise [ConfigError] When the partition count or replication factor are out of valid range
  # @raise [RdkafkaError] When the topic name is invalid or the topic already exists
  # @raise [RdkafkaError] When the topic configuration is invalid
  # @return [CreateTopicHandle] Create topic handle that can be used to wait for the result of
  #   creating the topic
  #
  # source://karafka-rdkafka//lib/rdkafka/admin.rb#109
  def create_topic(topic_name, partition_count, replication_factor, topic_config = T.unsafe(nil)); end

  # Delete acl
  #
  # @param resource_type - values of type rd_kafka_ResourceType_t
  #   https://github.com/confluentinc/librdkafka/blob/292d2a66b9921b783f08147807992e603c7af059/src/rdkafka.h#L7307
  #   valid values are:
  #   RD_KAFKA_RESOURCE_TOPIC   = 2
  #   RD_KAFKA_RESOURCE_GROUP   = 3
  #   RD_KAFKA_RESOURCE_BROKER  = 4
  # @param resource_pattern_type - values of type rd_kafka_ResourcePatternType_t
  #   https://github.com/confluentinc/librdkafka/blob/292d2a66b9921b783f08147807992e603c7af059/src/rdkafka.h#L7320
  #   valid values are:
  #   RD_KAFKA_RESOURCE_PATTERN_MATCH    = 2
  #   RD_KAFKA_RESOURCE_PATTERN_LITERAL  = 3
  #   RD_KAFKA_RESOURCE_PATTERN_PREFIXED = 4
  # @param operation - values of type rd_kafka_AclOperation_t
  #   https://github.com/confluentinc/librdkafka/blob/292d2a66b9921b783f08147807992e603c7af059/src/rdkafka.h#L8403
  #   valid values are:
  #   RD_KAFKA_ACL_OPERATION_ALL              = 2
  #   RD_KAFKA_ACL_OPERATION_READ             = 3
  #   RD_KAFKA_ACL_OPERATION_WRITE            = 4
  #   RD_KAFKA_ACL_OPERATION_CREATE           = 5
  #   RD_KAFKA_ACL_OPERATION_DELETE           = 6
  #   RD_KAFKA_ACL_OPERATION_ALTER            = 7
  #   RD_KAFKA_ACL_OPERATION_DESCRIBE         = 8
  #   RD_KAFKA_ACL_OPERATION_CLUSTER_ACTION   = 9
  #   RD_KAFKA_ACL_OPERATION_DESCRIBE_CONFIGS = 10
  #   RD_KAFKA_ACL_OPERATION_ALTER_CONFIGS    = 11
  #   RD_KAFKA_ACL_OPERATION_IDEMPOTENT_WRITE = 12
  # @param permission_type - values of type rd_kafka_AclPermissionType_t
  #   https://github.com/confluentinc/librdkafka/blob/292d2a66b9921b783f08147807992e603c7af059/src/rdkafka.h#L8435
  #   valid values are:
  #   RD_KAFKA_ACL_PERMISSION_TYPE_DENY  = 2
  #   RD_KAFKA_ACL_PERMISSION_TYPE_ALLOW = 3
  # @raise [RdkafkaError]
  # @return [DeleteAclHandle] Delete acl handle that can be used to wait for the result of deleting the acl
  #
  # source://karafka-rdkafka//lib/rdkafka/admin.rb#497
  def delete_acl(resource_type:, resource_name:, resource_pattern_type:, principal:, host:, operation:, permission_type:); end

  # source://karafka-rdkafka//lib/rdkafka/admin.rb#181
  def delete_group(group_id); end

  # Deletes the named topic
  #
  # @raise [RdkafkaError] When the topic name is invalid or the topic does not exist
  # @return [DeleteTopicHandle] Delete topic handle that can be used to wait for the result of
  #   deleting the topic
  #
  # source://karafka-rdkafka//lib/rdkafka/admin.rb#239
  def delete_topic(topic_name); end

  # Describe acls
  #
  # @param resource_type - values of type rd_kafka_ResourceType_t
  #   https://github.com/confluentinc/librdkafka/blob/292d2a66b9921b783f08147807992e603c7af059/src/rdkafka.h#L7307
  #   valid values are:
  #   RD_KAFKA_RESOURCE_TOPIC   = 2
  #   RD_KAFKA_RESOURCE_GROUP   = 3
  #   RD_KAFKA_RESOURCE_BROKER  = 4
  # @param resource_pattern_type - values of type rd_kafka_ResourcePatternType_t
  #   https://github.com/confluentinc/librdkafka/blob/292d2a66b9921b783f08147807992e603c7af059/src/rdkafka.h#L7320
  #   valid values are:
  #   RD_KAFKA_RESOURCE_PATTERN_MATCH    = 2
  #   RD_KAFKA_RESOURCE_PATTERN_LITERAL  = 3
  #   RD_KAFKA_RESOURCE_PATTERN_PREFIXED = 4
  # @param operation - values of type rd_kafka_AclOperation_t
  #   https://github.com/confluentinc/librdkafka/blob/292d2a66b9921b783f08147807992e603c7af059/src/rdkafka.h#L8403
  #   valid values are:
  #   RD_KAFKA_ACL_OPERATION_ALL              = 2
  #   RD_KAFKA_ACL_OPERATION_READ             = 3
  #   RD_KAFKA_ACL_OPERATION_WRITE            = 4
  #   RD_KAFKA_ACL_OPERATION_CREATE           = 5
  #   RD_KAFKA_ACL_OPERATION_DELETE           = 6
  #   RD_KAFKA_ACL_OPERATION_ALTER            = 7
  #   RD_KAFKA_ACL_OPERATION_DESCRIBE         = 8
  #   RD_KAFKA_ACL_OPERATION_CLUSTER_ACTION   = 9
  #   RD_KAFKA_ACL_OPERATION_DESCRIBE_CONFIGS = 10
  #   RD_KAFKA_ACL_OPERATION_ALTER_CONFIGS    = 11
  #   RD_KAFKA_ACL_OPERATION_IDEMPOTENT_WRITE = 12
  # @param permission_type - values of type rd_kafka_AclPermissionType_t
  #   https://github.com/confluentinc/librdkafka/blob/292d2a66b9921b783f08147807992e603c7af059/src/rdkafka.h#L8435
  #   valid values are:
  #   RD_KAFKA_ACL_PERMISSION_TYPE_DENY  = 2
  #   RD_KAFKA_ACL_PERMISSION_TYPE_ALLOW = 3
  # @raise [RdkafkaError]
  # @return [DescribeAclHandle] Describe acl handle that can be used to wait for the result of fetching acls
  #
  # source://karafka-rdkafka//lib/rdkafka/admin.rb#604
  def describe_acl(resource_type:, resource_name:, resource_pattern_type:, principal:, host:, operation:, permission_type:); end

  # Describe configs
  #
  # @note Several resources can be requested at one go, but only one broker at a time
  # @param resources [Array<Hash>] Array where elements are hashes with two keys:
  #   - `:resource_type` - numerical resource type based on Kafka API
  #   - `:resource_name` - string with resource name
  # @raise [RdkafkaError]
  # @return [DescribeConfigsHandle] Describe config handle that can be used to wait for the
  #   result of fetching resources with their appropriate configs
  #
  # source://karafka-rdkafka//lib/rdkafka/admin.rb#678
  def describe_configs(resources); end

  # source://karafka-rdkafka//lib/rdkafka/admin.rb#72
  def finalizer; end

  # Alters in an incremental way all the configs provided for given resources
  #
  # name, value and the proper op_type to perform on this value.
  #
  # @note Several resources can be requested at one go, but only one broker at a time
  # @note The results won't contain altered values but only the altered resources
  # @param resources_with_configs [Array<Hash>] resources with the configs key that contains
  # @raise [RdkafkaError]
  # @return [IncrementalAlterConfigsHandle] Incremental alter configs handle that can be used to
  #   wait for the result of altering resources with their appropriate configs
  #
  # source://karafka-rdkafka//lib/rdkafka/admin.rb#751
  def incremental_alter_configs(resources_with_configs); end

  # Performs the metadata request using admin
  #
  # @param topic_name [String, nil] metadat about particular topic or all if nil
  # @param timeout_ms [Integer] metadata request timeout
  # @return [Metadata] requested metadata
  #
  # source://karafka-rdkafka//lib/rdkafka/admin.rb#81
  def metadata(topic_name = T.unsafe(nil), timeout_ms = T.unsafe(nil)); end

  # @return [String] admin name
  #
  # source://karafka-rdkafka//lib/rdkafka/admin.rb#66
  def name; end

  # Starts the native Kafka polling thread and kicks off the init polling
  #
  # @note Not needed to run unless explicit start was disabled
  #
  # source://karafka-rdkafka//lib/rdkafka/admin.rb#61
  def start; end

  private

  # @raise [Rdkafka::ClosedAdminError]
  #
  # source://karafka-rdkafka//lib/rdkafka/admin.rb#828
  def closed_admin_check(method); end

  class << self
    # Allows us to retrieve librdkafka errors with descriptions
    # Useful for debugging and building UIs, etc.
    #
    # @return [Hash<Integer, Hash>] hash with errors mapped by code
    #
    # source://karafka-rdkafka//lib/rdkafka/admin.rb#12
    def describe_errors; end
  end
end

# Extracts attributes of rd_kafka_AclBinding_t
#
# source://karafka-rdkafka//lib/rdkafka/admin/acl_binding_result.rb#7
class Rdkafka::Admin::AclBindingResult
  # @return [AclBindingResult] a new instance of AclBindingResult
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/acl_binding_result.rb#17
  def initialize(matching_acl); end

  # Returns the value of attribute error_string.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/acl_binding_result.rb#8
  def error_string; end

  # Returns the value of attribute matching_acl_host.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/acl_binding_result.rb#8
  def matching_acl_host; end

  # Returns the value of attribute matching_acl_operation.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/acl_binding_result.rb#8
  def matching_acl_operation; end

  # Returns the value of attribute matching_acl_resource_pattern_type.
  # This attribute was initially released under the name that is now an alias
  # We keep it for backwards compatibility but it was changed for the consistency
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/acl_binding_result.rb#8
  def matching_acl_pattern_type; end

  # Returns the value of attribute matching_acl_permission_type.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/acl_binding_result.rb#8
  def matching_acl_permission_type; end

  # Returns the value of attribute matching_acl_principal.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/acl_binding_result.rb#8
  def matching_acl_principal; end

  # Returns the value of attribute matching_acl_resource_name.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/acl_binding_result.rb#8
  def matching_acl_resource_name; end

  # Returns the value of attribute matching_acl_resource_pattern_type.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/acl_binding_result.rb#8
  def matching_acl_resource_pattern_type; end

  # Returns the value of attribute matching_acl_resource_type.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/acl_binding_result.rb#8
  def matching_acl_resource_type; end

  # Returns the value of attribute result_error.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/acl_binding_result.rb#8
  def result_error; end
end

# A single config binding result that represents its values extracted from C
#
# source://karafka-rdkafka//lib/rdkafka/admin/config_binding_result.rb#6
class Rdkafka::Admin::ConfigBindingResult
  # @param config_ptr [FFI::Pointer] config pointer
  # @return [ConfigBindingResult] a new instance of ConfigBindingResult
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/config_binding_result.rb#10
  def initialize(config_ptr); end

  # Returns the value of attribute default.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/config_binding_result.rb#7
  def default; end

  # Returns the value of attribute name.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/config_binding_result.rb#7
  def name; end

  # Returns the value of attribute read_only.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/config_binding_result.rb#7
  def read_only; end

  # Returns the value of attribute sensitive.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/config_binding_result.rb#7
  def sensitive; end

  # Returns the value of attribute synonym.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/config_binding_result.rb#7
  def synonym; end

  # Returns the value of attribute synonyms.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/config_binding_result.rb#7
  def synonyms; end

  # Returns the value of attribute value.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/config_binding_result.rb#7
  def value; end
end

# A simple binding that represents the requested config resource
#
# source://karafka-rdkafka//lib/rdkafka/admin/config_resource_binding_result.rb#6
class Rdkafka::Admin::ConfigResourceBindingResult
  # @return [ConfigResourceBindingResult] a new instance of ConfigResourceBindingResult
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/config_resource_binding_result.rb#9
  def initialize(config_resource_ptr); end

  # Returns the value of attribute configs.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/config_resource_binding_result.rb#7
  def configs; end

  # Returns the value of attribute configs_count.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/config_resource_binding_result.rb#7
  def configs_count; end

  # Returns the value of attribute name.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/config_resource_binding_result.rb#7
  def name; end

  # Returns the value of attribute type.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/config_resource_binding_result.rb#7
  def type; end
end

# source://karafka-rdkafka//lib/rdkafka/admin/create_acl_handle.rb#5
class Rdkafka::Admin::CreateAclHandle < ::Rdkafka::AbstractHandle
  # @return [CreateAclReport] instance with rdkafka_response value as 0 and rdkafka_response_string value as empty string if the acl creation was successful
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/create_acl_handle.rb#16
  def create_result; end

  # @return [String] the name of the operation
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/create_acl_handle.rb#11
  def operation_name; end

  # @raise [RdkafkaError]
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/create_acl_handle.rb#20
  def raise_error; end
end

# source://karafka-rdkafka//lib/rdkafka/admin/create_acl_report.rb#5
class Rdkafka::Admin::CreateAclReport
  # @return [CreateAclReport] a new instance of CreateAclReport
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/create_acl_report.rb#16
  def initialize(rdkafka_response:, rdkafka_response_string:); end

  # Upon successful creation of Acl RD_KAFKA_RESP_ERR_NO_ERROR - 0 is returned as rdkafka_response
  #
  # @return [Integer]
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/create_acl_report.rb#9
  def rdkafka_response; end

  # Upon successful creation of Acl empty string will be returned as rdkafka_response_string
  #
  # @return [String]
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/create_acl_report.rb#14
  def rdkafka_response_string; end
end

# source://karafka-rdkafka//lib/rdkafka/admin/create_partitions_handle.rb#3
class Rdkafka::Admin::CreatePartitionsHandle < ::Rdkafka::AbstractHandle
  # @return [Boolean] whether the create topic was successful
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/create_partitions_handle.rb#15
  def create_result; end

  # @return [String] the name of the operation
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/create_partitions_handle.rb#10
  def operation_name; end

  # source://karafka-rdkafka//lib/rdkafka/admin/create_partitions_handle.rb#19
  def raise_error; end
end

# source://karafka-rdkafka//lib/rdkafka/admin/create_partitions_report.rb#3
class Rdkafka::Admin::CreatePartitionsReport < ::Rdkafka::Admin::CreateTopicReport; end

# source://karafka-rdkafka//lib/rdkafka/admin/create_topic_handle.rb#5
class Rdkafka::Admin::CreateTopicHandle < ::Rdkafka::AbstractHandle
  # @return [Boolean] whether the create topic was successful
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/create_topic_handle.rb#17
  def create_result; end

  # @return [String] the name of the operation
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/create_topic_handle.rb#12
  def operation_name; end

  # source://karafka-rdkafka//lib/rdkafka/admin/create_topic_handle.rb#21
  def raise_error; end
end

# source://karafka-rdkafka//lib/rdkafka/admin/create_topic_report.rb#5
class Rdkafka::Admin::CreateTopicReport
  # @return [CreateTopicReport] a new instance of CreateTopicReport
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/create_topic_report.rb#14
  def initialize(error_string, result_name); end

  # Any error message generated from the CreateTopic
  #
  # @return [String]
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/create_topic_report.rb#8
  def error_string; end

  # The name of the topic created
  #
  # @return [String]
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/create_topic_report.rb#12
  def result_name; end
end

# source://karafka-rdkafka//lib/rdkafka/admin/delete_acl_handle.rb#5
class Rdkafka::Admin::DeleteAclHandle < ::Rdkafka::AbstractHandle
  # @return [DeleteAclReport] instance with an array of matching_acls
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/delete_acl_handle.rb#18
  def create_result; end

  # @return [String] the name of the operation
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/delete_acl_handle.rb#13
  def operation_name; end

  # @raise [RdkafkaError]
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/delete_acl_handle.rb#22
  def raise_error; end
end

# source://karafka-rdkafka//lib/rdkafka/admin/delete_acl_report.rb#5
class Rdkafka::Admin::DeleteAclReport
  # @return [DeleteAclReport] a new instance of DeleteAclReport
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/delete_acl_report.rb#11
  def initialize(matching_acls:, matching_acls_count:); end

  # deleted acls
  #
  # @return [Rdkafka::Bindings::AclBindingResult]
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/delete_acl_report.rb#9
  def deleted_acls; end
end

# source://karafka-rdkafka//lib/rdkafka/admin/delete_groups_handle.rb#5
class Rdkafka::Admin::DeleteGroupsHandle < ::Rdkafka::AbstractHandle
  # source://karafka-rdkafka//lib/rdkafka/admin/delete_groups_handle.rb#16
  def create_result; end

  # @return [String] the name of the operation
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/delete_groups_handle.rb#12
  def operation_name; end

  # @raise [RdkafkaError]
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/delete_groups_handle.rb#20
  def raise_error; end
end

# source://karafka-rdkafka//lib/rdkafka/admin/delete_groups_report.rb#5
class Rdkafka::Admin::DeleteGroupsReport
  # @return [DeleteGroupsReport] a new instance of DeleteGroupsReport
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/delete_groups_report.rb#14
  def initialize(error_string, result_name); end

  # Any error message generated from the DeleteTopic
  #
  # @return [String]
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/delete_groups_report.rb#8
  def error_string; end

  # The name of the topic deleted
  #
  # @return [String]
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/delete_groups_report.rb#12
  def result_name; end
end

# source://karafka-rdkafka//lib/rdkafka/admin/delete_topic_handle.rb#5
class Rdkafka::Admin::DeleteTopicHandle < ::Rdkafka::AbstractHandle
  # @return [Boolean] whether the delete topic was successful
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/delete_topic_handle.rb#17
  def create_result; end

  # @return [String] the name of the operation
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/delete_topic_handle.rb#12
  def operation_name; end

  # source://karafka-rdkafka//lib/rdkafka/admin/delete_topic_handle.rb#21
  def raise_error; end
end

# source://karafka-rdkafka//lib/rdkafka/admin/delete_topic_report.rb#5
class Rdkafka::Admin::DeleteTopicReport
  # @return [DeleteTopicReport] a new instance of DeleteTopicReport
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/delete_topic_report.rb#14
  def initialize(error_string, result_name); end

  # Any error message generated from the DeleteTopic
  #
  # @return [String]
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/delete_topic_report.rb#8
  def error_string; end

  # The name of the topic deleted
  #
  # @return [String]
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/delete_topic_report.rb#12
  def result_name; end
end

# source://karafka-rdkafka//lib/rdkafka/admin/describe_acl_handle.rb#5
class Rdkafka::Admin::DescribeAclHandle < ::Rdkafka::AbstractHandle
  # @return [DescribeAclReport] instance with an array of acls that matches the request filters.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/describe_acl_handle.rb#18
  def create_result; end

  # @return [String] the name of the operation.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/describe_acl_handle.rb#13
  def operation_name; end

  # @raise [RdkafkaError]
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/describe_acl_handle.rb#22
  def raise_error; end
end

# source://karafka-rdkafka//lib/rdkafka/admin/describe_acl_report.rb#5
class Rdkafka::Admin::DescribeAclReport
  # @return [DescribeAclReport] a new instance of DescribeAclReport
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/describe_acl_report.rb#11
  def initialize(acls:, acls_count:); end

  # acls that exists in the cluster for the resource_type, resource_name and pattern_type filters provided in the request.
  #
  # @return [Rdkafka::Bindings::AclBindingResult] array of matching acls.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/describe_acl_report.rb#9
  def acls; end
end

# source://karafka-rdkafka//lib/rdkafka/admin/describe_configs_handle.rb#5
class Rdkafka::Admin::DescribeConfigsHandle < ::Rdkafka::AbstractHandle
  # @return [DescribeAclReport] instance with an array of acls that matches the request filters.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/describe_configs_handle.rb#18
  def create_result; end

  # @return [String] the name of the operation.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/describe_configs_handle.rb#13
  def operation_name; end

  # @raise [RdkafkaError]
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/describe_configs_handle.rb#25
  def raise_error; end
end

# source://karafka-rdkafka//lib/rdkafka/admin/describe_configs_report.rb#5
class Rdkafka::Admin::DescribeConfigsReport
  # @return [DescribeConfigsReport] a new instance of DescribeConfigsReport
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/describe_configs_report.rb#8
  def initialize(config_entries:, entry_count:); end

  # Returns the value of attribute resources.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/describe_configs_report.rb#6
  def resources; end

  private

  # source://karafka-rdkafka//lib/rdkafka/admin/describe_configs_report.rb#40
  def validate!(config_resource_result_ptr); end
end

# source://karafka-rdkafka//lib/rdkafka/admin/incremental_alter_configs_handle.rb#5
class Rdkafka::Admin::IncrementalAlterConfigsHandle < ::Rdkafka::AbstractHandle
  # @return [DescribeAclReport] instance with an array of acls that matches the request filters.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/incremental_alter_configs_handle.rb#18
  def create_result; end

  # @return [String] the name of the operation.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/incremental_alter_configs_handle.rb#13
  def operation_name; end

  # @raise [RdkafkaError]
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/incremental_alter_configs_handle.rb#25
  def raise_error; end
end

# source://karafka-rdkafka//lib/rdkafka/admin/incremental_alter_configs_report.rb#5
class Rdkafka::Admin::IncrementalAlterConfigsReport
  # @return [IncrementalAlterConfigsReport] a new instance of IncrementalAlterConfigsReport
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/incremental_alter_configs_report.rb#8
  def initialize(config_entries:, entry_count:); end

  # Returns the value of attribute resources.
  #
  # source://karafka-rdkafka//lib/rdkafka/admin/incremental_alter_configs_report.rb#6
  def resources; end

  private

  # source://karafka-rdkafka//lib/rdkafka/admin/incremental_alter_configs_report.rb#40
  def validate!(config_resource_result_ptr); end
end

# Base error class.
#
# source://karafka-rdkafka//lib/rdkafka/error.rb#5
class Rdkafka::BaseError < ::RuntimeError; end

# @note There are two types of responses related to errors:
#   - rd_kafka_error_t - a C object that we need to remap into an error or null when no error
#   - rd_kafka_resp_err_t - response error code (numeric) that we can use directly
#
#   It is critical to ensure, that we handle them correctly. The result type should be:
#   - rd_kafka_error_t - :pointer
#   - rd_kafka_resp_err_t - :int
# @private
#
# source://karafka-rdkafka//lib/rdkafka/bindings.rb#14
module Rdkafka::Bindings
  extend ::FFI::Library

  def rd_kafka_AclBindingFilter_new(*_arg0); end
  def rd_kafka_AclBinding_destroy(*_arg0); end
  def rd_kafka_AclBinding_error(*_arg0); end
  def rd_kafka_AclBinding_host(*_arg0); end
  def rd_kafka_AclBinding_name(*_arg0); end
  def rd_kafka_AclBinding_new(*_arg0); end
  def rd_kafka_AclBinding_operation(*_arg0); end
  def rd_kafka_AclBinding_permission_type(*_arg0); end
  def rd_kafka_AclBinding_principal(*_arg0); end
  def rd_kafka_AclBinding_resource_pattern_type(*_arg0); end
  def rd_kafka_AclBinding_restype(*_arg0); end
  def rd_kafka_AdminOptions_destroy(*_arg0); end
  def rd_kafka_AdminOptions_new(*_arg0); end
  def rd_kafka_AdminOptions_set_opaque(*_arg0); end
  def rd_kafka_ConfigEntry_is_default(*_arg0); end
  def rd_kafka_ConfigEntry_is_read_only(*_arg0); end
  def rd_kafka_ConfigEntry_is_sensitive(*_arg0); end
  def rd_kafka_ConfigEntry_is_synonym(*_arg0); end
  def rd_kafka_ConfigEntry_name(*_arg0); end
  def rd_kafka_ConfigEntry_synonyms(*_arg0); end
  def rd_kafka_ConfigEntry_value(*_arg0); end
  def rd_kafka_ConfigResource_add_incremental_config(*_arg0); end
  def rd_kafka_ConfigResource_configs(*_arg0); end
  def rd_kafka_ConfigResource_destroy_array(*_arg0); end
  def rd_kafka_ConfigResource_error(*_arg0); end
  def rd_kafka_ConfigResource_error_string(*_arg0); end
  def rd_kafka_ConfigResource_new(*_arg0); end
  def rd_kafka_CreateAcls(*_arg0); end
  def rd_kafka_CreateAcls_result_acls(*_arg0); end
  def rd_kafka_CreatePartitions(*_arg0); end
  def rd_kafka_CreatePartitions_result_topics(*_arg0); end
  def rd_kafka_CreateTopics(*_arg0); end
  def rd_kafka_CreateTopics_result_topics(*_arg0); end
  def rd_kafka_DeleteAcls(*_arg0); end
  def rd_kafka_DeleteAcls_result_response_error(*_arg0); end
  def rd_kafka_DeleteAcls_result_response_matching_acls(*_arg0); end
  def rd_kafka_DeleteAcls_result_responses(*_arg0); end
  def rd_kafka_DeleteGroup_destroy(*_arg0); end
  def rd_kafka_DeleteGroup_new(*_arg0); end
  def rd_kafka_DeleteGroups(*_arg0); end
  def rd_kafka_DeleteGroups_result_groups(*_arg0); end
  def rd_kafka_DeleteTopic_destroy(*_arg0); end
  def rd_kafka_DeleteTopic_new(*_arg0); end
  def rd_kafka_DeleteTopics(*_arg0); end
  def rd_kafka_DeleteTopics_result_topics(*_arg0); end
  def rd_kafka_DescribeAcls(*_arg0); end
  def rd_kafka_DescribeAcls_result_acls(*_arg0); end
  def rd_kafka_DescribeConfigs(*_arg0); end
  def rd_kafka_DescribeConfigs_result_resources(*_arg0); end
  def rd_kafka_IncrementalAlterConfigs(*_arg0); end
  def rd_kafka_IncrementalAlterConfigs_result_resources(*_arg0); end
  def rd_kafka_NewPartitions_destroy(*_arg0); end
  def rd_kafka_NewPartitions_new(*_arg0); end
  def rd_kafka_NewTopic_destroy(*_arg0); end
  def rd_kafka_NewTopic_new(*_arg0); end
  def rd_kafka_NewTopic_set_config(*_arg0); end
  def rd_kafka_abort_transaction(*_arg0); end
  def rd_kafka_acl_result_error(*_arg0); end
  def rd_kafka_assign(*_arg0); end
  def rd_kafka_assignment(*_arg0); end
  def rd_kafka_assignment_lost(*_arg0); end
  def rd_kafka_begin_transaction(*_arg0); end
  def rd_kafka_clusterid(*_arg0); end
  def rd_kafka_commit(*_arg0); end
  def rd_kafka_commit_transaction(*_arg0); end
  def rd_kafka_committed(*_arg0); end
  def rd_kafka_conf_new(*_arg0); end
  def rd_kafka_conf_set(*_arg0); end
  def rd_kafka_conf_set_background_event_cb(*_arg0); end
  def rd_kafka_conf_set_dr_msg_cb(*_arg0); end
  def rd_kafka_conf_set_error_cb(*_arg0); end
  def rd_kafka_conf_set_log_cb(*_arg0); end
  def rd_kafka_conf_set_oauthbearer_token_refresh_cb(*_arg0); end
  def rd_kafka_conf_set_opaque(*_arg0); end
  def rd_kafka_conf_set_rebalance_cb(*_arg0); end
  def rd_kafka_conf_set_stats_cb(*_arg0); end
  def rd_kafka_consumer_close(*_arg0); end
  def rd_kafka_consumer_group_metadata(*_arg0); end
  def rd_kafka_consumer_group_metadata_destroy(*_arg0); end
  def rd_kafka_consumer_poll(*_arg0); end
  def rd_kafka_destroy(*_arg0); end
  def rd_kafka_err2name(*_arg0); end
  def rd_kafka_err2str(*_arg0); end
  def rd_kafka_error_code(*_arg0); end
  def rd_kafka_error_destroy(*_arg0); end
  def rd_kafka_error_is_fatal(*_arg0); end
  def rd_kafka_error_is_retriable(*_arg0); end
  def rd_kafka_error_string(*_arg0); end
  def rd_kafka_error_txn_requires_abort(*_arg0); end
  def rd_kafka_event_CreateAcls_result(*_arg0); end
  def rd_kafka_event_CreatePartitions_result(*_arg0); end
  def rd_kafka_event_CreateTopics_result(*_arg0); end
  def rd_kafka_event_DeleteAcls_result(*_arg0); end
  def rd_kafka_event_DeleteGroups_result(*_arg0); end
  def rd_kafka_event_DeleteTopics_result(*_arg0); end
  def rd_kafka_event_DescribeAcls_result(*_arg0); end
  def rd_kafka_event_DescribeConfigs_result(*_arg0); end
  def rd_kafka_event_IncrementalAlterConfigs_result(*_arg0); end
  def rd_kafka_event_error(*_arg0); end
  def rd_kafka_event_error_string(*_arg0); end
  def rd_kafka_event_opaque(*_arg0); end
  def rd_kafka_event_type(*_arg0); end
  def rd_kafka_flush(*_arg0); end
  def rd_kafka_get_err_descs(*_arg0); end
  def rd_kafka_group_result_error(*_arg0); end
  def rd_kafka_group_result_name(*_arg0); end
  def rd_kafka_header_get_all(*_arg0); end
  def rd_kafka_incremental_assign(*_arg0); end
  def rd_kafka_incremental_unassign(*_arg0); end
  def rd_kafka_init_transactions(*_arg0); end
  def rd_kafka_memberid(*_arg0); end
  def rd_kafka_message_destroy(*_arg0); end
  def rd_kafka_message_headers(*_arg0); end
  def rd_kafka_message_timestamp(*_arg0); end
  def rd_kafka_metadata(*_arg0); end
  def rd_kafka_metadata_destroy(*_arg0); end
  def rd_kafka_msg_partitioner_consistent(*_arg0); end
  def rd_kafka_msg_partitioner_consistent_random(*_arg0); end
  def rd_kafka_msg_partitioner_fnv1a(*_arg0); end
  def rd_kafka_msg_partitioner_fnv1a_random(*_arg0); end
  def rd_kafka_msg_partitioner_murmur2(*_arg0); end
  def rd_kafka_msg_partitioner_murmur2_random(*_arg0); end
  def rd_kafka_msg_partitioner_random(*_arg0); end
  def rd_kafka_name(*_arg0); end
  def rd_kafka_new(*_arg0); end
  def rd_kafka_oauthbearer_set_token(*_arg0); end
  def rd_kafka_oauthbearer_set_token_failure(*_arg0); end
  def rd_kafka_offsets_for_times(*_arg0); end
  def rd_kafka_offsets_store(*_arg0); end
  def rd_kafka_outq_len(*_arg0); end
  def rd_kafka_pause_partitions(*_arg0); end
  def rd_kafka_poll(*_arg0); end
  def rd_kafka_poll_set_consumer(*_arg0); end
  def rd_kafka_position(*_arg0); end

  # source://ffi/1.17.0/lib/ffi/variadic.rb#61
  def rd_kafka_producev(*args); end

  def rd_kafka_purge(*_arg0); end
  def rd_kafka_query_watermark_offsets(*_arg0); end
  def rd_kafka_queue_destroy(*_arg0); end
  def rd_kafka_queue_get_background(*_arg0); end
  def rd_kafka_queue_get_main(*_arg0); end
  def rd_kafka_rebalance_protocol(*_arg0); end
  def rd_kafka_resume_partitions(*_arg0); end
  def rd_kafka_seek(*_arg0); end
  def rd_kafka_send_offsets_to_transaction(*_arg0); end
  def rd_kafka_set_log_queue(*_arg0); end
  def rd_kafka_subscribe(*_arg0); end
  def rd_kafka_subscription(*_arg0); end
  def rd_kafka_topic_conf_new(*_arg0); end
  def rd_kafka_topic_conf_set(*_arg0); end
  def rd_kafka_topic_destroy(*_arg0); end
  def rd_kafka_topic_name(*_arg0); end
  def rd_kafka_topic_new(*_arg0); end
  def rd_kafka_topic_partition_list_add(*_arg0); end
  def rd_kafka_topic_partition_list_copy(*_arg0); end
  def rd_kafka_topic_partition_list_destroy(*_arg0); end
  def rd_kafka_topic_partition_list_new(*_arg0); end
  def rd_kafka_topic_partition_list_set_offset(*_arg0); end
  def rd_kafka_topic_result_error(*_arg0); end
  def rd_kafka_topic_result_error_string(*_arg0); end
  def rd_kafka_topic_result_name(*_arg0); end
  def rd_kafka_unsubscribe(*_arg0); end

  class << self
    # source://karafka-rdkafka//lib/rdkafka/bindings.rb#17
    def lib_extension; end

    # source://karafka-rdkafka//lib/rdkafka/bindings.rb#377
    def partitioner(str, partition_count, partitioner_name = T.unsafe(nil)); end

    def rd_kafka_AclBindingFilter_new(*_arg0); end
    def rd_kafka_AclBinding_destroy(*_arg0); end
    def rd_kafka_AclBinding_error(*_arg0); end
    def rd_kafka_AclBinding_host(*_arg0); end
    def rd_kafka_AclBinding_name(*_arg0); end
    def rd_kafka_AclBinding_new(*_arg0); end
    def rd_kafka_AclBinding_operation(*_arg0); end
    def rd_kafka_AclBinding_permission_type(*_arg0); end
    def rd_kafka_AclBinding_principal(*_arg0); end
    def rd_kafka_AclBinding_resource_pattern_type(*_arg0); end
    def rd_kafka_AclBinding_restype(*_arg0); end
    def rd_kafka_AdminOptions_destroy(*_arg0); end
    def rd_kafka_AdminOptions_new(*_arg0); end
    def rd_kafka_AdminOptions_set_opaque(*_arg0); end
    def rd_kafka_ConfigEntry_is_default(*_arg0); end
    def rd_kafka_ConfigEntry_is_read_only(*_arg0); end
    def rd_kafka_ConfigEntry_is_sensitive(*_arg0); end
    def rd_kafka_ConfigEntry_is_synonym(*_arg0); end
    def rd_kafka_ConfigEntry_name(*_arg0); end
    def rd_kafka_ConfigEntry_synonyms(*_arg0); end
    def rd_kafka_ConfigEntry_value(*_arg0); end
    def rd_kafka_ConfigResource_add_incremental_config(*_arg0); end
    def rd_kafka_ConfigResource_configs(*_arg0); end
    def rd_kafka_ConfigResource_destroy_array(*_arg0); end
    def rd_kafka_ConfigResource_error(*_arg0); end
    def rd_kafka_ConfigResource_error_string(*_arg0); end
    def rd_kafka_ConfigResource_new(*_arg0); end
    def rd_kafka_CreateAcls(*_arg0); end
    def rd_kafka_CreateAcls_result_acls(*_arg0); end
    def rd_kafka_CreatePartitions(*_arg0); end
    def rd_kafka_CreatePartitions_result_topics(*_arg0); end
    def rd_kafka_CreateTopics(*_arg0); end
    def rd_kafka_CreateTopics_result_topics(*_arg0); end
    def rd_kafka_DeleteAcls(*_arg0); end
    def rd_kafka_DeleteAcls_result_response_error(*_arg0); end
    def rd_kafka_DeleteAcls_result_response_matching_acls(*_arg0); end
    def rd_kafka_DeleteAcls_result_responses(*_arg0); end
    def rd_kafka_DeleteGroup_destroy(*_arg0); end
    def rd_kafka_DeleteGroup_new(*_arg0); end
    def rd_kafka_DeleteGroups(*_arg0); end
    def rd_kafka_DeleteGroups_result_groups(*_arg0); end
    def rd_kafka_DeleteTopic_destroy(*_arg0); end
    def rd_kafka_DeleteTopic_new(*_arg0); end
    def rd_kafka_DeleteTopics(*_arg0); end
    def rd_kafka_DeleteTopics_result_topics(*_arg0); end
    def rd_kafka_DescribeAcls(*_arg0); end
    def rd_kafka_DescribeAcls_result_acls(*_arg0); end
    def rd_kafka_DescribeConfigs(*_arg0); end
    def rd_kafka_DescribeConfigs_result_resources(*_arg0); end
    def rd_kafka_IncrementalAlterConfigs(*_arg0); end
    def rd_kafka_IncrementalAlterConfigs_result_resources(*_arg0); end
    def rd_kafka_NewPartitions_destroy(*_arg0); end
    def rd_kafka_NewPartitions_new(*_arg0); end
    def rd_kafka_NewTopic_destroy(*_arg0); end
    def rd_kafka_NewTopic_new(*_arg0); end
    def rd_kafka_NewTopic_set_config(*_arg0); end
    def rd_kafka_abort_transaction(*_arg0); end
    def rd_kafka_acl_result_error(*_arg0); end
    def rd_kafka_assign(*_arg0); end
    def rd_kafka_assignment(*_arg0); end
    def rd_kafka_assignment_lost(*_arg0); end
    def rd_kafka_begin_transaction(*_arg0); end
    def rd_kafka_clusterid(*_arg0); end
    def rd_kafka_commit(*_arg0); end
    def rd_kafka_commit_transaction(*_arg0); end
    def rd_kafka_committed(*_arg0); end
    def rd_kafka_conf_new(*_arg0); end
    def rd_kafka_conf_set(*_arg0); end
    def rd_kafka_conf_set_background_event_cb(*_arg0); end
    def rd_kafka_conf_set_dr_msg_cb(*_arg0); end
    def rd_kafka_conf_set_error_cb(*_arg0); end
    def rd_kafka_conf_set_log_cb(*_arg0); end
    def rd_kafka_conf_set_oauthbearer_token_refresh_cb(*_arg0); end
    def rd_kafka_conf_set_opaque(*_arg0); end
    def rd_kafka_conf_set_rebalance_cb(*_arg0); end
    def rd_kafka_conf_set_stats_cb(*_arg0); end
    def rd_kafka_consumer_close(*_arg0); end
    def rd_kafka_consumer_group_metadata(*_arg0); end
    def rd_kafka_consumer_group_metadata_destroy(*_arg0); end
    def rd_kafka_consumer_poll(*_arg0); end
    def rd_kafka_destroy(*_arg0); end
    def rd_kafka_err2name(*_arg0); end
    def rd_kafka_err2str(*_arg0); end
    def rd_kafka_error_code(*_arg0); end
    def rd_kafka_error_destroy(*_arg0); end
    def rd_kafka_error_is_fatal(*_arg0); end
    def rd_kafka_error_is_retriable(*_arg0); end
    def rd_kafka_error_string(*_arg0); end
    def rd_kafka_error_txn_requires_abort(*_arg0); end
    def rd_kafka_event_CreateAcls_result(*_arg0); end
    def rd_kafka_event_CreatePartitions_result(*_arg0); end
    def rd_kafka_event_CreateTopics_result(*_arg0); end
    def rd_kafka_event_DeleteAcls_result(*_arg0); end
    def rd_kafka_event_DeleteGroups_result(*_arg0); end
    def rd_kafka_event_DeleteTopics_result(*_arg0); end
    def rd_kafka_event_DescribeAcls_result(*_arg0); end
    def rd_kafka_event_DescribeConfigs_result(*_arg0); end
    def rd_kafka_event_IncrementalAlterConfigs_result(*_arg0); end
    def rd_kafka_event_error(*_arg0); end
    def rd_kafka_event_error_string(*_arg0); end
    def rd_kafka_event_opaque(*_arg0); end
    def rd_kafka_event_type(*_arg0); end
    def rd_kafka_flush(*_arg0); end
    def rd_kafka_get_err_descs(*_arg0); end
    def rd_kafka_group_result_error(*_arg0); end
    def rd_kafka_group_result_name(*_arg0); end
    def rd_kafka_header_get_all(*_arg0); end
    def rd_kafka_incremental_assign(*_arg0); end
    def rd_kafka_incremental_unassign(*_arg0); end
    def rd_kafka_init_transactions(*_arg0); end
    def rd_kafka_memberid(*_arg0); end
    def rd_kafka_message_destroy(*_arg0); end
    def rd_kafka_message_headers(*_arg0); end
    def rd_kafka_message_timestamp(*_arg0); end
    def rd_kafka_metadata(*_arg0); end
    def rd_kafka_metadata_destroy(*_arg0); end
    def rd_kafka_msg_partitioner_consistent(*_arg0); end
    def rd_kafka_msg_partitioner_consistent_random(*_arg0); end
    def rd_kafka_msg_partitioner_fnv1a(*_arg0); end
    def rd_kafka_msg_partitioner_fnv1a_random(*_arg0); end
    def rd_kafka_msg_partitioner_murmur2(*_arg0); end
    def rd_kafka_msg_partitioner_murmur2_random(*_arg0); end
    def rd_kafka_msg_partitioner_random(*_arg0); end
    def rd_kafka_name(*_arg0); end
    def rd_kafka_new(*_arg0); end
    def rd_kafka_oauthbearer_set_token(*_arg0); end
    def rd_kafka_oauthbearer_set_token_failure(*_arg0); end
    def rd_kafka_offsets_for_times(*_arg0); end
    def rd_kafka_offsets_store(*_arg0); end
    def rd_kafka_outq_len(*_arg0); end
    def rd_kafka_pause_partitions(*_arg0); end
    def rd_kafka_poll(*_arg0); end
    def rd_kafka_poll_set_consumer(*_arg0); end
    def rd_kafka_position(*_arg0); end

    # source://ffi/1.17.0/lib/ffi/variadic.rb#61
    def rd_kafka_producev(*args); end

    def rd_kafka_purge(*_arg0); end
    def rd_kafka_query_watermark_offsets(*_arg0); end
    def rd_kafka_queue_destroy(*_arg0); end
    def rd_kafka_queue_get_background(*_arg0); end
    def rd_kafka_queue_get_main(*_arg0); end
    def rd_kafka_rebalance_protocol(*_arg0); end
    def rd_kafka_resume_partitions(*_arg0); end
    def rd_kafka_seek(*_arg0); end
    def rd_kafka_send_offsets_to_transaction(*_arg0); end
    def rd_kafka_set_log_queue(*_arg0); end
    def rd_kafka_subscribe(*_arg0); end
    def rd_kafka_subscription(*_arg0); end
    def rd_kafka_topic_conf_new(*_arg0); end
    def rd_kafka_topic_conf_set(*_arg0); end
    def rd_kafka_topic_destroy(*_arg0); end
    def rd_kafka_topic_name(*_arg0); end
    def rd_kafka_topic_new(*_arg0); end
    def rd_kafka_topic_partition_list_add(*_arg0); end
    def rd_kafka_topic_partition_list_copy(*_arg0); end
    def rd_kafka_topic_partition_list_destroy(*_arg0); end
    def rd_kafka_topic_partition_list_new(*_arg0); end
    def rd_kafka_topic_partition_list_set_offset(*_arg0); end
    def rd_kafka_topic_result_error(*_arg0); end
    def rd_kafka_topic_result_error_string(*_arg0); end
    def rd_kafka_topic_result_name(*_arg0); end
    def rd_kafka_unsubscribe(*_arg0); end
  end
end

# Configs management
#
# Structs for management of configurations
# Each configuration is attached to a resource and one resource can have many configuration
# details. Each resource will also have separate errors results if obtaining configuration
# was not possible for any reason
#
# source://karafka-rdkafka//lib/rdkafka/bindings.rb#107
class Rdkafka::Bindings::ConfigResource < ::FFI::Struct; end

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#188
Rdkafka::Bindings::LogCallback = T.let(T.unsafe(nil), FFI::Function)

# Message struct
#
# source://karafka-rdkafka//lib/rdkafka/bindings.rb#58
class Rdkafka::Bindings::Message < ::FFI::Struct; end

# Extracting data from group results
#
# source://karafka-rdkafka//lib/rdkafka/bindings.rb#542
class Rdkafka::Bindings::NativeError < ::FFI::Struct; end

# Errors
#
# source://karafka-rdkafka//lib/rdkafka/bindings.rb#144
class Rdkafka::Bindings::NativeErrorDesc < ::FFI::Struct; end

# The OAuth callback is currently global and contextless.
# This means that the callback will be called for all instances, and the callback must be able to determine to which instance it is associated.
# The instance name will be provided in the callback, allowing the callback to reference the correct instance.
#
# An example of how to use the instance name in the callback is given below.
# The `refresh_token` is configured as the `oauthbearer_token_refresh_callback`.
# `instances` is a map of client names to client instances, maintained by the user.
#
# ```
#   def refresh_token(config, client_name)
#     client = instances[client_name]
#     client.oauthbearer_set_token(
#       token: 'new-token-value',
#       lifetime_ms: token-lifetime-ms,
#       principal_name: 'principal-name'
#     )
#   end
# ```
#
# source://karafka-rdkafka//lib/rdkafka/bindings.rb#251
Rdkafka::Bindings::OAuthbearerTokenRefreshCallback = T.let(T.unsafe(nil), FFI::Function)

# Partitioner
#
# source://karafka-rdkafka//lib/rdkafka/bindings.rb#371
Rdkafka::Bindings::PARTITIONERS = T.let(T.unsafe(nil), Hash)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#514
Rdkafka::Bindings::RD_KAFKA_ACL_OPERATION_ALL = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#519
Rdkafka::Bindings::RD_KAFKA_ACL_OPERATION_ALTER = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#523
Rdkafka::Bindings::RD_KAFKA_ACL_OPERATION_ALTER_CONFIGS = T.let(T.unsafe(nil), Integer)

# rd_kafka_AclOperation_t - https://github.com/confluentinc/librdkafka/blob/292d2a66b9921b783f08147807992e603c7af059/src/rdkafka.h#L8403
#
# source://karafka-rdkafka//lib/rdkafka/bindings.rb#513
Rdkafka::Bindings::RD_KAFKA_ACL_OPERATION_ANY = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#521
Rdkafka::Bindings::RD_KAFKA_ACL_OPERATION_CLUSTER_ACTION = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#517
Rdkafka::Bindings::RD_KAFKA_ACL_OPERATION_CREATE = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#518
Rdkafka::Bindings::RD_KAFKA_ACL_OPERATION_DELETE = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#520
Rdkafka::Bindings::RD_KAFKA_ACL_OPERATION_DESCRIBE = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#522
Rdkafka::Bindings::RD_KAFKA_ACL_OPERATION_DESCRIBE_CONFIGS = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#524
Rdkafka::Bindings::RD_KAFKA_ACL_OPERATION_IDEMPOTENT_WRITE = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#515
Rdkafka::Bindings::RD_KAFKA_ACL_OPERATION_READ = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#516
Rdkafka::Bindings::RD_KAFKA_ACL_OPERATION_WRITE = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#530
Rdkafka::Bindings::RD_KAFKA_ACL_PERMISSION_TYPE_ALLOW = T.let(T.unsafe(nil), Integer)

# rd_kafka_AclPermissionType_t - https://github.com/confluentinc/librdkafka/blob/292d2a66b9921b783f08147807992e603c7af059/src/rdkafka.h#L8435
#
# source://karafka-rdkafka//lib/rdkafka/bindings.rb#528
Rdkafka::Bindings::RD_KAFKA_ACL_PERMISSION_TYPE_ANY = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#529
Rdkafka::Bindings::RD_KAFKA_ACL_PERMISSION_TYPE_DENY = T.let(T.unsafe(nil), Integer)

# Create Acls
#
# source://karafka-rdkafka//lib/rdkafka/bindings.rb#457
Rdkafka::Bindings::RD_KAFKA_ADMIN_OP_CREATEACLS = T.let(T.unsafe(nil), Integer)

# Create partitions
#
# source://karafka-rdkafka//lib/rdkafka/bindings.rb#412
Rdkafka::Bindings::RD_KAFKA_ADMIN_OP_CREATEPARTITIONS = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#413
Rdkafka::Bindings::RD_KAFKA_ADMIN_OP_CREATEPARTITIONS_RESULT = T.let(T.unsafe(nil), Integer)

# Create Topics
#
# source://karafka-rdkafka//lib/rdkafka/bindings.rb#390
Rdkafka::Bindings::RD_KAFKA_ADMIN_OP_CREATETOPICS = T.let(T.unsafe(nil), Integer)

# Delete Acls
#
# source://karafka-rdkafka//lib/rdkafka/bindings.rb#466
Rdkafka::Bindings::RD_KAFKA_ADMIN_OP_DELETEACLS = T.let(T.unsafe(nil), Integer)

# Delete Group
#
# source://karafka-rdkafka//lib/rdkafka/bindings.rb#423
Rdkafka::Bindings::RD_KAFKA_ADMIN_OP_DELETEGROUPS = T.let(T.unsafe(nil), Integer)

# Delete Topics
#
# source://karafka-rdkafka//lib/rdkafka/bindings.rb#402
Rdkafka::Bindings::RD_KAFKA_ADMIN_OP_DELETETOPICS = T.let(T.unsafe(nil), Integer)

# Describe Acls
#
# source://karafka-rdkafka//lib/rdkafka/bindings.rb#477
Rdkafka::Bindings::RD_KAFKA_ADMIN_OP_DESCRIBEACLS = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#132
Rdkafka::Bindings::RD_KAFKA_ADMIN_OP_DESCRIBECONFIGS = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#135
Rdkafka::Bindings::RD_KAFKA_ADMIN_OP_INCREMENTALALTERCONFIGS = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#140
Rdkafka::Bindings::RD_KAFKA_ALTER_CONFIG_OP_TYPE_APPEND = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#139
Rdkafka::Bindings::RD_KAFKA_ALTER_CONFIG_OP_TYPE_DELETE = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#138
Rdkafka::Bindings::RD_KAFKA_ALTER_CONFIG_OP_TYPE_SET = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#141
Rdkafka::Bindings::RD_KAFKA_ALTER_CONFIG_OP_TYPE_SUBTRACT = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#458
Rdkafka::Bindings::RD_KAFKA_EVENT_CREATEACLS_RESULT = T.let(T.unsafe(nil), Integer)

# rd_kafka_admin_op_t
#
# source://karafka-rdkafka//lib/rdkafka/bindings.rb#391
Rdkafka::Bindings::RD_KAFKA_EVENT_CREATETOPICS_RESULT = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#467
Rdkafka::Bindings::RD_KAFKA_EVENT_DELETEACLS_RESULT = T.let(T.unsafe(nil), Integer)

# rd_kafka_admin_op_t
#
# source://karafka-rdkafka//lib/rdkafka/bindings.rb#424
Rdkafka::Bindings::RD_KAFKA_EVENT_DELETEGROUPS_RESULT = T.let(T.unsafe(nil), Integer)

# rd_kafka_admin_op_t
#
# source://karafka-rdkafka//lib/rdkafka/bindings.rb#403
Rdkafka::Bindings::RD_KAFKA_EVENT_DELETETOPICS_RESULT = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#478
Rdkafka::Bindings::RD_KAFKA_EVENT_DESCRIBEACLS_RESULT = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#133
Rdkafka::Bindings::RD_KAFKA_EVENT_DESCRIBECONFIGS_RESULT = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#136
Rdkafka::Bindings::RD_KAFKA_EVENT_INCREMENTALALTERCONFIGS_RESULT = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#358
Rdkafka::Bindings::RD_KAFKA_MSG_F_COPY = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#34
Rdkafka::Bindings::RD_KAFKA_OFFSET_BEGINNING = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#33
Rdkafka::Bindings::RD_KAFKA_OFFSET_END = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#36
Rdkafka::Bindings::RD_KAFKA_OFFSET_INVALID = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#35
Rdkafka::Bindings::RD_KAFKA_OFFSET_STORED = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#356
Rdkafka::Bindings::RD_KAFKA_PURGE_F_INFLIGHT = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#355
Rdkafka::Bindings::RD_KAFKA_PURGE_F_QUEUE = T.let(T.unsafe(nil), Integer)

# rd_kafka_ResourceType_t - https://github.com/confluentinc/librdkafka/blob/292d2a66b9921b783f08147807992e603c7af059/src/rdkafka.h#L7307
#
# source://karafka-rdkafka//lib/rdkafka/bindings.rb#499
Rdkafka::Bindings::RD_KAFKA_RESOURCE_ANY = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#502
Rdkafka::Bindings::RD_KAFKA_RESOURCE_BROKER = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#501
Rdkafka::Bindings::RD_KAFKA_RESOURCE_GROUP = T.let(T.unsafe(nil), Integer)

# rd_kafka_ResourcePatternType_t - https://github.com/confluentinc/librdkafka/blob/292d2a66b9921b783f08147807992e603c7af059/src/rdkafka.h#L7320
#
# source://karafka-rdkafka//lib/rdkafka/bindings.rb#506
Rdkafka::Bindings::RD_KAFKA_RESOURCE_PATTERN_ANY = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#508
Rdkafka::Bindings::RD_KAFKA_RESOURCE_PATTERN_LITERAL = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#507
Rdkafka::Bindings::RD_KAFKA_RESOURCE_PATTERN_MATCH = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#509
Rdkafka::Bindings::RD_KAFKA_RESOURCE_PATTERN_PREFIXED = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#500
Rdkafka::Bindings::RD_KAFKA_RESOURCE_TOPIC = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#31
Rdkafka::Bindings::RD_KAFKA_RESP_ERR_NO_ERROR = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#27
Rdkafka::Bindings::RD_KAFKA_RESP_ERR__ASSIGN_PARTITIONS = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#30
Rdkafka::Bindings::RD_KAFKA_RESP_ERR__NOENT = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#28
Rdkafka::Bindings::RD_KAFKA_RESP_ERR__REVOKE_PARTITIONS = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#29
Rdkafka::Bindings::RD_KAFKA_RESP_ERR__STATE = T.let(T.unsafe(nil), Integer)

# Producer
#
# source://karafka-rdkafka//lib/rdkafka/bindings.rb#344
Rdkafka::Bindings::RD_KAFKA_VTYPE_END = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#353
Rdkafka::Bindings::RD_KAFKA_VTYPE_HEADER = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#354
Rdkafka::Bindings::RD_KAFKA_VTYPE_HEADERS = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#349
Rdkafka::Bindings::RD_KAFKA_VTYPE_KEY = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#351
Rdkafka::Bindings::RD_KAFKA_VTYPE_MSGFLAGS = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#350
Rdkafka::Bindings::RD_KAFKA_VTYPE_OPAQUE = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#347
Rdkafka::Bindings::RD_KAFKA_VTYPE_PARTITION = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#346
Rdkafka::Bindings::RD_KAFKA_VTYPE_RKT = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#352
Rdkafka::Bindings::RD_KAFKA_VTYPE_TIMESTAMP = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#345
Rdkafka::Bindings::RD_KAFKA_VTYPE_TOPIC = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#348
Rdkafka::Bindings::RD_KAFKA_VTYPE_VALUE = T.let(T.unsafe(nil), Integer)

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#38
class Rdkafka::Bindings::SizePtr < ::FFI::Struct; end

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#210
Rdkafka::Bindings::StatsCallback = T.let(T.unsafe(nil), FFI::Function)

# TopicPartition ad TopicPartitionList structs
#
# source://karafka-rdkafka//lib/rdkafka/bindings.rb#78
class Rdkafka::Bindings::TopicPartition < ::FFI::Struct; end

# source://karafka-rdkafka//lib/rdkafka/bindings.rb#89
class Rdkafka::Bindings::TopicPartitionList < ::FFI::Struct; end

# source://karafka-rdkafka//lib/rdkafka/callbacks.rb#4
module Rdkafka::Callbacks; end

# @private
#
# source://karafka-rdkafka//lib/rdkafka/callbacks.rb#160
class Rdkafka::Callbacks::BackgroundEventCallback
  class << self
    # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#161
    def call(_, event_ptr, _); end

    # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#294
    def process_create_acl(event_ptr); end

    # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#276
    def process_create_partitions(event_ptr); end

    # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#186
    def process_create_topic(event_ptr); end

    # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#311
    def process_delete_acl(event_ptr); end

    # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#240
    def process_delete_groups(event_ptr); end

    # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#258
    def process_delete_topic(event_ptr); end

    # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#333
    def process_describe_acl(event_ptr); end

    # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#204
    def process_describe_configs(event_ptr); end

    # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#222
    def process_incremental_alter_configs(event_ptr); end
  end
end

# FFI Function used for Create Topic and Delete Topic callbacks
#
# source://karafka-rdkafka//lib/rdkafka/callbacks.rb#153
Rdkafka::Callbacks::BackgroundEventCallbackFunction = T.let(T.unsafe(nil), FFI::Function)

# Extracts attributes of rd_kafka_acl_result_t
#
# @private
#
# source://karafka-rdkafka//lib/rdkafka/callbacks.rb#52
class Rdkafka::Callbacks::CreateAclResult
  # @return [CreateAclResult] a new instance of CreateAclResult
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#55
  def initialize(acl_result_pointer); end

  # Returns the value of attribute error_string.
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#53
  def error_string; end

  # Returns the value of attribute result_error.
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#53
  def result_error; end

  class << self
    # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#61
    def create_acl_results_from_array(count, array_pointer); end
  end
end

# Extracts attributes of rd_kafka_DeleteAcls_result_response_t
#
# @private
#
# source://karafka-rdkafka//lib/rdkafka/callbacks.rb#72
class Rdkafka::Callbacks::DeleteAclResult
  # @return [DeleteAclResult] a new instance of DeleteAclResult
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#75
  def initialize(acl_result_pointer); end

  # Returns the value of attribute error_string.
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#73
  def error_string; end

  # Returns the value of attribute matching_acls.
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#73
  def matching_acls; end

  # Returns the value of attribute matching_acls_count.
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#73
  def matching_acls_count; end

  # Returns the value of attribute result_error.
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#73
  def result_error; end

  class << self
    # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#88
    def delete_acl_results_from_array(count, array_pointer); end
  end
end

# @private
#
# source://karafka-rdkafka//lib/rdkafka/callbacks.rb#359
class Rdkafka::Callbacks::DeliveryCallback
  class << self
    # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#360
    def call(_, message_ptr, opaque_ptr); end
  end
end

# FFI Function used for Message Delivery callbacks
#
# source://karafka-rdkafka//lib/rdkafka/callbacks.rb#352
Rdkafka::Callbacks::DeliveryCallbackFunction = T.let(T.unsafe(nil), FFI::Function)

# Extracts attributes of rd_kafka_DeleteAcls_result_response_t
#
# @private
#
# source://karafka-rdkafka//lib/rdkafka/callbacks.rb#99
class Rdkafka::Callbacks::DescribeAclResult
  # @return [DescribeAclResult] a new instance of DescribeAclResult
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#102
  def initialize(event_ptr); end

  # Returns the value of attribute error_string.
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#100
  def error_string; end

  # Returns the value of attribute matching_acls.
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#100
  def matching_acls; end

  # Returns the value of attribute matching_acls_count.
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#100
  def matching_acls_count; end

  # Returns the value of attribute result_error.
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#100
  def result_error; end
end

# source://karafka-rdkafka//lib/rdkafka/callbacks.rb#116
class Rdkafka::Callbacks::DescribeConfigsResult
  # @return [DescribeConfigsResult] a new instance of DescribeConfigsResult
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#119
  def initialize(event_ptr); end

  # Returns the value of attribute error_string.
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#117
  def error_string; end

  # Returns the value of attribute result_error.
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#117
  def result_error; end

  # Returns the value of attribute results.
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#117
  def results; end

  # Returns the value of attribute results_count.
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#117
  def results_count; end
end

# source://karafka-rdkafka//lib/rdkafka/callbacks.rb#26
class Rdkafka::Callbacks::GroupResult
  # @return [GroupResult] a new instance of GroupResult
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#28
  def initialize(group_result_pointer); end

  # Returns the value of attribute error_string.
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#27
  def error_string; end

  # Returns the value of attribute result_error.
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#27
  def result_error; end

  # Returns the value of attribute result_name.
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#27
  def result_name; end

  class << self
    # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#41
    def create_group_results_from_array(count, array_pointer); end
  end
end

# source://karafka-rdkafka//lib/rdkafka/callbacks.rb#134
class Rdkafka::Callbacks::IncrementalAlterConfigsResult
  # @return [IncrementalAlterConfigsResult] a new instance of IncrementalAlterConfigsResult
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#137
  def initialize(event_ptr); end

  # Returns the value of attribute error_string.
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#135
  def error_string; end

  # Returns the value of attribute result_error.
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#135
  def result_error; end

  # Returns the value of attribute results.
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#135
  def results; end

  # Returns the value of attribute results_count.
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#135
  def results_count; end
end

# Extracts attributes of a rd_kafka_topic_result_t
#
# @private
#
# source://karafka-rdkafka//lib/rdkafka/callbacks.rb#9
class Rdkafka::Callbacks::TopicResult
  # @return [TopicResult] a new instance of TopicResult
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#12
  def initialize(topic_result_pointer); end

  # Returns the value of attribute error_string.
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#10
  def error_string; end

  # Returns the value of attribute result_error.
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#10
  def result_error; end

  # Returns the value of attribute result_name.
  #
  # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#10
  def result_name; end

  class << self
    # source://karafka-rdkafka//lib/rdkafka/callbacks.rb#18
    def create_topic_results_from_array(count, array_pointer); end
  end
end

# Error class for public consumer method calls on a closed admin.
#
# source://karafka-rdkafka//lib/rdkafka/error.rb#160
class Rdkafka::ClosedAdminError < ::Rdkafka::BaseError
  # @return [ClosedAdminError] a new instance of ClosedAdminError
  #
  # source://karafka-rdkafka//lib/rdkafka/error.rb#161
  def initialize(method); end
end

# Error class for public consumer method calls on a closed consumer.
#
# source://karafka-rdkafka//lib/rdkafka/error.rb#146
class Rdkafka::ClosedConsumerError < ::Rdkafka::BaseError
  # @return [ClosedConsumerError] a new instance of ClosedConsumerError
  #
  # source://karafka-rdkafka//lib/rdkafka/error.rb#147
  def initialize(method); end
end

# source://karafka-rdkafka//lib/rdkafka/error.rb#166
class Rdkafka::ClosedInnerError < ::Rdkafka::BaseError
  # @return [ClosedInnerError] a new instance of ClosedInnerError
  #
  # source://karafka-rdkafka//lib/rdkafka/error.rb#167
  def initialize; end
end

# Error class for public producer method calls on a closed producer.
#
# source://karafka-rdkafka//lib/rdkafka/error.rb#153
class Rdkafka::ClosedProducerError < ::Rdkafka::BaseError
  # @return [ClosedProducerError] a new instance of ClosedProducerError
  #
  # source://karafka-rdkafka//lib/rdkafka/error.rb#154
  def initialize(method); end
end

# Configuration for a Kafka consumer or producer. You can create an instance and use
# the consumer and producer methods to create a client. Documentation of the available
# configuration options is available on https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md.
#
# source://karafka-rdkafka//lib/rdkafka/config.rb#7
class Rdkafka::Config
  # Returns a new config with the provided options which are merged with {DEFAULT_CONFIG}.
  #
  # @param config_hash [Hash{String,Symbol => String}] The config options for rdkafka
  # @return [Config]
  #
  # source://karafka-rdkafka//lib/rdkafka/config.rb#148
  def initialize(config_hash = T.unsafe(nil)); end

  # Get a config option with the specified key
  #
  # @param key [String] The config option's key
  # @return [String, nil] The config option or `nil` if it is not present
  #
  # source://karafka-rdkafka//lib/rdkafka/config.rb#169
  def [](key); end

  # Set a config option.
  #
  # @param key [String] The config option's key
  # @param value [String] The config option's value
  # @return [nil]
  #
  # source://karafka-rdkafka//lib/rdkafka/config.rb#160
  def []=(key, value); end

  # Creates an admin instance with this configuration.
  #
  # @param native_kafka_auto_start [Boolean] should the native kafka operations be started
  #   automatically. Defaults to true. Set to false only when doing complex initialization.
  # @raise [ConfigError] When the configuration contains invalid options
  # @raise [ClientCreationError] When the native client cannot be created
  # @return [Admin] The created admin instance
  #
  # source://karafka-rdkafka//lib/rdkafka/config.rb#271
  def admin(native_kafka_auto_start: T.unsafe(nil)); end

  # Creates a consumer with this configuration.
  #
  # @param native_kafka_auto_start [Boolean] should the native kafka operations be started
  #   automatically. Defaults to true. Set to false only when doing complex initialization.
  # @raise [ConfigError] When the configuration contains invalid options
  # @raise [ClientCreationError] When the native client cannot be created
  # @return [Consumer] The created consumer
  #
  # source://karafka-rdkafka//lib/rdkafka/config.rb#204
  def consumer(native_kafka_auto_start: T.unsafe(nil)); end

  # Should we use a single queue for the underlying consumer and events.
  #
  # This is an advanced API that allows for more granular control of the polling process.
  # When this value is set to `false` (`true` by defualt), there will be two queues that need to
  # be polled:
  #   - main librdkafka queue for events
  #   - consumer queue with messages and rebalances
  #
  # It is recommended to use the defaults and only set it to `false` in advance multi-threaded
  # and complex cases where granular events handling control is needed.
  #
  # @param poll_set [Boolean]
  #
  # source://karafka-rdkafka//lib/rdkafka/config.rb#192
  def consumer_poll_set=(poll_set); end

  # Get notifications on partition assignment/revocation for the subscribed topics
  #
  # @param listener [Object, #on_partitions_assigned, #on_partitions_revoked] listener instance
  #
  # source://karafka-rdkafka//lib/rdkafka/config.rb#176
  def consumer_rebalance_listener=(listener); end

  # Create a producer with this configuration.
  #
  # @param native_kafka_auto_start [Boolean] should the native kafka operations be started
  #   automatically. Defaults to true. Set to false only when doing complex initialization.
  # @raise [ConfigError] When the configuration contains invalid options
  # @raise [ClientCreationError] When the native client cannot be created
  # @return [Producer] The created producer
  #
  # source://karafka-rdkafka//lib/rdkafka/config.rb#238
  def producer(native_kafka_auto_start: T.unsafe(nil)); end

  private

  # This method is only intended to be used to create a client,
  # using it in another way will leak memory.
  #
  # source://karafka-rdkafka//lib/rdkafka/config.rb#301
  def native_config(opaque = T.unsafe(nil)); end

  # source://karafka-rdkafka//lib/rdkafka/config.rb#343
  def native_kafka(config, type); end

  class << self
    # Makes sure that there is a thread for consuming logs
    # We do not spawn thread immediately and we need to check if it operates to support forking
    #
    # source://karafka-rdkafka//lib/rdkafka/config.rb#35
    def ensure_log_thread; end

    # Returns the current error callback, by default this is nil.
    #
    # @return [Proc, nil]
    #
    # source://karafka-rdkafka//lib/rdkafka/config.rb#104
    def error_callback; end

    # Set a callback that will be called every time the underlying client emits an error.
    # If this callback is not set, global errors such as brokers becoming unavailable will only be sent to the logger, as defined by librdkafka.
    # The callback is called with an instance of RdKafka::Error.
    #
    # @param callback [Proc, #call] The callback
    # @raise [TypeError]
    # @return [nil]
    #
    # source://karafka-rdkafka//lib/rdkafka/config.rb#96
    def error_callback=(callback); end

    # Returns a queue whose contents will be passed to the configured logger. Each entry
    # should follow the format [Logger::Severity, String]. The benefit over calling the
    # logger directly is that this is safe to use from trap contexts.
    #
    # @return [Queue]
    #
    # source://karafka-rdkafka//lib/rdkafka/config.rb#56
    def log_queue; end

    # Returns the current logger, by default this is a logger to stdout.
    #
    # @return [Logger]
    #
    # source://karafka-rdkafka//lib/rdkafka/config.rb#29
    def logger; end

    # Set the logger that will be used for all logging output by this library.
    #
    # @param logger [Logger] The logger to be used
    # @raise [NoLoggerError]
    # @return [nil]
    #
    # source://karafka-rdkafka//lib/rdkafka/config.rb#65
    def logger=(logger); end

    # Returns the current oauthbearer_token_refresh_callback callback, by default this is nil.
    #
    # @return [Proc, nil]
    #
    # source://karafka-rdkafka//lib/rdkafka/config.rb#122
    def oauthbearer_token_refresh_callback; end

    # Sets the SASL/OAUTHBEARER token refresh callback.
    # This callback will be triggered when it is time to refresh the client's OAUTHBEARER token
    #
    # @param callback [Proc, #call] The callback
    # @raise [TypeError]
    # @return [nil]
    #
    # source://karafka-rdkafka//lib/rdkafka/config.rb#114
    def oauthbearer_token_refresh_callback=(callback); end

    # @private
    #
    # source://karafka-rdkafka//lib/rdkafka/config.rb#127
    def opaques; end

    # Returns the current statistics callback, by default this is nil.
    #
    # @return [Proc, nil]
    #
    # source://karafka-rdkafka//lib/rdkafka/config.rb#85
    def statistics_callback; end

    # Set a callback that will be called every time the underlying client emits statistics.
    # You can configure if and how often this happens using `statistics.interval.ms`.
    # The callback is called with a hash that's documented here: https://github.com/confluentinc/librdkafka/blob/master/STATISTICS.md
    #
    # @param callback [Proc, #call] The callback
    # @raise [TypeError]
    # @return [nil]
    #
    # source://karafka-rdkafka//lib/rdkafka/config.rb#77
    def statistics_callback=(callback); end
  end
end

# Error that is returned by the underlying rdkafka library if the client cannot be created.
#
# source://karafka-rdkafka//lib/rdkafka/config.rb#292
class Rdkafka::Config::ClientCreationError < ::RuntimeError; end

# Error that is returned by the underlying rdkafka error if an invalid configuration option is present.
#
# source://karafka-rdkafka//lib/rdkafka/config.rb#289
class Rdkafka::Config::ConfigError < ::RuntimeError; end

# Default config that can be overwritten.
#
# source://karafka-rdkafka//lib/rdkafka/config.rb#132
Rdkafka::Config::DEFAULT_CONFIG = T.let(T.unsafe(nil), Hash)

# Error that is raised when trying to set a nil logger
#
# source://karafka-rdkafka//lib/rdkafka/config.rb#295
class Rdkafka::Config::NoLoggerError < ::RuntimeError; end

# Required config that cannot be overwritten.
#
# source://karafka-rdkafka//lib/rdkafka/config.rb#138
Rdkafka::Config::REQUIRED_CONFIG = T.let(T.unsafe(nil), Hash)

# A consumer of Kafka messages. It uses the high-level consumer approach where the Kafka
# brokers automatically assign partitions and load balance partitions over consumers that
# have the same `:"group.id"` set in their configuration.
#
# To create a consumer set up a {Config} and call {Config#consumer consumer} on that. It is
# mandatory to set `:"group.id"` in the configuration.
#
# Consumer implements `Enumerable`, so you can use `each` to consume messages, or for example
# `each_slice` to consume batches of messages.
#
# source://karafka-rdkafka//lib/rdkafka/consumer.rb#13
class Rdkafka::Consumer
  include ::Enumerable
  include ::Rdkafka::Helpers::Time
  include ::Rdkafka::Helpers::OAuth

  # @private
  # @return [Consumer] a new instance of Consumer
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#19
  def initialize(native_kafka); end

  # Atomic assignment of partitions to consume
  #
  # @param list [TopicPartitionList] The topic with partitions to assign
  # @raise [RdkafkaError] When assigning fails
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#181
  def assign(list); end

  # Returns the current partition assignment.
  #
  # @raise [RdkafkaError] When getting the assignment fails.
  # @return [TopicPartitionList]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#205
  def assignment; end

  # @return [Boolean] true if our current assignment has been lost involuntarily.
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#229
  def assignment_lost?; end

  # Close this consumer
  #
  # @return [nil]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#42
  def close; end

  # Whether this consumer has closed
  #
  # @return [Boolean]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#54
  def closed?; end

  # Returns the ClusterId as reported in broker metadata.
  #
  # @return [String, nil]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#362
  def cluster_id; end

  # Manually commit the current offsets of this consumer.
  #
  # To use this set `enable.auto.commit`to `false` to disable automatic triggering
  # of commits.
  #
  # If `enable.auto.offset.store` is set to `true` the offset of the last consumed
  # message for every partition is used. If set to `false` you can use {store_offset} to
  # indicate when a message has been fully processed.
  #
  # @param list [TopicPartitionList, nil] The topic with partitions to commit
  # @param async [Boolean] Whether to commit async or wait for the commit to finish
  # @raise [RdkafkaError] When committing fails
  # @return [nil]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#508
  def commit(list = T.unsafe(nil), async = T.unsafe(nil)); end

  # Return the current committed offset per partition for this consumer group.
  # The offset field of each requested partition will either be set to stored offset or to -1001
  # in case there was no stored offset for that partition.
  #
  # @param list [TopicPartitionList, nil] The topic with partitions to get the offsets for or nil
  #   to use the current subscription.
  # @param timeout_ms [Integer] The timeout for fetching this information.
  # @raise [RdkafkaError] When getting the committed positions fails.
  # @return [TopicPartitionList]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#246
  def committed(list = T.unsafe(nil), timeout_ms = T.unsafe(nil)); end

  # Returns pointer to the consumer group metadata. It is used only in the context of
  # exactly-once-semantics in transactions, this is why it is never remapped to Ruby
  #
  # This API is **not** usable by itself from Ruby
  #
  # @note This pointer **needs** to be removed with `#rd_kafka_consumer_group_metadata_destroy`
  # @private
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#700
  def consumer_group_metadata_pointer; end

  # Poll for new messages and yield for each received one. Iteration
  # will end when the consumer is closed.
  #
  # If `enable.partition.eof` is turned on in the config this will raise an error when an eof is
  # reached, so you probably want to disable that when using this method of iteration.
  #
  # @raise [RdkafkaError] When polling fails
  # @return [nil]
  # @yieldparam message [Message] Received message
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#594
  def each; end

  # Poll for new messages and yield them in batches that may contain
  # messages from more than one partition.
  #
  # Rather than yield each message immediately as soon as it is received,
  # each_batch will attempt to wait for as long as `timeout_ms` in order
  # to create a batch of up to but no more than `max_items` in size.
  #
  # Said differently, if more than `max_items` are available within
  # `timeout_ms`, then `each_batch` will yield early with `max_items` in the
  # array, but if `timeout_ms` passes by with fewer messages arriving, it
  # will yield an array of fewer messages, quite possibly zero.
  #
  # In order to prevent wrongly auto committing many messages at once across
  # possibly many partitions, callers must explicitly indicate which messages
  # have been successfully processed as some consumed messages may not have
  # been yielded yet. To do this, the caller should set
  # `enable.auto.offset.store` to false and pass processed messages to
  # {store_offset}. It is also possible, though more complex, to set
  # 'enable.auto.commit' to false and then pass a manually assembled
  # TopicPartitionList to {commit}.
  #
  # As with `each`, iteration will end when the consumer is closed.
  #
  # Exception behavior is more complicated than with `each`, in that if
  # :yield_on_error is true, and an exception is raised during the
  # poll, and messages have already been received, they will be yielded to
  # the caller before the exception is allowed to propagate.
  #
  # If you are setting either auto.commit or auto.offset.store to false in
  # the consumer configuration, then you should let yield_on_error keep its
  # default value of false because you are guaranteed to see these messages
  # again. However, if both auto.commit and auto.offset.store are set to
  # true, you should set yield_on_error to true so you can process messages
  # that you may or may not see again.
  #
  # which will be propagated after processing of the partial batch is complete.
  #
  # @param max_items [Integer] Maximum size of the yielded array of messages
  # @param bytes_threshold [Integer] Threshold number of total message bytes in the yielded array of messages
  # @param timeout_ms [Integer] max time to wait for up to max_items
  # @raise [RdkafkaError] When polling fails
  # @return [nil]
  # @yield [messages, pending_exception]
  # @yieldparam messages [Array] An array of received Message
  # @yieldparam pending_exception [Exception] normally nil, or an exception
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#657
  def each_batch(max_items: T.unsafe(nil), bytes_threshold: T.unsafe(nil), timeout_ms: T.unsafe(nil), yield_on_error: T.unsafe(nil), &block); end

  # Polls the main rdkafka queue (not the consumer one). Do **NOT** use it if `consumer_poll_set`
  #   was set to `true`.
  #
  # Events will cause application-provided callbacks to be called.
  #
  # Events (in the context of the consumer):
  #   - error callbacks
  #   - stats callbacks
  #   - any other callbacks supported by librdkafka that are not part of the consumer_poll, that
  #     would have a callback configured and activated.
  #
  # This method needs to be called at regular intervals to serve any queued callbacks waiting to
  # be called. When in use, does **NOT** replace `#poll` but needs to run complementary with it.
  #
  # @note This method technically should be called `#poll` and the current `#poll` should be
  #   called `#consumer_poll` though we keep the current naming convention to make it backward
  #   compatible.
  # @param timeout_ms [Integer] poll timeout. If set to 0 will run async, when set to -1 will
  #   block until any events available.
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#579
  def events_poll(timeout_ms = T.unsafe(nil)); end

  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#36
  def finalizer; end

  # Calculate the consumer lag per partition for the provided topic partition list.
  # You can get a suitable list by calling {committed} or {position} (TODO). It is also
  # possible to create one yourself, in this case you have to provide a list that
  # already contains all the partitions you need the lag for.
  #
  # @param topic_partition_list [TopicPartitionList] The list to calculate lag for.
  # @param watermark_timeout_ms [Integer] The timeout for each query watermark call.
  # @raise [RdkafkaError] When querying the broker fails.
  # @return [Hash<String, Hash<Integer, Integer>>] A hash containing all topics with the lag
  #   per partition
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#338
  def lag(topic_partition_list, watermark_timeout_ms = T.unsafe(nil)); end

  # Returns this client's broker-assigned group member id
  #
  # This currently requires the high-level KafkaConsumer
  #
  # @return [String, nil]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#374
  def member_id; end

  # @return [String] consumer name
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#30
  def name; end

  # Lookup offset for the given partitions by timestamp.
  #
  # @param list [TopicPartitionList] The TopicPartitionList with timestamps instead of offsets
  # @raise [RdKafkaError] When the OffsetForTimes lookup fails
  # @return [TopicPartitionList]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#471
  def offsets_for_times(list, timeout_ms = T.unsafe(nil)); end

  # Pause producing or consumption for the provided list of partitions
  #
  # @param list [TopicPartitionList] The topic with partitions to pause
  # @raise [RdkafkaTopicPartitionListError] When pausing subscription fails.
  # @return [nil]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#104
  def pause(list); end

  # Poll for the next message on one of the subscribed topics
  #
  # @param timeout_ms [Integer] Timeout of this poll
  # @raise [RdkafkaError] When polling fails
  # @return [Message, nil] A message or nil if there was no new message within the timeout
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#535
  def poll(timeout_ms); end

  # Return the current positions (offsets) for topics and partitions.
  # The offset field of each requested partition will be set to the offset of the last consumed message + 1, or nil in case there was no previous message.
  #
  # @param list [TopicPartitionList, nil] The topic with partitions to get the offsets for or nil to use the current subscription.
  # @raise [RdkafkaError] When getting the positions fails.
  # @return [TopicPartitionList]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#278
  def position(list = T.unsafe(nil)); end

  # Query broker for low (oldest/beginning) and high (newest/end) offsets for a partition.
  #
  # @param topic [String] The topic to query
  # @param partition [Integer] The partition to query
  # @param timeout_ms [Integer] The timeout for querying the broker
  # @raise [RdkafkaError] When querying the broker fails.
  # @return [Integer] The low and high watermark
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#303
  def query_watermark_offsets(topic, partition, timeout_ms = T.unsafe(nil)); end

  # Resumes producing consumption for the provided list of partitions
  #
  # @param list [TopicPartitionList] The topic with partitions to pause
  # @raise [RdkafkaError] When resume subscription fails.
  # @return [nil]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#132
  def resume(list); end

  # Seek to a particular message. The next poll on the topic/partition will return the
  # message at the given offset.
  #
  # @param message [Rdkafka::Consumer::Message] The message to which to seek
  # @raise [RdkafkaError] When seeking fails
  # @return [nil]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#437
  def seek(message); end

  # Starts the native Kafka polling thread and kicks off the init polling
  #
  # @note Not needed to run unless explicit start was disabled
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#25
  def start; end

  # Store offset of a message to be used in the next commit of this consumer
  #
  # When using this `enable.auto.offset.store` should be set to `false` in the config.
  #
  # @param message [Rdkafka::Consumer::Message] The message which offset will be stored
  # @param metadata [String, nil] commit metadata string or nil if none
  # @raise [RdkafkaError] When storing the offset fails
  # @return [nil]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#389
  def store_offset(message, metadata = T.unsafe(nil)); end

  # Subscribes to one or more topics letting Kafka handle partition assignments.
  #
  # @param topics [Array<String>] One or more topic names
  # @raise [RdkafkaError] When subscribing fails
  # @return [nil]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#63
  def subscribe(*topics); end

  # Returns the current subscription to topics and partitions
  #
  # @raise [RdkafkaError] When getting the subscription fails.
  # @return [TopicPartitionList]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#158
  def subscription; end

  # Unsubscribe from all subscribed topics.
  #
  # @raise [RdkafkaError] When unsubscribing fails
  # @return [nil]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#87
  def unsubscribe; end

  private

  # @raise [Rdkafka::ClosedConsumerError]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer.rb#710
  def closed_consumer_check(method); end
end

# Interface to return headers for a consumer message
#
# source://karafka-rdkafka//lib/rdkafka/consumer/headers.rb#6
module Rdkafka::Consumer::Headers
  class << self
    # Reads a librdkafka native message's headers and returns them as a Ruby Hash
    #
    # @param native_message [Rdkafka::Bindings::Message]
    # @private
    # @raise [Rdkafka::RdkafkaError] when fail to read headers
    # @return [Hash<String, String>] headers Hash for the native_message
    #
    # source://karafka-rdkafka//lib/rdkafka/consumer/headers.rb#26
    def from_native(native_message); end
  end
end

# source://karafka-rdkafka//lib/rdkafka/consumer/headers.rb#7
class Rdkafka::Consumer::Headers::HashWithSymbolKeysTreatedLikeStrings < ::Hash
  # source://karafka-rdkafka//lib/rdkafka/consumer/headers.rb#8
  def [](key); end
end

# A message that was consumed from a topic.
#
# source://karafka-rdkafka//lib/rdkafka/consumer/message.rb#6
class Rdkafka::Consumer::Message
  # @private
  # @return [Message] a new instance of Message
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer/message.rb#35
  def initialize(native_message); end

  # @return [Hash<String, String>] a message headers
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer/message.rb#32
  def headers; end

  # This message's key
  #
  # @return [String, nil]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer/message.rb#21
  def key; end

  # This message's offset in its partition
  #
  # @return [Integer]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer/message.rb#25
  def offset; end

  # The partition this message was consumed from
  #
  # @return [Integer]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer/message.rb#13
  def partition; end

  # This message's payload
  #
  # @return [String, nil]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer/message.rb#17
  def payload; end

  # This message's timestamp, if provided by the broker
  #
  # @return [Time, nil]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer/message.rb#29
  def timestamp; end

  # Human readable representation of this message.
  #
  # @return [String]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer/message.rb#68
  def to_s; end

  # The topic this message was consumed from
  #
  # @return [String]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer/message.rb#9
  def topic; end

  # source://karafka-rdkafka//lib/rdkafka/consumer/message.rb#74
  def truncate(string); end
end

# Information about a partition, used in {TopicPartitionList}.
#
# source://karafka-rdkafka//lib/rdkafka/consumer/partition.rb#6
class Rdkafka::Consumer::Partition
  # @private
  # @return [Partition] a new instance of Partition
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer/partition.rb#24
  def initialize(partition, offset, err = T.unsafe(nil), metadata = T.unsafe(nil)); end

  # Whether another partition is equal to this
  #
  # @return [Boolean]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer/partition.rb#50
  def ==(other); end

  # Partition's error code
  #
  # @return [Integer]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer/partition.rb#17
  def err; end

  # Human readable representation of this partition.
  #
  # @return [String]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer/partition.rb#44
  def inspect; end

  # Partition metadata in the context of a consumer
  #
  # @return [String, nil]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer/partition.rb#21
  def metadata; end

  # Partition's offset
  #
  # @return [Integer, nil]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer/partition.rb#13
  def offset; end

  # Partition number
  #
  # @return [Integer]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer/partition.rb#9
  def partition; end

  # Human readable representation of this partition.
  #
  # @return [String]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer/partition.rb#33
  def to_s; end
end

# A list of topics with their partition information
#
# source://karafka-rdkafka//lib/rdkafka/consumer/topic_partition_list.rb#6
class Rdkafka::Consumer::TopicPartitionList
  # Create a topic partition list.
  #
  # @param data [Hash{String => nil,Partition}] The topic and partition data or nil to create an empty list
  # @return [TopicPartitionList]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer/topic_partition_list.rb#12
  def initialize(data = T.unsafe(nil)); end

  # source://karafka-rdkafka//lib/rdkafka/consumer/topic_partition_list.rb#92
  def ==(other); end

  # Add a topic with optionally partitions to the list.
  # Calling this method multiple times for the same topic will overwrite the previous configuraton.
  #
  # @example Add a topic with unassigned partitions
  #   tpl.add_topic("topic")
  # @example Add a topic with assigned partitions
  #   tpl.add_topic("topic", (0..8))
  # @example Add a topic with all topics up to a count
  #   tpl.add_topic("topic", 9)
  # @param topic [String] The topic's name
  # @param partitions [Array<Integer>, Range<Integer>, Integer] The topic's partitions or partition count
  # @return [nil]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer/topic_partition_list.rb#53
  def add_topic(topic, partitions = T.unsafe(nil)); end

  # Add a topic with partitions and offsets set to the list
  # Calling this method multiple times for the same topic will overwrite the previous configuraton.
  #
  # @param topic [String] The topic's name
  # @param partitions_with_offsets [Hash<Integer, Integer>] The topic's partitions and offsets
  # @param partitions_with_offsets [Array<Consumer::Partition>] The topic's partitions with offsets
  #   and metadata (if any)
  # @return [nil]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer/topic_partition_list.rb#73
  def add_topic_and_partitions_with_offsets(topic, partitions_with_offsets); end

  # Number of items in the list
  #
  # @return [Integer]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer/topic_partition_list.rb#18
  def count; end

  # Whether this list is empty
  #
  # @return [Boolean]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer/topic_partition_list.rb#32
  def empty?; end

  # Return a `Hash` with the topics as keys and and an array of partition information as the value if present.
  #
  # @return [Hash{String => Array<Partition>,nil}]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer/topic_partition_list.rb#82
  def to_h; end

  # Create a native tpl with the contents of this object added.
  #
  # The pointer will be cleaned by `rd_kafka_topic_partition_list_destroy` when GC releases it.
  #
  # @private
  # @return [FFI::Pointer]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer/topic_partition_list.rb#143
  def to_native_tpl; end

  # Human readable representation of this list.
  #
  # @return [String]
  #
  # source://karafka-rdkafka//lib/rdkafka/consumer/topic_partition_list.rb#88
  def to_s; end

  class << self
    # Create a new topic partition list based of a native one.
    #
    # @param pointer [FFI::Pointer] Optional pointer to an existing native list. Its contents will be copied.
    # @private
    # @return [TopicPartitionList]
    #
    # source://karafka-rdkafka//lib/rdkafka/consumer/topic_partition_list.rb#103
    def from_native_tpl(pointer); end
  end
end

# Namespace for some small utilities used in multiple components
#
# source://karafka-rdkafka//lib/rdkafka/helpers/time.rb#5
module Rdkafka::Helpers; end

# source://karafka-rdkafka//lib/rdkafka/helpers/oauth.rb#4
module Rdkafka::Helpers::OAuth
  # Set the OAuthBearer token
  #
  # @param token [String] the mandatory token value to set, often (but not necessarily) a JWS compact serialization as per https://tools.ietf.org/html/rfc7515#section-3.1.
  # @param lifetime_ms [Integer] when the token expires, in terms of the number of milliseconds since the epoch. See https://currentmillis.com/.
  # @param principal_name [String] the mandatory Kafka principal name associated with the token.
  # @param extensions [Hash] optional SASL extensions key-value pairs to be communicated to the broker as additional key-value pairs during the initial client response as per https://tools.ietf.org/html/rfc7628#section-3.1.
  # @return [Integer] 0 on success
  #
  # source://karafka-rdkafka//lib/rdkafka/helpers/oauth.rb#13
  def oauthbearer_set_token(token:, lifetime_ms:, principal_name:, extensions: T.unsafe(nil)); end

  # Marks failed oauth token acquire in librdkafka
  #
  # @param reason [String] human readable error reason for failing to acquire token
  #
  # source://karafka-rdkafka//lib/rdkafka/helpers/oauth.rb#33
  def oauthbearer_set_token_failure(reason); end

  private

  # extension_size is the number of keys + values which should be a non-negative even number
  # https://github.com/confluentinc/librdkafka/blob/master/src/rdkafka_sasl_oauthbearer.c#L327-L347
  #
  # source://karafka-rdkafka//lib/rdkafka/helpers/oauth.rb#52
  def extension_size(extensions); end

  # Flatten the extensions hash into a string according to the spec, https://datatracker.ietf.org/doc/html/rfc7628#section-3.1
  #
  # source://karafka-rdkafka//lib/rdkafka/helpers/oauth.rb#45
  def flatten_extensions(extensions); end
end

# Time related methods used across Karafka
#
# source://karafka-rdkafka//lib/rdkafka/helpers/time.rb#7
module Rdkafka::Helpers::Time
  # @return [Float] current monotonic time in seconds with microsecond precision
  #
  # source://karafka-rdkafka//lib/rdkafka/helpers/time.rb#9
  def monotonic_now; end
end

# source://karafka-rdkafka//lib/rdkafka/version.rb#6
Rdkafka::LIBRDKAFKA_SOURCE_SHA256 = T.let(T.unsafe(nil), String)

# source://karafka-rdkafka//lib/rdkafka/version.rb#5
Rdkafka::LIBRDKAFKA_VERSION = T.let(T.unsafe(nil), String)

# source://karafka-rdkafka//lib/rdkafka/metadata.rb#4
class Rdkafka::Metadata
  # @return [Metadata] a new instance of Metadata
  #
  # source://karafka-rdkafka//lib/rdkafka/metadata.rb#15
  def initialize(native_client, topic_name = T.unsafe(nil), timeout_ms = T.unsafe(nil)); end

  # Returns the value of attribute brokers.
  #
  # source://karafka-rdkafka//lib/rdkafka/metadata.rb#5
  def brokers; end

  # Returns the value of attribute topics.
  #
  # source://karafka-rdkafka//lib/rdkafka/metadata.rb#5
  def topics; end

  private

  # source://karafka-rdkafka//lib/rdkafka/metadata.rb#52
  def metadata_from_native(ptr); end
end

# source://karafka-rdkafka//lib/rdkafka/metadata.rb#92
class Rdkafka::Metadata::BrokerMetadata < ::Rdkafka::Metadata::CustomFFIStruct; end

# source://karafka-rdkafka//lib/rdkafka/metadata.rb#72
class Rdkafka::Metadata::CustomFFIStruct < ::FFI::Struct
  # source://karafka-rdkafka//lib/rdkafka/metadata.rb#73
  def to_h; end
end

# source://karafka-rdkafka//lib/rdkafka/metadata.rb#83
class Rdkafka::Metadata::Metadata < ::Rdkafka::Metadata::CustomFFIStruct; end

# source://karafka-rdkafka//lib/rdkafka/metadata.rb#105
class Rdkafka::Metadata::PartitionMetadata < ::Rdkafka::Metadata::CustomFFIStruct; end

# Errors upon which we retry the metadata fetch
#
# source://karafka-rdkafka//lib/rdkafka/metadata.rb#8
Rdkafka::Metadata::RETRIED_ERRORS = T.let(T.unsafe(nil), Array)

# source://karafka-rdkafka//lib/rdkafka/metadata.rb#98
class Rdkafka::Metadata::TopicMetadata < ::Rdkafka::Metadata::CustomFFIStruct; end

# A wrapper around a native kafka that polls and cleanly exits
#
# @private
#
# source://karafka-rdkafka//lib/rdkafka/native_kafka.rb#6
class Rdkafka::NativeKafka
  # @return [NativeKafka] a new instance of NativeKafka
  #
  # source://karafka-rdkafka//lib/rdkafka/native_kafka.rb#7
  def initialize(inner, run_polling_thread:, opaque:, auto_start: T.unsafe(nil)); end

  # source://karafka-rdkafka//lib/rdkafka/native_kafka.rb#102
  def close(object_id = T.unsafe(nil)); end

  # @return [Boolean]
  #
  # source://karafka-rdkafka//lib/rdkafka/native_kafka.rb#98
  def closed?; end

  # source://karafka-rdkafka//lib/rdkafka/native_kafka.rb#94
  def finalizer; end

  # source://karafka-rdkafka//lib/rdkafka/native_kafka.rb#38
  def start; end

  # source://karafka-rdkafka//lib/rdkafka/native_kafka.rb#82
  def synchronize(&block); end

  # source://karafka-rdkafka//lib/rdkafka/native_kafka.rb#70
  def with_inner; end
end

# @private
#
# source://karafka-rdkafka//lib/rdkafka/config.rb#368
class Rdkafka::Opaque
  # source://karafka-rdkafka//lib/rdkafka/config.rb#372
  def call_delivery_callback(delivery_report, delivery_handle); end

  # source://karafka-rdkafka//lib/rdkafka/config.rb#376
  def call_on_partitions_assigned(list); end

  # source://karafka-rdkafka//lib/rdkafka/config.rb#383
  def call_on_partitions_revoked(list); end

  # Returns the value of attribute consumer_rebalance_listener.
  #
  # source://karafka-rdkafka//lib/rdkafka/config.rb#370
  def consumer_rebalance_listener; end

  # Sets the attribute consumer_rebalance_listener
  #
  # @param value the value to set the attribute consumer_rebalance_listener to.
  #
  # source://karafka-rdkafka//lib/rdkafka/config.rb#370
  def consumer_rebalance_listener=(_arg0); end

  # Returns the value of attribute producer.
  #
  # source://karafka-rdkafka//lib/rdkafka/config.rb#369
  def producer; end

  # Sets the attribute producer
  #
  # @param value the value to set the attribute producer to.
  #
  # source://karafka-rdkafka//lib/rdkafka/config.rb#369
  def producer=(_arg0); end
end

# A producer for Kafka messages. To create a producer set up a {Config} and call {Config#producer producer} on that.
#
# source://karafka-rdkafka//lib/rdkafka/producer.rb#5
class Rdkafka::Producer
  include ::Rdkafka::Helpers::Time
  include ::Rdkafka::Helpers::OAuth

  # @param native_kafka [NativeKafka]
  # @param partitioner_name [String, nil] name of the partitioner we want to use or nil to use
  #   the "consistent_random" default
  # @private
  # @return [Producer] a new instance of Producer
  #
  # source://karafka-rdkafka//lib/rdkafka/producer.rb#38
  def initialize(native_kafka, partitioner_name); end

  # source://karafka-rdkafka//lib/rdkafka/producer.rb#170
  def abort_transaction(timeout_ms = T.unsafe(nil)); end

  # Figures out the arity of a given block/method
  #
  # @param callback [#call, Proc]
  # @return [Integer] arity of the provided block/method
  #
  # source://karafka-rdkafka//lib/rdkafka/producer.rb#451
  def arity(callback); end

  # source://karafka-rdkafka//lib/rdkafka/producer.rb#150
  def begin_transaction; end

  # Calls (if registered) the delivery callback
  #
  # @param delivery_report [Producer::DeliveryReport]
  # @param delivery_handle [Producer::DeliveryHandle]
  #
  # source://karafka-rdkafka//lib/rdkafka/producer.rb#434
  def call_delivery_callback(delivery_report, delivery_handle); end

  # Close this producer and wait for the internal poll queue to empty.
  #
  # source://karafka-rdkafka//lib/rdkafka/producer.rb#207
  def close; end

  # Whether this producer has closed
  #
  # @return [Boolean]
  #
  # source://karafka-rdkafka//lib/rdkafka/producer.rb#225
  def closed?; end

  # source://karafka-rdkafka//lib/rdkafka/producer.rb#160
  def commit_transaction(timeout_ms = T.unsafe(nil)); end

  # Returns the current delivery callback, by default this is nil.
  #
  # @private
  # @return [Proc, nil]
  #
  # source://karafka-rdkafka//lib/rdkafka/producer.rb#26
  def delivery_callback; end

  # Set a callback that will be called every time a message is successfully produced.
  # The callback is called with a {DeliveryReport} and {DeliveryHandle}
  #
  # @param callback [Proc, #call] The callback
  # @raise [TypeError]
  # @return [nil]
  #
  # source://karafka-rdkafka//lib/rdkafka/producer.rb#132
  def delivery_callback=(callback); end

  # Returns the number of arguments accepted by the callback, by default this is nil.
  #
  # @private
  # @return [Integer, nil]
  #
  # source://karafka-rdkafka//lib/rdkafka/producer.rb#32
  def delivery_callback_arity; end

  # Wait until all outstanding producer requests are completed, with the given timeout
  # in seconds. Call this before closing a producer to ensure delivery of all messages.
  #
  # @note We raise an exception for other errors because based on the librdkafka docs, there
  #   should be no other errors.
  # @note For `timed_out` we do not raise an error to keep it backwards compatible
  # @param timeout_ms [Integer] how long should we wait for flush of all messages
  # @return [Boolean] true if no more data and all was flushed, false in case there are still
  #   outgoing messages after the timeout
  #
  # source://karafka-rdkafka//lib/rdkafka/producer.rb#240
  def flush(timeout_ms = T.unsafe(nil)); end

  # Init transactions
  # Run once per producer
  #
  # source://karafka-rdkafka//lib/rdkafka/producer.rb#140
  def init_transactions; end

  # @return [String] producer name
  #
  # source://karafka-rdkafka//lib/rdkafka/producer.rb#120
  def name; end

  # Partition count for a given topic.
  #
  # @note If 'allow.auto.create.topics' is set to true in the broker, the topic will be
  #   auto-created after returning nil.
  # @note We cache the partition count for a given topic for given time.
  #   This prevents us in case someone uses `partition_key` from querying for the count with
  #   each message. Instead we query once every 30 seconds at most if we have a valid partition
  #   count or every 5 seconds in case we were not able to obtain number of partitions
  # @param topic [String] The topic name.
  # @return [Integer] partition count for a given topic or `-1` if it could not be obtained.
  #
  # source://karafka-rdkafka//lib/rdkafka/producer.rb#291
  def partition_count(topic); end

  # Produces a message to a Kafka topic. The message is added to rdkafka's queue, call {DeliveryHandle#wait wait} on the returned delivery handle to make sure it is delivered.
  #
  # When no partition is specified the underlying Kafka library picks a partition based on the key. If no key is specified, a random partition will be used.
  # When a timestamp is provided this is used instead of the auto-generated timestamp.
  #
  # @param topic [String] The topic to produce to
  # @param payload [String, nil] The message's payload
  # @param key [String, nil] The message's key
  # @param partition [Integer, nil] Optional partition to produce to
  # @param partition_key [String, nil] Optional partition key based on which partition assignment can happen
  # @param timestamp [Time, Integer, nil] Optional timestamp of this message. Integer timestamp is in milliseconds since Jan 1 1970.
  # @param headers [Hash<String,String>] Optional message headers
  # @param label [Object, nil] a label that can be assigned when producing a message that will be part of the delivery handle and the delivery report
  # @param topic_config [Hash] topic config for given message dispatch. Allows to send messages to topics with different configuration
  # @raise [RdkafkaError] When adding the message to rdkafka's queue failed
  # @return [DeliveryHandle] Delivery handle that can be used to wait for the result of producing this message
  #
  # source://karafka-rdkafka//lib/rdkafka/producer.rb#319
  def produce(topic:, payload: T.unsafe(nil), key: T.unsafe(nil), partition: T.unsafe(nil), partition_key: T.unsafe(nil), timestamp: T.unsafe(nil), headers: T.unsafe(nil), label: T.unsafe(nil), topic_config: T.unsafe(nil)); end

  # Purges the outgoing queue and releases all resources.
  #
  # Useful when closing the producer with outgoing messages to unstable clusters or when for
  # any other reasons waiting cannot go on anymore. This purges both the queue and all the
  # inflight requests + updates the delivery handles statuses so they can be materialized into
  # `purge_queue` errors.
  #
  # source://karafka-rdkafka//lib/rdkafka/producer.rb#261
  def purge; end

  # Sends provided offsets of a consumer to the transaction for collective commit
  #
  # @note Use **only** in the context of an active transaction
  # @param consumer [Consumer] consumer that owns the given tpls
  # @param tpl [Consumer::TopicPartitionList]
  # @param timeout_ms [Integer] offsets send timeout
  #
  # source://karafka-rdkafka//lib/rdkafka/producer.rb#185
  def send_offsets_to_transaction(consumer, tpl, timeout_ms = T.unsafe(nil)); end

  # Sets alternative set of configuration details that can be set per topic
  #
  # @note It is not allowed to re-set the same topic config twice because of the underlying
  #   librdkafka caching
  # @param topic [String] The topic name
  # @param config [Hash] config we want to use per topic basis
  # @param config_hash [Integer] hash of the config. We expect it here instead of computing it,
  #   because it is already computed during the retrieval attempt in the `#produce` flow.
  #
  # source://karafka-rdkafka//lib/rdkafka/producer.rb#74
  def set_topic_config(topic, config, config_hash); end

  # Starts the native Kafka polling thread and kicks off the init polling
  #
  # @note Not needed to run unless explicit start was disabled
  #
  # source://karafka-rdkafka//lib/rdkafka/producer.rb#115
  def start; end

  private

  # Ensures, no operations can happen on a closed producer
  #
  # @param method [Symbol] name of the method that invoked producer
  # @raise [Rdkafka::ClosedProducerError]
  #
  # source://karafka-rdkafka//lib/rdkafka/producer.rb#463
  def closed_producer_check(method); end
end

# Handle to wait for a delivery report which is returned when
# producing a message.
#
# source://karafka-rdkafka//lib/rdkafka/producer/delivery_handle.rb#7
class Rdkafka::Producer::DeliveryHandle < ::Rdkafka::AbstractHandle
  # @return [DeliveryReport] a report on the delivery of the message
  #
  # source://karafka-rdkafka//lib/rdkafka/producer/delivery_handle.rb#23
  def create_result; end

  # @return [Object, nil] label set during message production or nil by default
  #
  # source://karafka-rdkafka//lib/rdkafka/producer/delivery_handle.rb#15
  def label; end

  # @return [Object, nil] label set during message production or nil by default
  #
  # source://karafka-rdkafka//lib/rdkafka/producer/delivery_handle.rb#15
  def label=(_arg0); end

  # @return [String] the name of the operation (e.g. "delivery")
  #
  # source://karafka-rdkafka//lib/rdkafka/producer/delivery_handle.rb#18
  def operation_name; end
end

# Delivery report for a successfully produced message.
#
# source://karafka-rdkafka//lib/rdkafka/producer/delivery_report.rb#6
class Rdkafka::Producer::DeliveryReport
  # @return [DeliveryReport] a new instance of DeliveryReport
  #
  # source://karafka-rdkafka//lib/rdkafka/producer/delivery_report.rb#36
  def initialize(partition, offset, topic_name = T.unsafe(nil), error = T.unsafe(nil), label = T.unsafe(nil)); end

  # Error in case happen during produce.
  #
  # @return [Integer]
  #
  # source://karafka-rdkafka//lib/rdkafka/producer/delivery_report.rb#22
  def error; end

  # @return [Object, nil] label set during message production or nil by default
  #
  # source://karafka-rdkafka//lib/rdkafka/producer/delivery_report.rb#25
  def label; end

  # The offset of the produced message.
  #
  # @return [Integer]
  #
  # source://karafka-rdkafka//lib/rdkafka/producer/delivery_report.rb#13
  def offset; end

  # The partition this message was produced to.
  #
  # @return [Integer]
  #
  # source://karafka-rdkafka//lib/rdkafka/producer/delivery_report.rb#9
  def partition; end

  # The name of the topic this message was produced to or nil in case delivery failed and we
  #   we not able to get the topic reference
  # We alias the `#topic_name` under `#topic` to make this consistent with `Consumer::Message`
  # where the topic name is under `#topic` method. That way we have a consistent name that
  # is present in both places
  #
  # We do not remove the original `#topic_name` because of backwards compatibility
  #
  # @return [String, nil]
  #
  # source://karafka-rdkafka//lib/rdkafka/producer/delivery_report.rb#18
  def topic; end

  # The name of the topic this message was produced to or nil in case delivery failed and we
  #   we not able to get the topic reference
  #
  # @return [String, nil]
  #
  # source://karafka-rdkafka//lib/rdkafka/producer/delivery_report.rb#18
  def topic_name; end
end

# Empty hash used as a default
#
# source://karafka-rdkafka//lib/rdkafka/producer.rb#13
Rdkafka::Producer::EMPTY_HASH = T.let(T.unsafe(nil), Hash)

# Cache partitions count for 30 seconds
#
# source://karafka-rdkafka//lib/rdkafka/producer.rb#10
Rdkafka::Producer::PARTITIONS_COUNT_TTL = T.let(T.unsafe(nil), Integer)

# Raised when there was a critical issue when invoking rd_kafka_topic_new
# This is a temporary solution until https://github.com/karafka/rdkafka-ruby/issues/451 is
# resolved and this is normalized in all the places
#
# source://karafka-rdkafka//lib/rdkafka/producer.rb#20
class Rdkafka::Producer::TopicHandleCreationError < ::RuntimeError; end

# Error returned by the underlying rdkafka library.
#
# source://karafka-rdkafka//lib/rdkafka/error.rb#8
class Rdkafka::RdkafkaError < ::Rdkafka::BaseError
  # @private
  # @raise [TypeError]
  # @return [RdkafkaError] a new instance of RdkafkaError
  #
  # source://karafka-rdkafka//lib/rdkafka/error.rb#70
  def initialize(response, message_prefix = T.unsafe(nil), broker_message: T.unsafe(nil), fatal: T.unsafe(nil), retryable: T.unsafe(nil), abortable: T.unsafe(nil)); end

  # Error comparison
  #
  # source://karafka-rdkafka//lib/rdkafka/error.rb#116
  def ==(another_error); end

  # @return [Boolean]
  #
  # source://karafka-rdkafka//lib/rdkafka/error.rb#128
  def abortable?; end

  # Error message sent by the broker
  #
  # @return [String]
  #
  # source://karafka-rdkafka//lib/rdkafka/error.rb#19
  def broker_message; end

  # This error's code, for example `:partition_eof`, `:msg_size_too_large`.
  #
  # @return [Symbol]
  #
  # source://karafka-rdkafka//lib/rdkafka/error.rb#89
  def code; end

  # @return [Boolean]
  #
  # source://karafka-rdkafka//lib/rdkafka/error.rb#120
  def fatal?; end

  # Whether this error indicates the partition is EOF.
  #
  # @return [Boolean]
  #
  # source://karafka-rdkafka//lib/rdkafka/error.rb#111
  def is_partition_eof?; end

  # Prefix to be used for human readable representation
  #
  # @return [String]
  #
  # source://karafka-rdkafka//lib/rdkafka/error.rb#15
  def message_prefix; end

  # The underlying raw error response
  #
  # @return [Integer]
  #
  # source://karafka-rdkafka//lib/rdkafka/error.rb#11
  def rdkafka_response; end

  # @return [Boolean]
  #
  # source://karafka-rdkafka//lib/rdkafka/error.rb#124
  def retryable?; end

  # Human readable representation of this error.
  #
  # @return [String]
  #
  # source://karafka-rdkafka//lib/rdkafka/error.rb#100
  def to_s; end

  class << self
    # source://karafka-rdkafka//lib/rdkafka/error.rb#44
    def build(response_ptr_or_code, message_prefix = T.unsafe(nil), broker_message: T.unsafe(nil)); end

    # source://karafka-rdkafka//lib/rdkafka/error.rb#22
    def build_from_c(response_ptr, message_prefix = T.unsafe(nil), broker_message: T.unsafe(nil)); end

    # source://karafka-rdkafka//lib/rdkafka/error.rb#63
    def validate!(response_ptr_or_code, message_prefix = T.unsafe(nil), broker_message: T.unsafe(nil)); end
  end
end

# Error with topic partition list returned by the underlying rdkafka library.
#
# source://karafka-rdkafka//lib/rdkafka/error.rb#134
class Rdkafka::RdkafkaTopicPartitionListError < ::Rdkafka::RdkafkaError
  # @private
  # @return [RdkafkaTopicPartitionListError] a new instance of RdkafkaTopicPartitionListError
  #
  # source://karafka-rdkafka//lib/rdkafka/error.rb#139
  def initialize(response, topic_partition_list, message_prefix = T.unsafe(nil)); end

  # @return [TopicPartitionList]
  #
  # source://karafka-rdkafka//lib/rdkafka/error.rb#136
  def topic_partition_list; end
end

# source://karafka-rdkafka//lib/rdkafka/version.rb#4
Rdkafka::VERSION = T.let(T.unsafe(nil), String)
