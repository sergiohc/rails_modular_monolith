# typed: true

# DO NOT EDIT MANUALLY
# This is an autogenerated file for types exported from the `karafka-web` gem.
# Please instead update this file by running `bin/tapioca gem karafka-web`.


# @see https://github.com/mperham/sidekiq/blob/main/lib/sidekiq/web/helpers.rb
#
# source://karafka-web//lib/karafka/web/version.rb#3
module Karafka
  class << self
    # source://karafka/2.4.2/lib/karafka.rb#94
    def boot_file; end

    # source://karafka/2.4.2/lib/karafka.rb#63
    def core_root; end

    # source://karafka/2.4.2/lib/karafka.rb#25
    def env; end

    # source://karafka/2.4.2/lib/karafka.rb#33
    def env=(environment); end

    # source://karafka/2.4.2/lib/karafka.rb#53
    def gem_root; end

    # source://karafka/2.4.2/lib/karafka.rb#38
    def logger; end

    # source://karafka/2.4.2/lib/karafka.rb#48
    def monitor; end

    # source://karafka/2.4.2/lib/karafka.rb#68
    def pro?; end

    # source://karafka/2.4.2/lib/karafka.rb#43
    def producer; end

    # source://karafka/2.4.2/lib/karafka.rb#74
    def rails?; end

    # source://karafka/2.4.2/lib/karafka.rb#103
    def refresh!; end

    # source://karafka/2.4.2/lib/karafka.rb#58
    def root; end
  end
end

# Karafka Web UI + Karafka web monitoring
#
# source://karafka-web//lib/karafka/web/version.rb#4
module Karafka::Web
  class << self
    # @return [Karafka::Web::Config] config instance
    #
    # source://karafka-web//lib/karafka/web.rb#41
    def config; end

    # Activates all the needed routing and sets up listener, etc
    # This needs to run **after** the optional configuration of the web component
    #
    # source://karafka-web//lib/karafka/web.rb#47
    def enable!; end

    # @return [String] root path of this gem
    #
    # source://karafka-web//lib/karafka/web.rb#24
    def gem_root; end

    # @note Do NOT memoize producer as it may be updated after forking
    # @return [WaterDrop::Producer, nil] waterdrop messages producer or nil if not yet fully
    #   initialized. It may not be fully initialized until the configuration is done
    #
    # source://karafka-web//lib/karafka/web.rb#19
    def producer; end

    # Sets up the whole configuration
    #
    # @param block [Block] configuration block
    #
    # source://karafka-web//lib/karafka/web.rb#30
    def setup(&block); end

    # @return [Array<String>] Web UI slogans we use to encourage people to support Karafka
    #
    # source://karafka-web//lib/karafka/web.rb#64
    def slogans; end
  end
end

# Proxy App that selects either Pro or regular app to handle the requests
#
# source://karafka-web//lib/karafka/web/app.rb#6
class Karafka::Web::App
  class << self
    # @param env [Hash] Rack env
    # @param block [Proc] Rack block
    #
    # source://karafka-web//lib/karafka/web/app.rb#10
    def call(env, &block); end

    # @return [Class] regular or pro Web engine
    #
    # source://karafka-web//lib/karafka/web/app.rb#15
    def engine; end
  end
end

# Web CLI
#
# source://karafka-web//lib/karafka/web/cli.rb#6
class Karafka::Web::Cli < ::Karafka::Cli
  class << self
    private

    # @return [Array<Class>] command classes
    #
    # source://karafka-web//lib/karafka/web/cli.rb#11
    def commands; end
  end
end

# Base command for all the Web Cli commands
#
# source://karafka-web//lib/karafka/web/cli/base.rb#7
class Karafka::Web::Cli::Base < ::Karafka::Cli::Base
  private

  # Takes the CLI user provided replication factor but if not present, uses the brokers count
  # to decide. For non-dev clusters (with one broker) we usually want to have replication of
  # two, just to have some redundancy.
  #
  # @param cli_replication_factor [Integer, false] user requested replication factor or false
  #   if we are supposed to compute the factor automatically
  # @return [Integer] replication factor for Karafka Web UI topics
  #
  # source://karafka-web//lib/karafka/web/cli/base.rb#29
  def compute_replication_factor(cli_replication_factor); end

  class << self
    # @return [Array<Class>] available commands
    #
    # source://karafka-web//lib/karafka/web/cli/base.rb#12
    def commands; end
  end
end

# Displays help
#
# source://karafka-web//lib/karafka/web/cli/help.rb#7
class Karafka::Web::Cli::Help < ::Karafka::Web::Cli::Base
  # Print available commands
  #
  # source://karafka-web//lib/karafka/web/cli/help.rb#11
  def call; end
end

# Installs Web UI
#
# source://karafka-web//lib/karafka/web/cli/install.rb#7
class Karafka::Web::Cli::Install < ::Karafka::Web::Cli::Base
  # Installs Karafka Web. Creates all needed topics, populates the data and adds the needed
  # code to `karafka.rb`.
  #
  # source://karafka-web//lib/karafka/web/cli/install.rb#19
  def call; end
end

# Migrates the Web UI topics and states if needed
#
# source://karafka-web//lib/karafka/web/cli/migrate.rb#7
class Karafka::Web::Cli::Migrate < ::Karafka::Web::Cli::Base
  # Creates new topics (if any) and populates missing data.
  # It does **not** remove topics and will not populate data if it is already there.
  #
  # Useful in two scenarios:
  #   1. When setting up Web-UI in a new environment, so the Web-UI has the proper initial
  #      state.
  #   2. When upgrading Web-UI in-between versions that would require extra topics and/or
  #      extra states populated.
  #
  # source://karafka-web//lib/karafka/web/cli/migrate.rb#25
  def call; end
end

# Resets the Web UI
#
# source://karafka-web//lib/karafka/web/cli/reset.rb#7
class Karafka::Web::Cli::Reset < ::Karafka::Web::Cli::Base
  # Resets Karafka Web. Removes the topics, creates them again and populates the initial
  # state again. This is useful in case the Web-UI metrics or anything else got corrupted.
  #
  # source://karafka-web//lib/karafka/web/cli/reset.rb#19
  def call; end
end

# Uninstalls the Web UI
#
# source://karafka-web//lib/karafka/web/cli/uninstall.rb#7
class Karafka::Web::Cli::Uninstall < ::Karafka::Web::Cli::Base
  # Uninstalls Karafka Web
  #
  # source://karafka-web//lib/karafka/web/cli/uninstall.rb#11
  def call; end
end

# Karafka::Web configuration
#
# source://karafka-web//lib/karafka/web/config.rb#6
class Karafka::Web::Config
  include ::Karafka::Core::Configurable
  include ::Karafka::Core::Configurable::InstanceMethods
  extend ::Karafka::Core::Configurable
  extend ::Karafka::Core::Configurable::ClassMethods
end

# Namespace for contracts across the web
#
# source://karafka-web//lib/karafka/web/tracking/consumers/contracts/report.rb#0
module Karafka::Web::Contracts; end

# Base for all the contracts
#
# source://karafka-web//lib/karafka/web/contracts/base.rb#8
class Karafka::Web::Contracts::Base < ::Karafka::Core::Contractable::Contract
  # @param data [Hash] data for validation
  # @raise [Errors::ContractError] invalid report
  # @return [Boolean] true if all good
  #
  # source://karafka-web//lib/karafka/web/contracts/base.rb#26
  def validate!(data); end

  class << self
    # This layer is not for users extensive feedback, thus we can easily use the minimum
    # error messaging there is.
    #
    # source://karafka-web//lib/karafka/web/contracts/base.rb#12
    def configure; end
  end
end

# Contract to validate Web-UI configuration
#
# source://karafka-web//lib/karafka/web/contracts/config.rb#7
class Karafka::Web::Contracts::Config < ::Karafka::Web::Contracts::Base; end

# Use the same regexp as Karafka for topics validation
#
# source://karafka-web//lib/karafka/web/contracts/config.rb#11
Karafka::Web::Contracts::Config::TOPIC_REGEXP = T.let(T.unsafe(nil), Regexp)

# Web reporting deserializer
#
# @note We use `symbolize_names` because we want to use the same convention of hash building
#   for producing, consuming and displaying metrics related data
# @note We have to check if we compress the data, because older Web-UI versions were not
#   compressing the payload.
#
# source://karafka-web//lib/karafka/web/deserializer.rb#12
class Karafka::Web::Deserializer
  # @param message [::Karafka::Messages::Message]
  # @return [Object] deserialized data
  #
  # source://karafka-web//lib/karafka/web/deserializer.rb#15
  def call(message); end
end

# Karafka::Web related errors
#
# source://karafka-web//lib/karafka/web/errors.rb#6
module Karafka::Web::Errors; end

# Base class for all errors related to the web ui
#
# source://karafka-web//lib/karafka/web/errors.rb#8
class Karafka::Web::Errors::BaseError < ::Karafka::Errors::BaseError; end

# Raised when the a report is not valid for any reason
# This should never happen and if you see this, please open an issue.
#
# source://karafka-web//lib/karafka/web/errors.rb#12
class Karafka::Web::Errors::ContractError < ::Karafka::Web::Errors::BaseError; end

# Errors specific to management
#
# source://karafka-web//lib/karafka/web/errors.rb#15
module Karafka::Web::Errors::Management; end

# Similar to processing error with the same name, it is raised when a critical
# incompatibility is detected.
#
# This error is raised when there was an attempt to operate on aggregated Web UI states
# that are already in a newer version that the one in the current process. We prevent
# this from happening not to corrupt the data. Please upgrade all the Web UI consumers to
# the same version
#
# source://karafka-web//lib/karafka/web/errors.rb#23
class Karafka::Web::Errors::Management::IncompatibleSchemaError < ::Karafka::Web::Errors::BaseError; end

# Processing related errors namespace
#
# source://karafka-web//lib/karafka/web/errors.rb#27
module Karafka::Web::Errors::Processing; end

# This error occurs when consumer running older version of the web-ui tries to materialize
# states from newer versions. Karafka Web-UI provides only backwards compatibility, so
# you need to have an up-to-date consumer materializing reported states.
#
# If you see this error, please make sure that the consumer process that is materializing
# your states is running at least the same version as the consumers that are reporting
# the states
#
# If you see this error do not worry. When you get a consumer with up-to-date version,
# all the historical metrics will catch up.
#
# source://karafka-web//lib/karafka/web/errors.rb#54
class Karafka::Web::Errors::Processing::IncompatibleSchemaError < ::Karafka::Web::Errors::BaseError; end

# Similar to the above. It should be created during install / migration
#
# source://karafka-web//lib/karafka/web/errors.rb#39
class Karafka::Web::Errors::Processing::MissingConsumersMetricsError < ::Karafka::Web::Errors::BaseError; end

# Similar to the one related to consumers states
#
# source://karafka-web//lib/karafka/web/errors.rb#42
class Karafka::Web::Errors::Processing::MissingConsumersMetricsTopicError < ::Karafka::Web::Errors::BaseError; end

# Raised when we try to process reports but we do not have the current state bootstrapped
# If you see this error, it probably means, that you did not bootstrap Web-UI correctly
#
# source://karafka-web//lib/karafka/web/errors.rb#30
class Karafka::Web::Errors::Processing::MissingConsumersStateError < ::Karafka::Web::Errors::BaseError; end

# Raised when we try to materialize the state but the consumers states topic does not
# exist and we do not have a way to get the initial state.
# It differs from the above because above indicates that the topic exists but that there
# is no initial state, while this indicates, that there is no consumers states topic.
#
# source://karafka-web//lib/karafka/web/errors.rb#36
class Karafka::Web::Errors::Processing::MissingConsumersStatesTopicError < ::Karafka::Web::Errors::BaseError; end

# Ui related errors
#
# source://karafka-web//lib/karafka/web/errors.rb#58
module Karafka::Web::Errors::Ui; end

# Raised when we cannot display a given view.
# This may mean, that request topic was not present or partition or a message.
#
# source://karafka-web//lib/karafka/web/errors.rb#61
class Karafka::Web::Errors::Ui::NotFoundError < ::Karafka::Web::Errors::BaseError; end

# Raised whe a given feature is available for Pro but not pro used
#
# source://karafka-web//lib/karafka/web/errors.rb#64
class Karafka::Web::Errors::Ui::ProOnlyError < ::Karafka::Web::Errors::BaseError; end

# Web UI Zeitwerk Inflector that allows us to have time prefixed files with migrations, similar
# to how Rails does that.
#
# source://karafka-web//lib/karafka/web/inflector.rb#7
class Karafka::Web::Inflector < ::Zeitwerk::GemInflector
  # @param basename [String] of the file to be loaded
  # @param abspath [String] absolute path of the file to be loaded
  # @return [String] Constant name to be used for given file
  #
  # source://karafka-web//lib/karafka/web/inflector.rb#19
  def camelize(basename, abspath); end
end

# Checks if given path is a migration one
#
# source://karafka-web//lib/karafka/web/inflector.rb#9
Karafka::Web::Inflector::MIGRATION_ABSPATH_REGEXP = T.let(T.unsafe(nil), Regexp)

# Checks if it is a migration file
#
# source://karafka-web//lib/karafka/web/inflector.rb#12
Karafka::Web::Inflector::MIGRATION_BASENAME_REGEXP = T.let(T.unsafe(nil), Regexp)

# Responsible for setup of the Web UI and Karafka Web-UI related components initialization.
#
# source://karafka-web//lib/karafka/web/installer.rb#6
class Karafka::Web::Installer
  include ::Karafka::Helpers::Colorize

  # Enables the Web-UI in the karafka app. Sets up needed routes and listeners.
  #
  # source://karafka-web//lib/karafka/web/installer.rb#88
  def enable!; end

  # Creates needed topics and the initial zero state, so even if no `karafka server` processes
  # are running, we can still display the empty UI. Also adds needed code to the `karafka.rb`
  # file.
  #
  # @param replication_factor [Integer] replication factor we want to use (1 by default)
  #
  # source://karafka-web//lib/karafka/web/installer.rb#14
  def install(replication_factor: T.unsafe(nil)); end

  # Creates missing topics and missing zero states. Needs to run for each environment where we
  # want to use Web-UI
  #
  # @param replication_factor [Integer] replication factor we want to use (1 by default)
  #
  # source://karafka-web//lib/karafka/web/installer.rb#38
  def migrate(replication_factor: T.unsafe(nil)); end

  # Removes all the Karafka topics and creates them again with the same replication factor
  #
  # @param replication_factor [Integer] replication factor we want to use (1 by default)
  #
  # source://karafka-web//lib/karafka/web/installer.rb#56
  def reset(replication_factor: T.unsafe(nil)); end

  # Removes all the Karafka Web topics and cleans after itself.
  #
  # source://karafka-web//lib/karafka/web/installer.rb#75
  def uninstall; end

  private

  # Waits for given number of seconds and prints `.` every second.
  #
  # @param time_in_seconds [Integer] time of wait
  #
  # source://karafka-web//lib/karafka/web/installer.rb#107
  def wait(time_in_seconds); end

  # Waits with a message, that we are waiting on topics
  # This is not doing much, just waiting as there are some cases that it takes a bit of time
  # for Kafka to actually propagate new topics knowledge across the cluster. We give it that
  # bit of time just in case.
  #
  # source://karafka-web//lib/karafka/web/installer.rb#98
  def wait_for_topics; end
end

# Namespace for all cross-context management operations that are needed to make sure everything
# operate as expected.
#
# source://karafka-web//lib/karafka/web.rb#0
module Karafka::Web::Management; end

# Namespace for all the commands related to management of the Web-UI in the context of
# Karafka. It includes things like installing, creating needed topics, etc.
#
# source://karafka-web//lib/karafka/web.rb#0
module Karafka::Web::Management::Actions; end

# Base class for all the commands that we use to manage
#
# source://karafka-web//lib/karafka/web/management/actions/base.rb#10
class Karafka::Web::Management::Actions::Base
  include ::Karafka::Helpers::Colorize

  private

  # @return [String] green colored word "already"
  #
  # source://karafka-web//lib/karafka/web/management/actions/base.rb#21
  def already; end

  # @return [Array<String>] topics available in the cluster
  #
  # source://karafka-web//lib/karafka/web/management/actions/base.rb#26
  def existing_topics_names; end

  # @return [String] green colored word "successfully"
  #
  # source://karafka-web//lib/karafka/web/management/actions/base.rb#16
  def successfully; end
end

# Cleans the boot file from Karafka Web-UI details.
#
# source://karafka-web//lib/karafka/web/management/actions/clean_boot_file.rb#8
class Karafka::Web::Management::Actions::CleanBootFile < ::Karafka::Web::Management::Actions::Base
  # Removes the Web-UI boot file data
  #
  # source://karafka-web//lib/karafka/web/management/actions/clean_boot_file.rb#15
  def call; end
end

# Web-UI enabled code
#
# source://karafka-web//lib/karafka/web/management/actions/clean_boot_file.rb#10
Karafka::Web::Management::Actions::CleanBootFile::ENABLER_CODE = T.let(T.unsafe(nil), String)

# Creates the records needed for the Web-UI to operate.
# It creates "almost" empty states because the rest is handled via migrations
#
# source://karafka-web//lib/karafka/web/management/actions/create_initial_states.rb#9
class Karafka::Web::Management::Actions::CreateInitialStates < ::Karafka::Web::Management::Actions::Base
  # Creates the initial states for the Web-UI if needed (if they don't exist)
  #
  # source://karafka-web//lib/karafka/web/management/actions/create_initial_states.rb#22
  def call; end

  private

  # @param type [String] type of state
  # @return [String] message that the state was created
  #
  # source://karafka-web//lib/karafka/web/management/actions/create_initial_states.rb#70
  def created(type); end

  # @param type [String] type of state
  # @return [String] message that the state is being created
  #
  # source://karafka-web//lib/karafka/web/management/actions/create_initial_states.rb#64
  def creating(type); end

  # @param type [String] type of state
  # @return [String] exists message
  #
  # source://karafka-web//lib/karafka/web/management/actions/create_initial_states.rb#58
  def exists(type); end

  # @param topic [String] topic name
  # @return [Boolean] true if there is already an initial record in a given topic
  #
  # source://karafka-web//lib/karafka/web/management/actions/create_initial_states.rb#52
  def exists?(topic); end
end

# Default metrics state
#
# source://karafka-web//lib/karafka/web/management/actions/create_initial_states.rb#17
Karafka::Web::Management::Actions::CreateInitialStates::DEFAULT_METRICS = T.let(T.unsafe(nil), Hash)

# Whole default empty state
# This will be further migrated by the migrator
#
# source://karafka-web//lib/karafka/web/management/actions/create_initial_states.rb#12
Karafka::Web::Management::Actions::CreateInitialStates::DEFAULT_STATE = T.let(T.unsafe(nil), Hash)

# Creates all the needed topics (if they don't exist).
# It does **not** populate data.
#
# source://karafka-web//lib/karafka/web/management/actions/create_topics.rb#9
class Karafka::Web::Management::Actions::CreateTopics < ::Karafka::Web::Management::Actions::Base
  # Runs the creation process
  #
  # @note The order of creation of those topics is important. In order to support the
  #   zero-downtime bootstrap, we use the presence of the states topic and its initial
  #   state existence as an indicator that the setup went as expected. It the consumers
  #   states topic exists and contains needed data, it means all went as expected and that
  #   topics created before it also exist (as no error).
  # @param replication_factor [Integer] replication factor for Web-UI topics
  #
  # source://karafka-web//lib/karafka/web/management/actions/create_topics.rb#19
  def call(replication_factor); end

  private

  # @param topic_name [String] name of the topic that we created
  # @return [String] formatted message
  #
  # source://karafka-web//lib/karafka/web/management/actions/create_topics.rb#153
  def created(topic_name); end

  # @param topic_name [String] name of the topic that we are creating
  # @return [String] formatted message
  #
  # source://karafka-web//lib/karafka/web/management/actions/create_topics.rb#147
  def creating(topic_name); end

  # @param topic_name [String] name of the topic that exists
  # @return [String] formatted message
  #
  # source://karafka-web//lib/karafka/web/management/actions/create_topics.rb#141
  def exists(topic_name); end
end

# Removes the Web-UI topics from Kafka
#
# source://karafka-web//lib/karafka/web/management/actions/delete_topics.rb#8
class Karafka::Web::Management::Actions::DeleteTopics < ::Karafka::Web::Management::Actions::Base
  # Removes the Web-UI topics
  #
  # source://karafka-web//lib/karafka/web/management/actions/delete_topics.rb#10
  def call; end
end

# @note This runs on each process start that has `karafka.rb`. It needs to be executed
#   also in the context of other processes types and not only karafka server, because it
#   installs producers instrumentation and routing as well.
#
# source://karafka-web//lib/karafka/web/management/actions/enable.rb#10
class Karafka::Web::Management::Actions::Enable < ::Karafka::Web::Management::Actions::Base
  # Enables routing consumer group and subscribes Web-UI listeners
  #
  # source://karafka-web//lib/karafka/web/management/actions/enable.rb#12
  def call; end

  private

  # Enables all the needed routes
  #
  # source://karafka-web//lib/karafka/web/management/actions/enable.rb#38
  def extend_routing; end

  # Enables tracking if it was not explicitly disabled by the user
  #
  # source://karafka-web//lib/karafka/web/management/actions/enable.rb#31
  def setup_tracking_activity; end

  # In most cases we want to close the producer if possible.
  # While we cannot do it easily in user processes and we should rely on WaterDrop
  # finalization logic, we can do it in `karafka server` on terminate
  #
  # In other places, this producer anyhow should not be used.
  #
  # source://karafka-web//lib/karafka/web/management/actions/enable.rb#117
  def subscribe_to_close_web_producer; end

  # Subscribes with all needed listeners
  #
  # source://karafka-web//lib/karafka/web/management/actions/enable.rb#94
  def subscribe_to_monitor; end
end

# Extends the boot file with Web components
#
# source://karafka-web//lib/karafka/web/management/actions/extend_boot_file.rb#8
class Karafka::Web::Management::Actions::ExtendBootFile < ::Karafka::Web::Management::Actions::Base
  # Adds needed code
  #
  # source://karafka-web//lib/karafka/web/management/actions/extend_boot_file.rb#24
  def call; end
end

# Code that is needed in the `karafka.rb` to connect Web UI to Karafka
#
# source://karafka-web//lib/karafka/web/management/actions/extend_boot_file.rb#10
Karafka::Web::Management::Actions::ExtendBootFile::ENABLER_CODE = T.let(T.unsafe(nil), String)

# Template with initial Web UI configuration
# Session secret needs to be set per user and per env
#
# source://karafka-web//lib/karafka/web/management/actions/extend_boot_file.rb#14
Karafka::Web::Management::Actions::ExtendBootFile::SETUP_TEMPLATE = T.let(T.unsafe(nil), String)

# Command to migrate states data
# Useful when we have older schema and need to move forward
#
# source://karafka-web//lib/karafka/web/management/actions/migrate_states_data.rb#9
class Karafka::Web::Management::Actions::MigrateStatesData < ::Karafka::Web::Management::Actions::Base
  # Runs needed migrations (if any) on the states topics
  #
  # source://karafka-web//lib/karafka/web/management/actions/migrate_states_data.rb#11
  def call; end
end

# Namespace for storing migrations of our Web UI topics data
#
# source://karafka-web//lib/karafka/web.rb#0
module Karafka::Web::Management::Migrations; end

# Base for all our migrations
#
# Each migration **MUST** have a `#migrate` method defined
# Migrations are expected to modify the provided state **IN PLACE**
#
# source://karafka-web//lib/karafka/web/management/migrations/0_base.rb#12
class Karafka::Web::Management::Migrations::Base
  include ::Karafka::Core::Helpers::Time

  class << self
    # @param version [String] sem-ver version
    # @return [Boolean] is the given migration applicable
    #
    # source://karafka-web//lib/karafka/web/management/migrations/0_base.rb#24
    def applicable?(version); end

    # @return [Integer] index for sorting. Older migrations are always applied first
    #
    # source://karafka-web//lib/karafka/web/management/migrations/0_base.rb#34
    def index; end

    # @param state [Hash] deserialized state to be modified
    # @raise [NotImplementedError]
    #
    # source://karafka-web//lib/karafka/web/management/migrations/0_base.rb#29
    def migrate(state); end

    # @return [Array<Class>] array with migrations sorted from oldest to latest. This is
    #   the order in which they need to be applied
    #
    # source://karafka-web//lib/karafka/web/management/migrations/0_base.rb#47
    def sorted_descendants; end

    # What resource does it relate it
    # One migration should modify only one resource type
    #
    # source://karafka-web//lib/karafka/web/management/migrations/0_base.rb#20
    def type; end

    # What resource does it relate it
    # One migration should modify only one resource type
    #
    # source://karafka-web//lib/karafka/web/management/migrations/0_base.rb#20
    def type=(_arg0); end

    # First version that should **NOT** be affected by this migration
    #
    # source://karafka-web//lib/karafka/web/management/migrations/0_base.rb#17
    def versions_until; end

    # First version that should **NOT** be affected by this migration
    #
    # source://karafka-web//lib/karafka/web/management/migrations/0_base.rb#17
    def versions_until=(_arg0); end
  end
end

# Adds bytes_sent and bytes_received to all the aggregated metrics samples, so we have
# charts that do not have to fill gaps or check anything
#
# source://karafka-web//lib/karafka/web/management/migrations/1699543515_fill_missing_received_and_sent_bytes_in_consumers_metrics.rb#9
class Karafka::Web::Management::Migrations::FillMissingReceivedAndSentBytesInConsumersMetrics < ::Karafka::Web::Management::Migrations::Base
  # @param state [Hash] metrics state
  #
  # source://karafka-web//lib/karafka/web/management/migrations/1699543515_fill_missing_received_and_sent_bytes_in_consumers_metrics.rb#14
  def migrate(state); end
end

# Similar to filling in consumers metrics, we initialize this with zeros so it is always
# present as expected
#
# source://karafka-web//lib/karafka/web/management/migrations/1699543515_fill_missing_received_and_sent_bytes_in_consumers_state.rb#9
class Karafka::Web::Management::Migrations::FillMissingReceivedAndSentBytesInConsumersState < ::Karafka::Web::Management::Migrations::Base
  # @param state [Hash]
  #
  # source://karafka-web//lib/karafka/web/management/migrations/1699543515_fill_missing_received_and_sent_bytes_in_consumers_state.rb#15
  def migrate(state); end
end

# Moves to using lag total as a normalization for both lags
#
# source://karafka-web//lib/karafka/web/management/migrations/1706607960_introduce_lag_total_in_metrics.rb#8
class Karafka::Web::Management::Migrations::IntroduceLagTotalInMetrics < ::Karafka::Web::Management::Migrations::Base
  # @param state [Hash]
  #
  # source://karafka-web//lib/karafka/web/management/migrations/1706607960_introduce_lag_total_in_metrics.rb#13
  def migrate(state); end
end

# Moves to using lag total as a normalization for both lags
#
# source://karafka-web//lib/karafka/web/management/migrations/1706607960_introduce_lag_total_in_states.rb#8
class Karafka::Web::Management::Migrations::IntroduceLagTotalInStates < ::Karafka::Web::Management::Migrations::Base
  # @param state [Hash]
  #
  # source://karafka-web//lib/karafka/web/management/migrations/1706607960_introduce_lag_total_in_states.rb#13
  def migrate(state); end
end

# Introduce waiting in consumers metrics to complement busy and enqueued for jobs metrics
#
# source://karafka-web//lib/karafka/web/management/migrations/1700234522_introduce_waiting_in_consumers_metrics.rb#8
class Karafka::Web::Management::Migrations::IntroduceWaitingInConsumersMetrics < ::Karafka::Web::Management::Migrations::Base
  # @param state [Hash]
  #
  # source://karafka-web//lib/karafka/web/management/migrations/1700234522_introduce_waiting_in_consumers_metrics.rb#13
  def migrate(state); end
end

# Introduce waiting in consumers metrics to complement busy and enqueued for jobs stats
#
# source://karafka-web//lib/karafka/web/management/migrations/1700234522_introduce_waiting_in_consumers_state.rb#8
class Karafka::Web::Management::Migrations::IntroduceWaitingInConsumersState < ::Karafka::Web::Management::Migrations::Base
  # @param state [Hash]
  #
  # source://karafka-web//lib/karafka/web/management/migrations/1700234522_introduce_waiting_in_consumers_state.rb#13
  def migrate(state); end
end

# Moves unused "processing" that was used instead of "busy" in older versions
#
# source://karafka-web//lib/karafka/web/management/migrations/1700234522_remove_processing_from_consumers_metrics.rb#8
class Karafka::Web::Management::Migrations::RemoveProcessingFromConsumersMetrics < ::Karafka::Web::Management::Migrations::Base
  # @param state [Hash]
  #
  # source://karafka-web//lib/karafka/web/management/migrations/1700234522_remove_processing_from_consumers_metrics.rb#13
  def migrate(state); end
end

# Moves unused "processing" that was used instead of "busy" in older versions
#
# source://karafka-web//lib/karafka/web/management/migrations/1700234522_remove_processing_from_consumers_state.rb#8
class Karafka::Web::Management::Migrations::RemoveProcessingFromConsumersState < ::Karafka::Web::Management::Migrations::Base
  # @param state [Hash]
  #
  # source://karafka-web//lib/karafka/web/management/migrations/1700234522_remove_processing_from_consumers_state.rb#13
  def migrate(state); end
end

# Renames total lag to hybrid to better represent what it is
#
# source://karafka-web//lib/karafka/web/management/migrations/1706611396_rename_lag_total_to_lag_hybrid_in_metrics.rb#8
class Karafka::Web::Management::Migrations::RenameLagTotalToLagHybridInMetrics < ::Karafka::Web::Management::Migrations::Base
  # @param state [Hash]
  #
  # source://karafka-web//lib/karafka/web/management/migrations/1706611396_rename_lag_total_to_lag_hybrid_in_metrics.rb#13
  def migrate(state); end
end

# Renames total lag to hybrid to better represent what it is
#
# source://karafka-web//lib/karafka/web/management/migrations/1706611396_rename_lag_total_to_lag_hybrid_in_states.rb#8
class Karafka::Web::Management::Migrations::RenameLagTotalToLagHybridInStates < ::Karafka::Web::Management::Migrations::Base
  # @param state [Hash]
  #
  # source://karafka-web//lib/karafka/web/management/migrations/1706611396_rename_lag_total_to_lag_hybrid_in_states.rb#13
  def migrate(state); end
end

# Initial migration that sets the consumers metrics initial first state.
# This is the basic of metrics as they were when they were introduced.
#
# source://karafka-web//lib/karafka/web/management/migrations/0_set_initial_consumers_metrics.rb#9
class Karafka::Web::Management::Migrations::SetInitialConsumersMetrics < ::Karafka::Web::Management::Migrations::Base
  # @param state [Hash] initial empty state
  #
  # source://karafka-web//lib/karafka/web/management/migrations/0_set_initial_consumers_metrics.rb#15
  def migrate(state); end
end

# Initial migration that sets the consumers state initial first state.
# This is the basic of state as they were when they were introduced.
#
# source://karafka-web//lib/karafka/web/management/migrations/0_set_initial_consumers_state.rb#9
class Karafka::Web::Management::Migrations::SetInitialConsumersState < ::Karafka::Web::Management::Migrations::Base
  # @param state [Hash]
  #
  # source://karafka-web//lib/karafka/web/management/migrations/0_set_initial_consumers_state.rb#15
  def migrate(state); end
end

# Since we have introduced notion of pause listeners, we need to reflect this in the
# UI, so the scaling changes are visible
#
# source://karafka-web//lib/karafka/web/management/migrations/1704722380_split_listeners_into_active_and_paused_in_metrics.rb#9
class Karafka::Web::Management::Migrations::SplitListenersIntoActiveAndPausedInMetrics < ::Karafka::Web::Management::Migrations::Base
  # @param state [Hash]
  #
  # source://karafka-web//lib/karafka/web/management/migrations/1704722380_split_listeners_into_active_and_paused_in_metrics.rb#14
  def migrate(state); end
end

# Since we have introduced notion of pause listeners, we need to reflect this in the
# UI, so the scaling changes are visible
#
# source://karafka-web//lib/karafka/web/management/migrations/1704722380_split_listeners_into_active_and_paused_in_states.rb#9
class Karafka::Web::Management::Migrations::SplitListenersIntoActiveAndPausedInStates < ::Karafka::Web::Management::Migrations::Base
  # @param state [Hash]
  #
  # source://karafka-web//lib/karafka/web/management/migrations/1704722380_split_listeners_into_active_and_paused_in_states.rb#14
  def migrate(state); end
end

# Migrator used to run migrations on the states topics
# There are cases during upgrades, where extra fields may be added and other data, so in
# order not to deal with cases of some information missing, we can just migrate the data
# and ensure all the fields that we require after upgrade are present
#
# Migrations are similar to the once that are present in Ruby on Rails conceptually.
#
# We take our most recent state and we can alter it "in place". The altered result will be
# passed to the consecutive migrations and then republished back to Kafka. This allows us
# to manage Web UI aggregated data easily.
#
# @note We do not migrate the consumers reports for the following reasons:
#   - if would be extremely hard to migrate them as they are being published and can be still
#   published when the migrations are running
#   - we would have to run migrations on each message
#   - we already have a mechanism in the processing consumer that skips outdated records for
#   rolling migrations
#   - those records are short-lived and the expectation is for the user not to run old and
#   new consumers together for an extensive period of time
# @note It will raise an error if we try to run migrations but the schemas we want to operate
#   are newer. This will prevent us from damaging the data and ensures that we only move
#   forward with the migrations. This can happen in case of a rolling upgrade, where old
#   instance that is going to be terminated would get a temporary assignment with already
#   migrated state.
#
# source://karafka-web//lib/karafka/web/management/migrator.rb#33
class Karafka::Web::Management::Migrator
  include ::Karafka::Web::Processing::Consumers::Aggregators

  # Picks needed data from Kafka, alters it with migrations and puts the updated data
  # back into Kafka. This ensures, that our Web UI topics that hold aggregated data are
  # always aligned with the Web UI expectations
  #
  # @note To simplify things we always migrate and update all the topics data even if only
  #   part was migrated. That way we always ensure that all the elements are up to date
  #
  # source://karafka-web//lib/karafka/web/management/migrator.rb#43
  def call; end

  private

  # @return [Hash] current consumers metrics most recent state
  #
  # source://karafka-web//lib/karafka/web/management/migrator.rb#111
  def consumers_metrics; end

  # @return [Hash] current consumers states most recent state
  #
  # source://karafka-web//lib/karafka/web/management/migrator.rb#106
  def consumers_state; end

  # Raise an exception if there would be an attempt to run migrations on a newer schema for
  # any states we manage. We can only move forward, so attempt to migrate for example from
  # 1.0.0 to 0.9.0 should be considered and error.
  #
  # source://karafka-web//lib/karafka/web/management/migrator.rb#54
  def ensure_migrable!; end

  # Applies migrations if needed and mutates the in-memory data
  #
  # @return [Boolean] were there any migrations applied
  #
  # source://karafka-web//lib/karafka/web/management/migrator.rb#75
  def migrate; end

  # Publishes all the states migrated records
  #
  # source://karafka-web//lib/karafka/web/management/migrator.rb#92
  def publish; end
end

# Namespace used to encapsulate all the components needed to process the states data and
# store it back in Kafka
#
# source://karafka-web//lib/karafka/web.rb#0
module Karafka::Web::Processing; end

# Consumer used to squash and process statistics coming from particular processes, so this
# data can be read and used. We consume this info overwriting the data we previously had
# (if any)
#
# source://karafka-web//lib/karafka/web/processing/consumer.rb#11
class Karafka::Web::Processing::Consumer < ::Karafka::BaseConsumer
  include ::Karafka::Core::Helpers::Time

  # Aggregates consumers state into a single current state representation
  #
  # source://karafka-web//lib/karafka/web/processing/consumer.rb#15
  def consume; end

  # Flush final state on shutdown
  #
  # source://karafka-web//lib/karafka/web/processing/consumer.rb#67
  def shutdown; end

  private

  # Prepares all the initial objects and ensures all the needed states are as expected
  #
  # @note We do not run it in the `#initialize` anymore as `#initialize` happens before
  #   the work starts so errors there are handled differently. We want this initial setup
  #   to operate and fail (if needed) during messages consumption phase
  #
  # source://karafka-web//lib/karafka/web/processing/consumer.rb#77
  def bootstrap!; end

  # Flushes the state of the Web-UI to the DB
  #
  # source://karafka-web//lib/karafka/web/processing/consumer.rb#103
  def dispatch; end

  # Persists the new current state by flushing it to Kafka
  #
  # source://karafka-web//lib/karafka/web/processing/consumer.rb#130
  def flush; end

  # Materializes the current state and metrics for flushing
  #
  # source://karafka-web//lib/karafka/web/processing/consumer.rb#117
  def materialize; end

  # @return [Boolean] is it time to persist the new current state
  #
  # source://karafka-web//lib/karafka/web/processing/consumer.rb#112
  def periodic_flush?; end

  # Ensures that the aggregated data complies with our schema expectation.
  # If you ever get to this place, this is probably a bug and you should report it.
  #
  # source://karafka-web//lib/karafka/web/processing/consumer.rb#124
  def validate!; end
end

# Namespace for consumer sub-components
#
# source://karafka-web//lib/karafka/web/management/migrator.rb#0
module Karafka::Web::Processing::Consumers; end

# Namespace for data aggregators that track changes based on the incoming reports and
# aggregate metrics over time
#
# source://karafka-web//lib/karafka/web/management/migrator.rb#0
module Karafka::Web::Processing::Consumers::Aggregators; end

# Base for all the consumer related aggregators that operate on processes reports
#
# @note It is important to understand, that we operate here on a moment in time and this
#   moment may not mean "current" now. There might have been a lag and we may be catching
#   up on older states. This is why we use `@aggregated_from` time instead of the real
#   now. In case of a lag, we want to aggregate and catch up with data, without
#   assigning it to the time of processing but aligning it with the time from which the
#   given reports came. This allows us to compensate for the potential lag related to
#   rebalances, downtimes, failures, etc.
#
# source://karafka-web//lib/karafka/web/processing/consumers/aggregators/base.rb#19
class Karafka::Web::Processing::Consumers::Aggregators::Base
  include ::Karafka::Core::Helpers::Time

  # @return [Base] a new instance of Base
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/aggregators/base.rb#22
  def initialize; end

  # Adds report to the internal active reports hash and updates the aggregation time
  # for internal time reference usage
  #
  # @param report [Hash] incoming process state report
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/aggregators/base.rb#29
  def add(report); end

  private

  # Updates the report for given process in memory
  #
  # @param report [Hash]
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/aggregators/base.rb#38
  def memoize_process_report(report); end

  # Updates the time of the aggregation
  #
  # @note Since this runs before eviction because of age, we always assume there is at
  #   least one report from which we can take the dispatch time
  # @return [Float] time of the aggregation
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/aggregators/base.rb#48
  def update_aggregated_from; end
end

# Aggregates metrics for metrics topic. Tracks consumers data and converts it into a
# state that can then be used to enrich previous time based states to get a time-series
# values for charts and metrics
#
# source://karafka-web//lib/karafka/web/processing/consumers/aggregators/metrics.rb#11
class Karafka::Web::Processing::Consumers::Aggregators::Metrics < ::Karafka::Web::Processing::Consumers::Aggregators::Base
  # @return [Metrics] a new instance of Metrics
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/aggregators/metrics.rb#16
  def initialize; end

  # Adds the current report to active reports and removes old once
  #
  # @param report [Hash] single process full report
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/aggregators/metrics.rb#25
  def add_report(report); end

  # Updates the aggregated stats metrics
  #
  # @param stats [Hash] aggregated statistics
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/aggregators/metrics.rb#34
  def add_stats(stats); end

  # Converts our current knowledge into a report hash.
  #
  # @note We materialize the consumers groups time series only here and not in real time,
  #   because we materialize it based on the tracked active collective state. Materializing
  #   on each update that would not be dispatched would be pointless.
  # @return [Hash] Statistics hash
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/aggregators/metrics.rb#48
  def to_h; end

  private

  # Materialize and add consumers groups states into the tracker
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/aggregators/metrics.rb#79
  def add_consumers_groups_metrics; end

  # Evicts outdated reports.
  #
  # @note This eviction differs from the one that we have for the states. For states we
  #   do not evict stopped because we want to report them for a moment. Here we do not
  #   care about what a stopped process was doing and we can also remove it from active
  #   reports.
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/aggregators/metrics.rb#70
  def evict_expired_processes; end

  # Converts our reports data into an iterator per partition
  # Compensates for a case where same partition data would be available for a short
  # period of time in multiple processes reports due to rebalances.
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/aggregators/metrics.rb#138
  def iterate_partitions_data; end

  # Materializes the current state of consumers group data
  #
  # @note We do **not** report on a per partition basis because it would significantly
  #   increase needed storage.
  # @return [Hash] hash with nested consumers and their topics details structure
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/aggregators/metrics.rb#91
  def materialize_consumers_groups_current_state; end

  # @return [Hash] the initial metric taken from Kafka
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/aggregators/metrics.rb#60
  def metrics; end
end

# Current schema version
# This is used for detecting incompatible changes and writing migrations
#
# source://karafka-web//lib/karafka/web/processing/consumers/aggregators/metrics.rb#14
Karafka::Web::Processing::Consumers::Aggregators::Metrics::SCHEMA_VERSION = T.let(T.unsafe(nil), String)

# Aggregator that tracks consumers processes states, aggregates the metrics and converts
# data points into a materialized current state.
#
# There are two types of metrics:
#   - totals - metrics that represent absolute values like number of messages processed
#     in total. Things that need to be incremented/updated with each incoming consumer
#     process report. They cannot be "batch computed" because they do not represent a
#     a state of time but progress.
#   - aggregated state - a state that represents a "snapshot" of things happening right
#     now. Right now is the moment of time on which we operate.
#
# source://karafka-web//lib/karafka/web/processing/consumers/aggregators/state.rb#19
class Karafka::Web::Processing::Consumers::Aggregators::State < ::Karafka::Web::Processing::Consumers::Aggregators::Base
  # @param schema_manager [Karafka::Web::Processing::Consumers::SchemaManager] schema
  #   manager that tracks the compatibility of schemas.
  # @return [State] a new instance of State
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/aggregators/state.rb#27
  def initialize(schema_manager); end

  # Uses provided process state report to update the current materialized state
  #
  # @param report [Hash] consumer process state report
  # @param offset [Integer] offset of the message with the state report. This offset is
  #   needed as we need to be able to get all the consumers reports from a given offset.
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/aggregators/state.rb#36
  def add(report, offset); end

  # @note We return a copy, because we use the internal one to track state changes and
  #   unless we would return a copy, other aggregators could have this mutated in an
  #   unexpected way
  # @return [Array<Hash, Float>] aggregated current stats value and time from which this
  #   aggregation comes from
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/aggregators/state.rb#55
  def stats; end

  # Sets the dispatch time and returns the hash that can be shipped to the states topic
  #
  # @param _args [Object] extra parsing arguments (not used)
  # @return [Hash] Hash that we can use to ship states data to Kafka
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/aggregators/state.rb#63
  def to_h(*_args); end

  private

  # Evicts expired processes from the current state
  # We consider processes dead if they do not report often enough
  #
  # @note We do not evict based on states (stopped), because we want to report the
  #   stopped processes for extra time within the ttl limitations. This makes tracking of
  #   things from UX perspective nicer.
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/aggregators/state.rb#105
  def evict_expired_processes; end

  # Increments the total counters based on the provided report
  #
  # @param report [Hash]
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/aggregators/state.rb#80
  def increment_total_counters(report); end

  # @param report [Hash]
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/aggregators/state.rb#168
  def iterate_partitions(report); end

  # Refreshes the counters that are computed based on incoming reports and not a total sum.
  # For this we use active reports we have in memory. It may not be accurate for the first
  # few seconds but it is much more optimal from performance perspective than computing
  # this fetching all data from Kafka for each view.
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/aggregators/state.rb#121
  def refresh_current_stats; end

  # @return [Hash] hash with current state from Kafka
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/aggregators/state.rb#74
  def state; end

  # Registers or updates the given process state based on the report
  #
  # @param report [Hash]
  # @param offset [Integer]
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/aggregators/state.rb#91
  def update_process_state(report, offset); end
end

# Current schema version
# This can be used in the future for detecting incompatible changes and writing
# migrations
#
# source://karafka-web//lib/karafka/web/processing/consumers/aggregators/state.rb#23
Karafka::Web::Processing::Consumers::Aggregators::State::SCHEMA_VERSION = T.let(T.unsafe(nil), String)

# Consumer tracking related contracts
#
# source://karafka-web//lib/karafka/web.rb#0
module Karafka::Web::Processing::Consumers::Contracts; end

# Contract used to validate the stats that are both present in state and metrics
#
# source://karafka-web//lib/karafka/web/processing/consumers/contracts/aggregated_stats.rb#10
class Karafka::Web::Processing::Consumers::Contracts::AggregatedStats < ::Karafka::Web::Contracts::Base; end

# Contract that describes the schema for metric reporting
#
# source://karafka-web//lib/karafka/web/processing/consumers/contracts/metrics.rb#9
class Karafka::Web::Processing::Consumers::Contracts::Metrics < ::Karafka::Web::Contracts::Base; end

# State process details contract
#
# source://karafka-web//lib/karafka/web/processing/consumers/contracts/process.rb#9
class Karafka::Web::Processing::Consumers::Contracts::Process < ::Karafka::Web::Contracts::Base; end

# Contract used to ensure the consistency of the data generated to the consumers states
# topic
#
# source://karafka-web//lib/karafka/web/processing/consumers/contracts/state.rb#11
class Karafka::Web::Processing::Consumers::Contracts::State < ::Karafka::Web::Contracts::Base; end

# Valid schema manager states
#
# source://karafka-web//lib/karafka/web/processing/consumers/contracts/state.rb#15
Karafka::Web::Processing::Consumers::Contracts::State::VALID_SCHEMA_STATES = T.let(T.unsafe(nil), Array)

# Topic metrics checks
#
# source://karafka-web//lib/karafka/web/processing/consumers/contracts/topic_stats.rb#9
class Karafka::Web::Processing::Consumers::Contracts::TopicStats < ::Karafka::Web::Contracts::Base; end

# Fetches the current consumers historical metrics data
#
# source://karafka-web//lib/karafka/web/processing/consumers/metrics.rb#8
class Karafka::Web::Processing::Consumers::Metrics
  class << self
    # Fetch the current metrics data that is expected to exist
    #
    # @return [Hash] latest (current) aggregated metrics state
    #
    # source://karafka-web//lib/karafka/web/processing/consumers/metrics.rb#13
    def current!; end
  end
end

# Schema manager is responsible for making sure, that the consumers reports messages that
# we consume have a compatible schema with the current process that is suppose to
# materialize them.
#
# In general we always support at least one major version back and we recommend upgrades
# to previous versions (0.5 => 0.6 => 0.7)
#
# This is needed in scenarios where a rolling deploy would get new karafka processes
# reporting data but consumption would still run in older.
#
# source://karafka-web//lib/karafka/web/processing/consumers/schema_manager.rb#16
class Karafka::Web::Processing::Consumers::SchemaManager
  # @return [SchemaManager] a new instance of SchemaManager
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/schema_manager.rb#24
  def initialize; end

  # @param message [Karafka::Messages::Message] consumer report
  # @return [Symbol] is the given message using older, newer or current schema
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/schema_manager.rb#31
  def call(message); end

  # Moves the schema manager state to incompatible to indicate in the Web-UI that we
  # cannot move forward because schema is incompatible.
  #
  # @note The state switch is one-direction only. If we encounter an incompatible message
  #   we need to stop processing so further checks even with valid should not switch it
  #   back to valid
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/schema_manager.rb#51
  def invalidate!; end

  # @return [String] state that we can use in the materialized state for the UI reporting
  #
  # source://karafka-web//lib/karafka/web/processing/consumers/schema_manager.rb#56
  def to_s; end
end

# Current reports version for comparing
#
# source://karafka-web//lib/karafka/web/processing/consumers/schema_manager.rb#18
Karafka::Web::Processing::Consumers::SchemaManager::CURRENT_VERSION = T.let(T.unsafe(nil), Gem::Version)

# Fetches the current consumer processes aggregated state
#
# source://karafka-web//lib/karafka/web/processing/consumers/state.rb#8
class Karafka::Web::Processing::Consumers::State
  class << self
    # Fetch the current consumers state that is expected to exist
    #
    # @return [Hash] last (current) aggregated processes state
    #
    # source://karafka-web//lib/karafka/web/processing/consumers/state.rb#13
    def current!; end
  end
end

# Object responsible for publishing states data back into Kafka so it can be used in the UI
#
# source://karafka-web//lib/karafka/web/processing/publisher.rb#7
class Karafka::Web::Processing::Publisher
  class << self
    # Publishes data back to Kafka in an async fashion
    #
    # @param consumers_state [Hash] consumers current state
    # @param consumers_metrics [Hash] consumers current metrics
    #
    # source://karafka-web//lib/karafka/web/processing/publisher.rb#13
    def publish(consumers_state, consumers_metrics); end

    # Publishes data back to Kafka in a sync fashion
    #
    # @param consumers_state [Hash] consumers current state
    # @param consumers_metrics [Hash] consumers current metrics
    #
    # source://karafka-web//lib/karafka/web/processing/publisher.rb#23
    def publish!(consumers_state, consumers_metrics); end

    private

    # Converts the states into format that we can dispatch to Kafka
    #
    # @param consumers_state [Hash] consumers current state
    # @param consumers_metrics [Hash] consumers current metrics
    # @return [Array<Hash>]
    #
    # source://karafka-web//lib/karafka/web/processing/publisher.rb#36
    def prepare_data(consumers_state, consumers_metrics); end
  end
end

# Allows us to accumulate and track time series data with given resolution
#
# We aggregate for last:
#   - 7 days (every day)
#   - 24 hours (every hour)
#   - 1 hour (every minute) + the most recent as an update every time (leading)
#
# @note Please note we publish always **absolute** metrics and not deltas in reference to
#   a given time window. This needs to be computed in the frontend as we want to have
#   state facts in the storage.
# @note Please note we evict and cleanup data only before we want to use it. This will put
#   more stress on memory but makes this tracker 70-90% faster. Since by default we anyhow
#   sample every few seconds, this trade-off makes sense.
#
# source://karafka-web//lib/karafka/web/processing/time_series_tracker.rb#20
class Karafka::Web::Processing::TimeSeriesTracker
  include ::Karafka::Core::Helpers::Time

  # @param existing [Hash] existing historical metrics (may be empty for the first state)
  # @return [TimeSeriesTracker] a new instance of TimeSeriesTracker
  #
  # source://karafka-web//lib/karafka/web/processing/time_series_tracker.rb#54
  def initialize(existing); end

  # Adds current state into the states for tracking
  #
  # @param current [Hash] hash with current state
  # @param state_time [Float] float UTC time from which the state comes
  #
  # source://karafka-web//lib/karafka/web/processing/time_series_tracker.rb#65
  def add(current, state_time); end

  # Evicts expired and duplicated series and returns the cleaned hash
  #
  # @return [Hash] aggregated historicals hash
  #
  # source://karafka-web//lib/karafka/web/processing/time_series_tracker.rb#72
  def to_h; end

  private

  # Removes historical metrics that are beyond our expected range, so we maintain a stable
  # count and not overload the states topic with extensive data.
  #
  # source://karafka-web//lib/karafka/web/processing/time_series_tracker.rb#103
  def evict; end

  # Import existing previous historical metrics as they are
  #
  # @param existing [Hash] existing historical metrics
  #
  # source://karafka-web//lib/karafka/web/processing/time_series_tracker.rb#83
  def import_existing(existing); end

  # Injects the current most recent stats sample into each of the time ranges on which we
  # operate. This allows us on all the charts to present the most recent value before a
  # given time window is completed
  #
  # @param current [Hash] current stats
  # @param state_time [Float] time from which this state comes
  #
  # source://karafka-web//lib/karafka/web/processing/time_series_tracker.rb#95
  def inject(current, state_time); end
end

# How many samples and in what resolution should we track for given time range
# are full in the UI
#
# @note We add one more than we want to display for delta computation when ranges
#
# source://karafka-web//lib/karafka/web/processing/time_series_tracker.rb#26
Karafka::Web::Processing::TimeSeriesTracker::TIME_RANGES = T.let(T.unsafe(nil), Hash)

# Namespace used to encapsulate all components needed to track and report states of particular
# processes
#
# source://karafka-web//lib/karafka/web/config.rb#0
module Karafka::Web::Tracking; end

# Namespace for all the things related to tracking consumers and consuming processes
#
# source://karafka-web//lib/karafka/web/config.rb#0
module Karafka::Web::Tracking::Consumers; end

# Consumer tracking related contracts
#
# source://karafka-web//lib/karafka/web/config.rb#0
module Karafka::Web::Tracking::Consumers::Contracts; end

# Expected data for each consumer group
# It's mostly about subscription groups details
#
# source://karafka-web//lib/karafka/web/tracking/consumers/contracts/consumer_group.rb#11
class Karafka::Web::Tracking::Consumers::Contracts::ConsumerGroup < ::Karafka::Web::Contracts::Base; end

# Contract for the job reporting details
#
# source://karafka-web//lib/karafka/web/tracking/consumers/contracts/job.rb#9
class Karafka::Web::Tracking::Consumers::Contracts::Job < ::Karafka::Web::Contracts::Base; end

# Partition metrics required for web to operate
#
# source://karafka-web//lib/karafka/web/tracking/consumers/contracts/partition.rb#9
class Karafka::Web::Tracking::Consumers::Contracts::Partition < ::Karafka::Web::Contracts::Base; end

# Main consumer process related reporting schema
#
# Any outgoing reporting needs to match this format for it to work with the statuses
# consumer.
#
# source://karafka-web//lib/karafka/web/tracking/consumers/contracts/report.rb#12
class Karafka::Web::Tracking::Consumers::Contracts::Report < ::Karafka::Web::Contracts::Base; end

# Expected data for each subscription group
# It's mostly about topics details
#
# source://karafka-web//lib/karafka/web/tracking/consumers/contracts/subscription_group.rb#10
class Karafka::Web::Tracking::Consumers::Contracts::SubscriptionGroup < ::Karafka::Web::Contracts::Base; end

# Expected topic information that needs to go out
#
# source://karafka-web//lib/karafka/web/tracking/consumers/contracts/topic.rb#9
class Karafka::Web::Tracking::Consumers::Contracts::Topic < ::Karafka::Web::Contracts::Base; end

# Consumer monitoring related listeners
#
# source://karafka-web//lib/karafka/web/config.rb#0
module Karafka::Web::Tracking::Consumers::Listeners; end

# Base consumers processes related listener
#
# source://karafka-web//lib/karafka/web/tracking/consumers/listeners/base.rb#10
class Karafka::Web::Tracking::Consumers::Listeners::Base
  include ::Karafka::Core::Helpers::Time
  extend ::Forwardable

  # source://forwardable/1.3.3/forwardable.rb#231
  def report(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def report!(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def track(*args, **_arg1, &block); end

  private

  # @return [Object] reporter in use
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/base.rb#25
  def reporter; end

  # @return [Object] sampler in use
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/base.rb#20
  def sampler; end
end

# Listener needed to start schedulers and other things that we need to collect and report
# data. We do not want to start this when code is loaded because it may not be fork
# compatible that way
#
# source://karafka-web//lib/karafka/web/tracking/consumers/listeners/booting.rb#11
class Karafka::Web::Tracking::Consumers::Listeners::Booting < ::Karafka::Web::Tracking::Consumers::Listeners::Base
  # Starts (if needed) the Web UI tracking scheduler thread that periodically pings
  # reporters to report needed data (when it is time).
  #
  # @param _event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/booting.rb#16
  def on_app_running(_event); end

  # Updates the web producer after fork if needed and adds ppid to nodes
  #
  # @param _event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/booting.rb#24
  def on_swarm_node_after_fork(_event); end
end

# Listener for listening on connections related events like polling, etc
#
# source://karafka-web//lib/karafka/web/tracking/consumers/listeners/connections.rb#9
class Karafka::Web::Tracking::Consumers::Listeners::Connections < ::Karafka::Web::Tracking::Consumers::Listeners::Base
  # When fetch loop is done it means this subscription group is no longer active and we
  # should stop reporting. The listener was stopped.
  #
  # @param event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/connections.rb#21
  def on_connection_listener_after_fetch_loop(event); end

  # Set first poll time before we start fetching so we always have a poll time
  # and we don't have to worry about it being always available
  #
  # @param event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/connections.rb#13
  def on_connection_listener_before_fetch_loop(event); end

  # Tracks the moment a poll happened on a given subscription group
  #
  # @param event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/connections.rb#35
  def on_connection_listener_fetch_loop_received(event); end
end

# Listener related to tracking errors, DLQs, and retries metrics for the Web UI
#
# source://karafka-web//lib/karafka/web/tracking/consumers/listeners/errors.rb#9
class Karafka::Web::Tracking::Consumers::Listeners::Errors < ::Karafka::Web::Tracking::Consumers::Listeners::Base
  include ::Karafka::Web::Tracking::Helpers::ErrorInfo

  # Count retries
  #
  # @param _event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/errors.rb#59
  def on_consumer_consuming_retry(_event); end

  # Count dead letter queue messages dispatches
  #
  # @param _event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/errors.rb#50
  def on_dead_letter_queue_dispatched(_event); end

  # Collects errors info and counts errors
  #
  # @param event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/errors.rb#20
  def on_error_occurred(event); end

  private

  # @param consumer [::Karafka::BaseConsumer]
  # @return [Hash] hash with consumer specific info for details of error
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/errors.rb#69
  def extract_consumer_info(consumer); end
end

# Schema used by consumers error reporting
#
# source://karafka-web//lib/karafka/web/tracking/consumers/listeners/errors.rb#13
Karafka::Web::Tracking::Consumers::Listeners::Errors::SCHEMA_VERSION = T.let(T.unsafe(nil), String)

# Tracks pausing and un-pausing of topics partitions for both user requested and
# automatic events.
#
# source://karafka-web//lib/karafka/web/tracking/consumers/listeners/pausing.rb#10
class Karafka::Web::Tracking::Consumers::Listeners::Pausing < ::Karafka::Web::Tracking::Consumers::Listeners::Base
  # Indicate pause ended
  #
  # @param event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/pausing.rb#26
  def on_client_resume(event); end

  # Indicate pause
  #
  # @param event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/pausing.rb#14
  def on_consumer_consuming_pause(event); end

  private

  # @param event [Karafka::Core::Monitoring::Event]
  # @return [String] pause id built from consumer group and topic details
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/pausing.rb#36
  def pause_id(event); end
end

# Listener that is used to collect metrics related to work processing
#
# source://karafka-web//lib/karafka/web/tracking/consumers/listeners/processing.rb#9
class Karafka::Web::Tracking::Consumers::Listeners::Processing < ::Karafka::Web::Tracking::Consumers::Listeners::Base
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/processing.rb#29
  def on_consumer_before_schedule_consume(event); end

  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/processing.rb#29
  def on_consumer_before_schedule_revoked(event); end

  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/processing.rb#29
  def on_consumer_before_schedule_shutdown(event); end

  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/processing.rb#29
  def on_consumer_before_schedule_tick(event); end

  # Counts work execution and processing states in consumer instances
  #
  # @param event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/processing.rb#45
  def on_consumer_consume(event); end

  # Collect info about consumption event that occurred and its metrics
  # Removes the job from running jobs
  #
  # @param event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/processing.rb#64
  def on_consumer_consumed(event); end

  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/processing.rb#117
  def on_consumer_revoke(event); end

  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/processing.rb#130
  def on_consumer_revoked(event); end

  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/processing.rb#130
  def on_consumer_shutdown(event); end

  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/processing.rb#117
  def on_consumer_shutting_down(event); end

  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/processing.rb#117
  def on_consumer_tick(event); end

  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/processing.rb#130
  def on_consumer_ticked(event); end

  # Removes failed job from active jobs
  #
  # @param event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/processing.rb#76
  def on_error_occurred(event); end

  # Collect time metrics about worker work execution time
  #
  # @param event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/processing.rb#13
  def on_worker_processed(event); end

  private

  # Gets consumer details for job tracking
  #
  # @note Be aware, that non consumption jobs may not have any messages (empty) in them
  #   when certain filters or features are applied. Please refer to the Karafka docs for
  #   more details.
  # @param consumer [::Karafka::BaseConsumer] consumer instance
  # @param type [String] job type
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/processing.rb#158
  def job_details(consumer, type); end

  # Generates a job id that we can use to track jobs in an unique way
  #
  # @param consumer [::Karafka::BaseConsumer] consumer instance
  # @param type [String] job type
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/processing.rb#147
  def job_id(consumer, type); end
end

# Listener used to collect metrics published by librdkafka
#
# source://karafka-web//lib/karafka/web/tracking/consumers/listeners/statistics.rb#9
class Karafka::Web::Tracking::Consumers::Listeners::Statistics < ::Karafka::Web::Tracking::Consumers::Listeners::Base
  # Collect Kafka metrics
  #
  # @param event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/statistics.rb#13
  def on_statistics_emitted(event); end

  private

  # Extracts and formats partition relevant metrics
  #
  # @param pt_stats [Hash]
  # @return [Hash] extracted partition metrics
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/statistics.rb#123
  def extract_partition_metrics(pt_stats); end

  # Extracts basic consumer group related details
  #
  # @param sg_id [String]
  # @param sg_stats [Hash]
  # @return [Hash] consumer group relevant details
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/statistics.rb#86
  def extract_sg_details(sg_id, sg_stats); end

  # @param pt_id [Integer]
  # @param pt_stats [Hash]
  # @return [Boolean] is this partition relevant to the current process, hence should we
  #   report about it in the context of the process.
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/statistics.rb#105
  def partition_reportable?(pt_id, pt_stats); end

  # @param sg_id [String] subscription group id
  # @param topic_name [String]
  # @param pt_id [Integer] partition id
  # @return [String] poll state / is partition paused or not
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/statistics.rb#157
  def poll_details(sg_id, topic_name, pt_id); end

  # Tracks network transfers from and to the client using a 1 minute rolling window
  #
  # @param statistics [Hash] statistics hash
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/statistics.rb#65
  def track_transfers(statistics); end
end

# Listener that triggers reporting on Karafka process status changes
# Whenever the whole process status changes, we do not want to wait and want to report
# as fast as possible, hence `report!`. This improves the user experience and since
# status changes do not happen that often, we can handle few extra reports dispatches.
#
# source://karafka-web//lib/karafka/web/tracking/consumers/listeners/status.rb#12
class Karafka::Web::Tracking::Consumers::Listeners::Status < ::Karafka::Web::Tracking::Consumers::Listeners::Base
  # Indicate as fast as possible that we've reached the quiet mode
  #
  # @param _event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/status.rb#31
  def on_app_quiet(_event); end

  # Indicate as fast as possible that we've started moving to the quiet mode
  #
  # @param _event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/status.rb#24
  def on_app_quieting(_event); end

  # Instrument on the fact that Karafka has stopped
  #
  # We do this actually before the process ends but we need to do this so the UI does not
  # have this "handling" stopping process.
  #
  # @param _event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/status.rb#49
  def on_app_stopped(_event); end

  # Instrument on the fact that we're stopping
  #
  # @param _event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/status.rb#38
  def on_app_stopping(_event); end

  # @note We do not use `#report!` here because this kicks in for each listener loop and
  #   those run the same time.
  # @param _event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/status.rb#17
  def on_connection_listener_before_fetch_loop(_event); end
end

# Listener used to attach tags to consumers for Web-UI usage. Those tags will be picked
# up by another listener
#
# @note We cannot attach here certain the ActiveJob consumer tags and they need to be
#   in Karafka itself (mainly the per AJ job tag) because from the outside consumer
#   perspective we have a single consumption that can run multiple different AJ jobs
# @note We can assign tags here and the order of tracking listeners does not matter,
#   because tags state for consumers is materialized in the moment of reporting.
#
# source://karafka-web//lib/karafka/web/tracking/consumers/listeners/tags.rb#17
class Karafka::Web::Tracking::Consumers::Listeners::Tags < ::Karafka::Web::Tracking::Consumers::Listeners::Base
  # @param event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/tags.rb#19
  def on_consumer_consume(event); end

  private

  # Adds ActiveJob consumer related tags
  #
  # @param consumer [Karafka::BaseConsumer]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/tags.rb#36
  def tag_active_job(consumer); end

  # Adds attempts counter if this is not the first attempt. Not the first means, there
  # was an error and we are re-processing.
  #
  # @param consumer [Karafka::BaseConsumer]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/tags.rb#46
  def tag_attempt(consumer); end

  # Tags long running job consumer work
  #
  # @param consumer [Karafka::BaseConsumer]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/tags.rb#75
  def tag_long_running_job(consumer); end

  # Tags virtual partitioned consumers and adds extra info if operates in a collapsed
  # mode
  #
  # @param consumer [Karafka::BaseConsumer]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/listeners/tags.rb#60
  def tag_virtual_partitions(consumer); end
end

# Reports the collected data about the process and sends it, so we can use it in the UI
#
# source://karafka-web//lib/karafka/web/tracking/consumers/reporter.rb#8
class Karafka::Web::Tracking::Consumers::Reporter < ::Karafka::Web::Tracking::Reporter
  # @return [Reporter] a new instance of Reporter
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/reporter.rb#20
  def initialize; end

  # We never report in initializing phase because things are not yet fully configured
  # We never report in the initialized because server is not yet ready until Karafka is
  # fully running and some of the things like listeners are not yet available
  #
  # This method will also be `false` in case we are not running in `karafka server` or
  # in embedding, because in those cases Karafka does not go beyond the `initialized` phase
  #
  # @return [Boolean] are we able to report consumer state
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/reporter.rb#36
  def active?; end

  # Dispatches the current state from sampler to appropriate topics
  #
  # @param forced [Boolean] should we report bypassing the time frequency or should we
  #   report only in case we would not send the report for long enough time.
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/reporter.rb#50
  def report(forced: T.unsafe(nil)); end

  # Reports bypassing frequency check. This can be used to report when state changes in the
  # process drastically. For example when process is stopping, we want to indicate this as
  # fast as possible in the UI, etc.
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/reporter.rb#105
  def report!; end

  private

  # Produces messages to Kafka.
  #
  # @note We pick either sync or async dependent on number of messages. The trick here is,
  #   that we do not want to end up overloading the internal queue with messages in case
  #   someone has a lot of errors from processing or other errors. Producing sync will wait
  #   for the delivery, hence will slow things down a little bit. On the other hand during
  #   normal operations we should not have that many messages to dispatch and it should not
  #   slowdown any processing.
  # @param messages [Array<Hash>]
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/reporter.rb#136
  def produce(messages); end

  # @param forced [Boolean] is this report forced. Forced means that as long as we can
  #   flush we will flush
  # @return [Boolean] Should we report or is it not yet time to do so
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/reporter.rb#114
  def report?(forced); end

  # @return [Object] sampler for the metrics
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/reporter.rb#122
  def sampler; end
end

# This mutex is shared between tracker and samplers so there is no case where metrics
# would be collected same time tracker reports
#
# source://karafka-web//lib/karafka/web/tracking/consumers/reporter.rb#18
Karafka::Web::Tracking::Consumers::Reporter::MUTEX = T.let(T.unsafe(nil), Thread::Mutex)

# Minimum number of messages to produce to produce them in sync mode
# This acts as a small back-off not to overload the system in case we would have
# extremely big number of errors happening
#
# source://karafka-web//lib/karafka/web/tracking/consumers/reporter.rb#12
Karafka::Web::Tracking::Consumers::Reporter::PRODUCE_SYNC_THRESHOLD = T.let(T.unsafe(nil), Integer)

# Samples for fetching and storing metrics samples about the consumer process
#
# source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#9
class Karafka::Web::Tracking::Consumers::Sampler < ::Karafka::Web::Tracking::Sampler
  include ::Karafka::Core::Helpers::Time

  # @return [Sampler] a new instance of Sampler
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#36
  def initialize; end

  # Clears counters and errors.
  # Used after data is reported by reported to start collecting new samples
  #
  # @note We do not clear processing or pauses or other things like this because we track
  #   their states and not values, so they need to be tracked between flushes.
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#112
  def clear; end

  # Returns the value of attribute consumer_groups.
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#12
  def consumer_groups; end

  # Returns the value of attribute counters.
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#12
  def counters; end

  # Returns the value of attribute errors.
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#12
  def errors; end

  # Returns the value of attribute jobs.
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#12
  def jobs; end

  # Returns the value of attribute pauses.
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#12
  def pauses; end

  # @note This should run before any mutex, so other threads can continue as those
  #   operations may invoke shell commands
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#120
  def sample; end

  # Returns the value of attribute subscription_groups.
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#12
  def subscription_groups; end

  # @return [Hash] report hash with all the details about consumer operations
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#66
  def to_report; end

  # We cannot report and track the same time, that is why we use mutex here. To make sure
  # that samples aggregations and counting does not interact with reporter flushing.
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#59
  def track; end

  # Returns the value of attribute windows.
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#12
  def windows; end

  private

  # @note We use one minute window to compensate for cases where metrics would be reported
  #   or recorded faster or slower. This normalizes data
  # @return [Integer] number of bytes received per second out of a one minute time window
  #   by all the consumers
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#314
  def bytes_received; end

  # @return [Integer] number of bytes sent per second out of a one minute time window by
  #   all the consumers
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#324
  def bytes_sent; end

  # @return [Array<Float>] load averages for last 1, 5 and 15 minutes
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#235
  def cpu_usage; end

  # @return [Integer] CPU count
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#259
  def cpus; end

  # Consumer group details need to be enriched with details about polling that comes from
  # Karafka level. It is also time based, hence we need to materialize it only at the
  # moment of message dispatch to have it accurate.
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#293
  def enriched_consumer_groups; end

  # @return [Hash] job queue statistics
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#192
  def jobs_queue_statistics; end

  # @return [Hash] number of active and standby listeners
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#155
  def listeners; end

  # @return [Integer] total amount of memory
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#212
  def memory_size; end

  # Loads our ps results into memory so we can extract from them whatever we need
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#269
  def memory_threads_ps; end

  # Total memory used in the OS
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#205
  def memory_total_usage; end

  # @return [Integer] memory used by this process in kilobytes
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#167
  def memory_usage; end

  # @note We memoize it on first run as forks should have their creation time matching the
  #   fork time.
  # @return [Float] time of start of this process
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#134
  def started_at; end

  # @note This returns total number of threads from the OS perspective including native
  #   extensions threads, etc.
  # @return [Integer] number of process threads.
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#252
  def threads; end

  # @return [Numeric] % utilization of all the threads. 100% means all the threads are
  #   utilized all the time within the given time window. 0% means, nothing is happening
  #   most if not all the time.
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#141
  def utilization; end

  # @return [Integer] number of threads that process work
  #
  # source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#264
  def workers; end
end

# Counters that count events occurrences during the given window
#
# source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#21
Karafka::Web::Tracking::Consumers::Sampler::COUNTERS_BASE = T.let(T.unsafe(nil), Hash)

# Current schema version
# This is used for detecting incompatible changes and not using outdated data during
# upgrades
#
# source://karafka-web//lib/karafka/web/tracking/consumers/sampler.rb#18
Karafka::Web::Tracking::Consumers::Sampler::SCHEMA_VERSION = T.let(T.unsafe(nil), String)

# Namespace for all tracking related contracts
#
# source://karafka-web//lib/karafka/web/config.rb#0
module Karafka::Web::Tracking::Contracts; end

# Contract for error reporting
# Since producers and consumers report their errors to the same topic, we need to have
# a unified contract for both
#
# source://karafka-web//lib/karafka/web/tracking/contracts/error.rb#11
class Karafka::Web::Tracking::Contracts::Error < ::Karafka::Web::Contracts::Base; end

# Namespace for tracking related helpers
#
# source://karafka-web//lib/karafka/web/config.rb#0
module Karafka::Web::Tracking::Helpers; end

# Module containing some helper methods useful for extracting extra errors info
#
# source://karafka-web//lib/karafka/web/tracking/helpers/error_info.rb#9
module Karafka::Web::Tracking::Helpers::ErrorInfo
  # Extracts the basic error info
  #
  # @param error [StandardError] error that occurred
  # @return [Array<String, String, String>] array with error name, message and backtrace
  #
  # source://karafka-web//lib/karafka/web/tracking/helpers/error_info.rb#14
  def extract_error_info(error); end

  # @param error [StandardError] error that occurred
  # @return [String] formatted exception message
  #
  # source://karafka-web//lib/karafka/web/tracking/helpers/error_info.rb#38
  def extract_error_message(error); end
end

# Namespace for time sensitive related buffers and operators
#
# source://karafka-web//lib/karafka/web/config.rb#0
module Karafka::Web::Tracking::Helpers::Ttls; end

# Array that allows us to store data points that expire over time automatically.
#
# source://karafka-web//lib/karafka/web/tracking/helpers/ttls/array.rb#10
class Karafka::Web::Tracking::Helpers::Ttls::Array
  include ::Karafka::Core::Helpers::Time
  include ::Enumerable

  # @param ttl [Integer] milliseconds ttl
  # @return [Array] a new instance of Array
  #
  # source://karafka-web//lib/karafka/web/tracking/helpers/ttls/array.rb#15
  def initialize(ttl); end

  # @param value [Object] adds value to the array
  # @return [Object] added element
  #
  # source://karafka-web//lib/karafka/web/tracking/helpers/ttls/array.rb#31
  def <<(value); end

  # Iterates over only active elements
  #
  # source://karafka-web//lib/karafka/web/tracking/helpers/ttls/array.rb#21
  def each; end

  # @return [Boolean] is the array empty
  #
  # source://karafka-web//lib/karafka/web/tracking/helpers/ttls/array.rb#40
  def empty?; end

  # Samples that are within our TTL time window with the times
  #
  # @return [Hash]
  #
  # source://karafka-web//lib/karafka/web/tracking/helpers/ttls/array.rb#48
  def samples; end

  # @return [::Array] pure array version with only active elements
  #
  # source://karafka-web//lib/karafka/web/tracking/helpers/ttls/array.rb#54
  def to_a; end

  private

  # Evicts outdated samples
  #
  # source://karafka-web//lib/karafka/web/tracking/helpers/ttls/array.rb#62
  def clear; end
end

# Hash that accumulates data that has an expiration date (ttl)
# Used to keep track of metrics in a window
#
# source://karafka-web//lib/karafka/web/tracking/helpers/ttls/hash.rb#10
class Karafka::Web::Tracking::Helpers::Ttls::Hash < ::Hash
  # @param ttl [Integer] milliseconds ttl
  # @return [Hash] a new instance of Hash
  #
  # source://karafka-web//lib/karafka/web/tracking/helpers/ttls/hash.rb#12
  def initialize(ttl); end

  # Takes a block where we provide a hash select filtering to select keys we are
  # interested in using for aggregated stats. Once filtered, builds a Stats object out
  # of the candidates
  #
  # @param block [Proc] block for selection of elements for stats
  # @return [Stats]
  # @yieldparam key [String]
  # @yieldparam samples [Ttls::Array]
  #
  # source://karafka-web//lib/karafka/web/tracking/helpers/ttls/hash.rb#24
  def stats_from(&block); end
end

# Object that simplifies computing aggregated statistics out of ttl data
# For TTL based operations we may collect samples from multiple consumers/producers etc
# but in the end we are interested in the collective result of the whole process.
#
# For example when we talk about data received from Kafka, we want to materialize total
# number of bytes and not bytes per given client connection. This layer simplifies this
# by doing necessary aggregations and providing the final results
#
# source://karafka-web//lib/karafka/web/tracking/helpers/ttls/stats.rb#15
class Karafka::Web::Tracking::Helpers::Ttls::Stats
  # @param ttls_hash [Ttls::Hash, Hash] hash with window based samples
  # @return [Stats] a new instance of Stats
  #
  # source://karafka-web//lib/karafka/web/tracking/helpers/ttls/stats.rb#17
  def initialize(ttls_hash); end

  # Computes the rate out of the samples provided on a per second basis. The samples need
  #   to come from the window aggregations
  #
  # @return [Float] per second rate value
  #
  # source://karafka-web//lib/karafka/web/tracking/helpers/ttls/stats.rb#30
  def rps; end
end

# Object used to track process metrics in time windows. Those are shared, meaning they do
# not refer to particular metric type but allow us to store whatever we want.
#
# We have following time windows:
#   - m1 - one minute big
#   - m5 - five minute big
#
# source://karafka-web//lib/karafka/web/tracking/helpers/ttls/windows.rb#14
class Karafka::Web::Tracking::Helpers::Ttls::Windows < ::Struct
  # @return [Ttls::Windows]
  #
  # source://karafka-web//lib/karafka/web/tracking/helpers/ttls/windows.rb#16
  def initialize; end

  # Clears the TTLs windows
  #
  # source://karafka-web//lib/karafka/web/tracking/helpers/ttls/windows.rb#24
  def clear; end

  # Returns the value of attribute m1
  #
  # @return [Object] the current value of m1
  def m1; end

  # Sets the attribute m1
  #
  # @param value [Object] the value to set the attribute m1 to.
  # @return [Object] the newly set value
  def m1=(_); end

  # Returns the value of attribute m5
  #
  # @return [Object] the current value of m5
  def m5; end

  # Sets the attribute m5
  #
  # @param value [Object] the value to set the attribute m5 to.
  # @return [Object] the newly set value
  def m5=(_); end

  class << self
    def [](*_arg0); end
    def inspect; end
    def keyword_init?; end
    def members; end
    def new(*_arg0); end
  end
end

# Class used to run shell command that also returns previous result in case of a failure
# This is used because children can get signals when performing stat fetches and then
# fetch is stopped. This can cause invalid results from sub-shell commands.
#
# This will return last result as log as there was one.
#
# source://karafka-web//lib/karafka/web/tracking/memoized_shell.rb#13
class Karafka::Web::Tracking::MemoizedShell
  # @return [MemoizedShell] a new instance of MemoizedShell
  #
  # source://karafka-web//lib/karafka/web/tracking/memoized_shell.rb#19
  def initialize; end

  # @param cmd [String]
  # @return [String, nil] sub-shell evaluation string result or nil if we were not able to
  #   run or re-run the call.
  #
  # source://karafka-web//lib/karafka/web/tracking/memoized_shell.rb#26
  def call(cmd); end
end

# Hpw many tries do we want to perform before giving up on the shell command
#
# source://karafka-web//lib/karafka/web/tracking/memoized_shell.rb#15
Karafka::Web::Tracking::MemoizedShell::MAX_ATTEMPTS = T.let(T.unsafe(nil), Integer)

# Namespace for all the things related to tracking producers
#
# source://karafka-web//lib/karafka/web/config.rb#0
module Karafka::Web::Tracking::Producers; end

# Namespace for producers listeners
#
# source://karafka-web//lib/karafka/web/config.rb#0
module Karafka::Web::Tracking::Producers::Listeners; end

# Base listener for producer related listeners
#
# source://karafka-web//lib/karafka/web/tracking/producers/listeners/base.rb#10
class Karafka::Web::Tracking::Producers::Listeners::Base
  include ::Karafka::Core::Helpers::Time
  extend ::Forwardable

  # source://forwardable/1.3.3/forwardable.rb#231
  def report(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def report!(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def track(*args, **_arg1, &block); end

  private

  # @return [Object] reporter in use
  #
  # source://karafka-web//lib/karafka/web/tracking/producers/listeners/base.rb#25
  def reporter; end

  # @return [Object] sampler in use
  #
  # source://karafka-web//lib/karafka/web/tracking/producers/listeners/base.rb#20
  def sampler; end
end

# Listener needed to start schedulers and other things that we need to collect and report
# data. We do not want to start this when code is loaded because it may not be fork
# compatible that way
#
# source://karafka-web//lib/karafka/web/tracking/producers/listeners/booting.rb#11
class Karafka::Web::Tracking::Producers::Listeners::Booting < ::Karafka::Web::Tracking::Producers::Listeners::Base
  # Starts (if needed) the Web UI tracking scheduler thread that periodically pings
  # reporters to report needed data (when it is time).
  #
  # @param _event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka-web//lib/karafka/web/tracking/producers/listeners/booting.rb#16
  def on_producer_connected(_event); end
end

# Listener for tracking producers published errors
#
# source://karafka-web//lib/karafka/web/tracking/producers/listeners/errors.rb#9
class Karafka::Web::Tracking::Producers::Listeners::Errors < ::Karafka::Web::Tracking::Producers::Listeners::Base
  include ::Karafka::Web::Tracking::Helpers::ErrorInfo

  # Tracks any producer related errors
  #
  # @param event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka-web//lib/karafka/web/tracking/producers/listeners/errors.rb#20
  def on_error_occurred(event); end

  private

  # @param type [String] error type
  # @param payload [Hash] error payload
  # @return [Hash] hash with details
  #
  # source://karafka-web//lib/karafka/web/tracking/producers/listeners/errors.rb#53
  def build_details(type, payload); end

  # @param event [Karafka::Core::Monitoring::Event]
  # @return [Hash] hash with error data for the sampler
  #
  # source://karafka-web//lib/karafka/web/tracking/producers/listeners/errors.rb#30
  def build_error_details(event); end
end

# Schema used by producers error reporting
#
# source://karafka-web//lib/karafka/web/tracking/producers/listeners/errors.rb#13
Karafka::Web::Tracking::Producers::Listeners::Errors::SCHEMA_VERSION = T.let(T.unsafe(nil), String)

# Reports the collected data about the producer and sends it, so we can use it in the UI
#
# @note Producer reported does not have to operate with the `forced` dispatch mainly
#   because there is no expectation on immediate status updates for producers and their
#   dispatch flow is always periodic based.
#
# source://karafka-web//lib/karafka/web/tracking/producers/reporter.rb#12
class Karafka::Web::Tracking::Producers::Reporter < ::Karafka::Web::Tracking::Reporter
  # @return [Reporter] a new instance of Reporter
  #
  # source://karafka-web//lib/karafka/web/tracking/producers/reporter.rb#24
  def initialize; end

  # Dispatches the current state from sampler to appropriate topics
  #
  # source://karafka-web//lib/karafka/web/tracking/producers/reporter.rb#32
  def report; end

  private

  # Produces messages to Kafka.
  #
  # @note We pick either sync or async dependent on number of messages. The trick here is,
  #   that we do not want to end up overloading the internal queue with messages in case
  #   someone has a lot of errors from processing or other errors. Producing sync will wait
  #   for the delivery, hence will slow things down a little bit. On the other hand during
  #   normal operations we should not have that many messages to dispatch and it should not
  #   slowdown any processing.
  # @param messages [Array<Hash>]
  #
  # source://karafka-web//lib/karafka/web/tracking/producers/reporter.rb#84
  def produce(messages); end

  # @return [Boolean] Should we report or is it not yet time to do so
  #
  # source://karafka-web//lib/karafka/web/tracking/producers/reporter.rb#63
  def report?; end

  # @return [Object] sampler for the metrics
  #
  # source://karafka-web//lib/karafka/web/tracking/producers/reporter.rb#70
  def sampler; end
end

# This mutex is shared between tracker and samplers so there is no case where metrics
# would be collected same time tracker reports
#
# source://karafka-web//lib/karafka/web/tracking/producers/reporter.rb#22
Karafka::Web::Tracking::Producers::Reporter::MUTEX = T.let(T.unsafe(nil), Thread::Mutex)

# Minimum number of messages to produce to produce them in sync mode
# This acts as a small back-off not to overload the system in case we would have
# extremely big number of errors happening
#
# source://karafka-web//lib/karafka/web/tracking/producers/reporter.rb#16
Karafka::Web::Tracking::Producers::Reporter::PRODUCE_SYNC_THRESHOLD = T.let(T.unsafe(nil), Integer)

# Samples for collecting producers related data we're interested in
#
# source://karafka-web//lib/karafka/web/tracking/producers/sampler.rb#9
class Karafka::Web::Tracking::Producers::Sampler < ::Karafka::Web::Tracking::Sampler
  include ::Karafka::Core::Helpers::Time

  # @return [Sampler] a new instance of Sampler
  #
  # source://karafka-web//lib/karafka/web/tracking/producers/sampler.rb#19
  def initialize; end

  # Clears the sampler (for use after data dispatch)
  #
  # source://karafka-web//lib/karafka/web/tracking/producers/sampler.rb#38
  def clear; end

  # Returns the value of attribute errors.
  #
  # source://karafka-web//lib/karafka/web/tracking/producers/sampler.rb#12
  def errors; end

  # We cannot report and track the same time, that is why we use mutex here. To make sure
  # that samples aggregations and counting does not interact with reporter flushing.
  #
  # source://karafka-web//lib/karafka/web/tracking/producers/sampler.rb#28
  def track; end
end

# Current schema version
# This can be used in the future for detecting incompatible changes and writing
# migrations
#
# source://karafka-web//lib/karafka/web/tracking/producers/sampler.rb#17
Karafka::Web::Tracking::Producers::Sampler::SCHEMA_VERSION = T.let(T.unsafe(nil), String)

# Base reporter from which all the reports should inherit
#
# source://karafka-web//lib/karafka/web/tracking/reporter.rb#7
class Karafka::Web::Tracking::Reporter
  include ::Karafka::Core::Helpers::Time

  # Can this reporter report. Since some reporters may report only in part of the processes
  # where Karafka is used (like `karafka server`) each may implement more complex rules.
  #
  # The basic is not to report unless we have a producer and this producer is active
  #
  # @return [Boolean]
  #
  # source://karafka-web//lib/karafka/web/tracking/reporter.rb#16
  def active?; end
end

# Base sampler with some basic info collectors
# This sampler should store **only** collectors that can be used for producers, consumers and
# the Web-UI itself. All specific to a given aspect of operations should be moved out.
#
# source://karafka-web//lib/karafka/web/tracking/sampler.rb#9
class Karafka::Web::Tracking::Sampler
  # @return [String] Karafka::Core version
  #
  # source://karafka-web//lib/karafka/web/tracking/sampler.rb#41
  def karafka_core_version; end

  # @return [String] Karafka version
  #
  # source://karafka-web//lib/karafka/web/tracking/sampler.rb#31
  def karafka_version; end

  # @return [String] Karafka Web UI version
  #
  # source://karafka-web//lib/karafka/web/tracking/sampler.rb#36
  def karafka_web_version; end

  # @return [String] librdkafka version
  #
  # source://karafka-web//lib/karafka/web/tracking/sampler.rb#51
  def librdkafka_version; end

  # @return [String] Unique process identifier
  #
  # source://karafka-web//lib/karafka/web/tracking/sampler.rb#11
  def process_id; end

  # @return [String] rdkafka version
  #
  # source://karafka-web//lib/karafka/web/tracking/sampler.rb#46
  def rdkafka_version; end

  # @return [String] currently used ruby version with details
  #
  # source://karafka-web//lib/karafka/web/tracking/sampler.rb#16
  def ruby_version; end

  # @return [String] WaterDrop version
  #
  # source://karafka-web//lib/karafka/web/tracking/sampler.rb#56
  def waterdrop_version; end
end

# Triggers reporters to report in an async mode in a separate thread
# We report this way to prevent any potential dead-locks in cases we would be emitting
# statistics during transactions.
#
# We should never use the notifications thread for sensitive IO bound operations.
#
# source://karafka-web//lib/karafka/web/tracking/scheduler.rb#11
class Karafka::Web::Tracking::Scheduler
  include ::Karafka::Helpers::Async
  extend ::Forwardable

  # source://forwardable/1.3.3/forwardable.rb#231
  def alive?(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def join(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def name(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def terminate(*args, **_arg1, &block); end

  private

  # Reports the process state once in a while
  #
  # source://karafka-web//lib/karafka/web/tracking/scheduler.rb#17
  def call; end

  # @return [Array] consumers and producers reporters
  #
  # source://karafka-web//lib/karafka/web/tracking/scheduler.rb#32
  def reporters; end
end

# Web UI namespace
#
# source://karafka-web//lib/karafka/web/config.rb#0
module Karafka::Web::Ui; end

# Main Roda Web App that servers all the metrics and stats
#
# source://karafka-web//lib/karafka/web/ui/app.rb#8
class Karafka::Web::Ui::App < ::Karafka::Web::Ui::Base
  include ::Karafka::Web::Ui::App::RodaCompiledTemplates
  include ::Roda::RodaPlugins::RenderEach::InstanceMethods
  include ::Roda::RodaPlugins::Partials::InstanceMethods
  include ::Roda::RodaPlugins::Sessions::InstanceMethods
  include ::Roda::RodaPlugins::RouteCsrf::InstanceMethods

  # source://karafka-web//lib/karafka/web/ui/app.rb#14
  def _roda_main_route(r); end

  private

  # source://roda/3.80.0/lib/roda.rb#438
  def _roda_after(res); end

  # source://karafka-web//lib/karafka/web/ui/base.rb#106
  def _roda_before; end
end

# source://karafka-web//lib/karafka/web/ui/app.rb#0
module Karafka::Web::Ui::App::RodaCompiledTemplates; end

# source://karafka-web//lib/karafka/web/ui/app.rb#0
class Karafka::Web::Ui::App::RodaRequest < ::Karafka::Web::Ui::Base::RodaRequest
  include ::Roda::RodaPlugins::Public::RequestMethods
  include ::Roda::RodaPlugins::Sessions::RequestMethods
end

# source://karafka-web//lib/karafka/web/ui/app.rb#0
class Karafka::Web::Ui::App::RodaResponse < ::Karafka::Web::Ui::Base::RodaResponse; end

# Base Roda application
#
# source://karafka-web//lib/karafka/web/ui/base.rb#7
class Karafka::Web::Ui::Base < ::Roda
  include ::Karafka::Web::Ui::Helpers::PathsHelper
  include ::Karafka::Web::Ui::Helpers::ApplicationHelper
  include ::Karafka::Web::Ui::Helpers::AlertsHelper
  include ::Roda::RodaPlugins::Render::InstanceMethods
  include ::Karafka::Web::Ui::Base::RodaCompiledTemplates
  include ::Roda::RodaPlugins::ErrorHandler::InstanceMethods
  include ::Roda::RodaPlugins::StatusHandler::InstanceMethods
  include ::Roda::RodaPlugins::Hooks::InstanceMethods
  include ::Roda::RodaPlugins::BeforeHook::InstanceMethods
  include ::Roda::RodaPlugins::Flash::InstanceMethods
  include ::Roda::RodaPlugins::Path::InstanceMethods
  include ::Roda::RodaPlugins::ContentSecurityPolicy::InstanceMethods
  extend ::Roda::RodaPlugins::Render::ClassMethods
  extend ::Roda::RodaPlugins::ErrorHandler::ClassMethods
  extend ::Roda::RodaPlugins::StatusHandler::ClassMethods
  extend ::Roda::RodaPlugins::NotFound::ClassMethods
  extend ::Roda::RodaPlugins::Hooks::ClassMethods
  extend ::Roda::RodaPlugins::Path::ClassMethods
  extend ::Roda::RodaPlugins::CustomBlockResults::ClassMethods
  extend ::Roda::RodaPlugins::ClassMatchers::ClassMethods

  # source://karafka-web//lib/karafka/web/ui/base.rb#130
  def current_path(query_data = T.unsafe(nil)); end

  # @return [Karafka::Web::Ui::Controllers::Requests::Params] curated params
  #
  # source://karafka-web//lib/karafka/web/ui/base.rb#156
  def params; end

  # Sets appropriate template variables based on the response object and renders the
  # expected view
  #
  # @param response [Karafka::Web::Ui::Controllers::Responses::Data] response data object
  #
  # source://karafka-web//lib/karafka/web/ui/base.rb#145
  def render_response(response); end

  private

  # source://roda/3.80.0/lib/roda.rb#438
  def _roda_after(res); end

  # source://karafka-web//lib/karafka/web/ui/base.rb#106
  def _roda_before; end

  # source://karafka-web//lib/karafka/web/ui/base.rb#106
  def _roda_before_10__hooks; end

  # source://karafka-web//lib/karafka/web/ui/base.rb#106
  def _roda_before_hook_7; end

  # source://roda/3.80.0/lib/roda.rb#157
  def _roda_path_String_1(*a); end

  # source://roda/3.80.0/lib/roda/plugins/path.rb#92
  def _roda_path_String_1_arity(str); end

  # source://roda/3.80.0/lib/roda/plugins/status_handler.rb#57
  def _roda_status_handler_404(result); end

  # source://karafka-web//lib/karafka/web/ui/base.rb#100
  def _roda_status_handler__404; end

  # source://karafka-web//lib/karafka/web/ui/base.rb#84
  def handle_error(e); end
end

# Details that need to be evaluated in the context of OSS or Pro web UI.
# If those would be evaluated in the base, they would not be initialized as expected
#
# source://karafka-web//lib/karafka/web/ui/base.rb#14
Karafka::Web::Ui::Base::CONTEXT_DETAILS = T.let(T.unsafe(nil), Proc)

# source://karafka-web//lib/karafka/web/ui/base.rb#0
module Karafka::Web::Ui::Base::RodaCompiledTemplates; end

# source://karafka-web//lib/karafka/web/ui/base.rb#0
class Karafka::Web::Ui::Base::RodaRequest < ::Roda::RodaRequest
  include ::Roda::RodaPlugins::RunAppendSlash::RequestMethods
  include ::Roda::RodaPlugins::CustomBlockResults::RequestMethods

  private

  # source://roda/3.80.0/lib/roda/plugins/class_matchers.rb#54
  def _match_class_Time; end
end

# source://karafka-web//lib/karafka/web/ui/base.rb#0
class Karafka::Web::Ui::Base::RodaResponse < ::Roda::RodaResponse
  include ::Roda::RodaPlugins::ContentSecurityPolicy::ResponseMethods
end

# Time matcher with optional hours, minutes and seconds
#
# source://karafka-web//lib/karafka/web/ui/base.rb#113
Karafka::Web::Ui::Base::TIME_MATCHER = T.let(T.unsafe(nil), Regexp)

# Namespace for controller related components in the Web UI app.
#
# source://karafka-web//lib/karafka/web/ui/base.rb#0
module Karafka::Web::Ui::Controllers; end

# Base controller from which all the controllers should inherit.
#
# source://karafka-web//lib/karafka/web/ui/controllers/base_controller.rb#9
class Karafka::Web::Ui::Controllers::BaseController
  include ::Karafka::Web::Ui::Lib::Paginations

  # @param params [Karafka::Web::Ui::Controllers::Requests::Params] request parameters
  # @return [BaseController] a new instance of BaseController
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/base_controller.rb#23
  def initialize(params); end

  private

  # Builds a halt 403 response
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/base_controller.rb#70
  def deny; end

  # Builds a file response object that will be used as a base to dispatch the file
  #
  # @param content [String] Payload we want to dispatch as a file
  # @param file_name [String] name under which the browser is suppose to save the file
  # @return [Responses::File] file response result
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/base_controller.rb#65
  def file(content, file_name); end

  # Raises the not found error
  #
  # @param resource_id [String] resource id that was not found
  # @raise [::Karafka::Web::Errors::Ui::NotFoundError]
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/base_controller.rb#104
  def not_found!(resource_id = T.unsafe(nil)); end

  # Initializes the expected pagination engine and assigns expected arguments
  #
  # @param args Any arguments accepted by the selected pagination engine
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/base_controller.rb#85
  def paginate(*args); end

  # Builds a redirect data object with assigned flashes (if any)
  #
  # @param path [String, Symbol] relative (without root path) path where we want to be
  #   redirected or `:back` to use referer back
  # @param flashes [Hash] hash where key is the flash type and value is the message
  # @return [Responses::Redirect] redirect result
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/base_controller.rb#56
  def redirect(path = T.unsafe(nil), flashes = T.unsafe(nil)); end

  # @param resources [Hash, Array, Lib::HashProxy] object for sorting
  # @return [Hash, Array, Lib::HashProxy] sorted results
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/base_controller.rb#76
  def refine(resources); end

  # Builds the render data object with assigned attributes based on instance variables.
  #
  # @return [Responses::Render] data that should be used to render appropriate view
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/base_controller.rb#32
  def render; end

  class << self
    # Attributes on which we can sort in a given controller
    #
    # source://karafka-web//lib/karafka/web/ui/controllers/base_controller.rb#17
    def sortable_attributes; end

    # Attributes on which we can sort in a given controller
    #
    # source://karafka-web//lib/karafka/web/ui/controllers/base_controller.rb#17
    def sortable_attributes=(_arg0); end
  end
end

# Alias for easier referencing
#
# source://karafka-web//lib/karafka/web/ui/controllers/base_controller.rb#13
Karafka::Web::Ui::Controllers::BaseController::Models = Karafka::Web::Ui::Models

# Pro message reporting info controller
#
# source://karafka-web//lib/karafka/web/ui/controllers/become_pro_controller.rb#8
class Karafka::Web::Ui::Controllers::BecomeProController < ::Karafka::Web::Ui::Controllers::BaseController
  # Display a message, that a given feature is available only in Pro
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/become_pro_controller.rb#10
  def show; end
end

# Selects cluster info and topics basic info
#
# source://karafka-web//lib/karafka/web/ui/controllers/cluster_controller.rb#8
class Karafka::Web::Ui::Controllers::ClusterController < ::Karafka::Web::Ui::Controllers::BaseController
  # Lists available brokers in the cluster
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/cluster_controller.rb#21
  def brokers; end

  # List partitions replication details
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/cluster_controller.rb#28
  def replication; end

  private

  # Make sure, that for the cluster view we always get the most recent cluster state
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/cluster_controller.rb#54
  def cluster_info; end

  # @param cluster_info [Rdkafka::Metadata] cluster metadata
  # @return [Array<Hash>] array with topics to be displayed sorted in an alphabetical
  #   order
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/cluster_controller.rb#61
  def displayable_topics(cluster_info); end
end

# Consumers (consuming processes - `karafka server`) processes display consumer
#
# source://karafka-web//lib/karafka/web/ui/controllers/consumers_controller.rb#8
class Karafka::Web::Ui::Controllers::ConsumersController < ::Karafka::Web::Ui::Controllers::BaseController
  # List page with consumers
  #
  # @note For now we load all and paginate over the squashed data.
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/consumers_controller.rb#17
  def index; end
end

# Main Karafka Pro Web-Ui dashboard controller
#
# source://karafka-web//lib/karafka/web/ui/controllers/dashboard_controller.rb#8
class Karafka::Web::Ui::Controllers::DashboardController < ::Karafka::Web::Ui::Controllers::BaseController
  # View with statistics dashboard details
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/dashboard_controller.rb#10
  def index; end
end

# Errors displaying controller
# It supports only scenarios with a single partition for errors
# If you have high load of errors, consider going Pro
#
# source://karafka-web//lib/karafka/web/ui/controllers/errors_controller.rb#10
class Karafka::Web::Ui::Controllers::ErrorsController < ::Karafka::Web::Ui::Controllers::BaseController
  # Lists first page of the errors
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/errors_controller.rb#12
  def index; end

  # @param offset [Integer] given error message offset
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/errors_controller.rb#27
  def show(offset); end

  private

  # @return [Array] Array with requested messages as well as pagination details and other
  #   obtained metadata
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/errors_controller.rb#41
  def current_page_data; end

  # @return [String] errors topic
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/errors_controller.rb#51
  def errors_topic; end
end

# Active jobs (work) reporting controller
#
# source://karafka-web//lib/karafka/web/ui/controllers/jobs_controller.rb#8
class Karafka::Web::Ui::Controllers::JobsController < ::Karafka::Web::Ui::Controllers::BaseController
  # Lists pending jobs
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/jobs_controller.rb#43
  def pending; end

  # Lists running jobs
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/jobs_controller.rb#18
  def running; end

  private

  # @param processes [Array<Process>]
  # @return [Lib::HashProxy] particular type jobs count
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/jobs_controller.rb#71
  def count_jobs_types(processes); end
end

# Namespace for request related components
#
# source://karafka-web//lib/karafka/web.rb#0
module Karafka::Web::Ui::Controllers::Requests; end

# Internal representation of params with sane sanitization
#
# source://karafka-web//lib/karafka/web/ui/controllers/requests/params.rb#10
class Karafka::Web::Ui::Controllers::Requests::Params
  # @param request_params [Hash] raw hash with params
  # @return [Params] a new instance of Params
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/requests/params.rb#23
  def initialize(request_params); end

  # @return [Integer] offset from which we want to start. `-1` indicates, that we want
  #   to show the first page discovered based on the high watermark offset. If no offset
  #   is provided, we go with the high offset first page approach
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/requests/params.rb#52
  def current_offset; end

  # @note It does basic sanitization
  # @return [Integer] current page for paginated views
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/requests/params.rb#34
  def current_page; end

  # @return [String] Range type for charts we want to fetch
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/requests/params.rb#43
  def current_range; end

  # @return [String] sort query value
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/requests/params.rb#28
  def sort; end
end

# What ranges we support for charts
# Anything else will be rejected
#
# source://karafka-web//lib/karafka/web/ui/controllers/requests/params.rb#13
Karafka::Web::Ui::Controllers::Requests::Params::ALLOWED_RANGES = T.let(T.unsafe(nil), Array)

# Response related components
#
# source://karafka-web//lib/karafka/web/ui/base.rb#0
module Karafka::Web::Ui::Controllers::Responses; end

# Response that will make Roda render 403 deny
#
# source://karafka-web//lib/karafka/web/ui/controllers/responses/deny.rb#9
class Karafka::Web::Ui::Controllers::Responses::Deny; end

# Response that tells Roda to ship the content under a file name
#
# source://karafka-web//lib/karafka/web/ui/controllers/responses/file.rb#9
class Karafka::Web::Ui::Controllers::Responses::File
  # @param content [String] data we want to send
  # @param file_name [String] name under which we want to send it
  # @return [File] a new instance of File
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/responses/file.rb#14
  def initialize(content, file_name); end

  # Returns the value of attribute content.
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/responses/file.rb#10
  def content; end

  # Returns the value of attribute file_name.
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/responses/file.rb#10
  def file_name; end
end

# Representation of a redirect response with optional flash messages
#
# source://karafka-web//lib/karafka/web/ui/controllers/responses/redirect.rb#9
class Karafka::Web::Ui::Controllers::Responses::Redirect
  # @param path [String, Symbol] relative (without root path) path where we want to be
  #   redirected or `:back` to use referer back
  # @param flashes [Hash] hash where key is the flash type and value is the message
  # @return [Redirect] a new instance of Redirect
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/responses/redirect.rb#15
  def initialize(path = T.unsafe(nil), flashes = T.unsafe(nil)); end

  # @return [Boolean] are we going back via referer and not explicit path
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/responses/redirect.rb#21
  def back?; end

  # Returns the value of attribute flashes.
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/responses/redirect.rb#10
  def flashes; end

  # Returns the value of attribute path.
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/responses/redirect.rb#10
  def path; end
end

# Response render data object. It is used to transfer attributes assigned in controllers
# into views
# It acts as a simplification / transport layer for assigned attributes
#
# source://karafka-web//lib/karafka/web/ui/controllers/responses/render.rb#12
class Karafka::Web::Ui::Controllers::Responses::Render
  # @param path [String] render path
  # @param attributes [Hash] attributes assigned in the controller
  # @return [Render] a new instance of Render
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/responses/render.rb#17
  def initialize(path, attributes); end

  # Returns the value of attribute attributes.
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/responses/render.rb#13
  def attributes; end

  # Returns the value of attribute path.
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/responses/render.rb#13
  def path; end
end

# Routing presentation controller
#
# source://karafka-web//lib/karafka/web/ui/controllers/routing_controller.rb#8
class Karafka::Web::Ui::Controllers::RoutingController < ::Karafka::Web::Ui::Controllers::BaseController
  # Routing list
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/routing_controller.rb#15
  def index; end

  # Given route details
  #
  # @param topic_id [String] topic id
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/routing_controller.rb#28
  def show(topic_id); end
end

# View that helps understand the status of the Web UI
# Many people reported problems understanding the requirements or misconfigured things.
# While all of the things are documented, people are lazy. Hence we provide a status
# page where we check that everything is as expected and if not, we can provide some
# helpful instructions on how to fix the issues.
#
# source://karafka-web//lib/karafka/web/ui/controllers/status_controller.rb#12
class Karafka::Web::Ui::Controllers::StatusController < ::Karafka::Web::Ui::Controllers::BaseController
  # Displays the Web UI setup status
  #
  # source://karafka-web//lib/karafka/web/ui/controllers/status_controller.rb#14
  def show; end
end

# Namespace for helpers used by the Web UI
#
# source://karafka-web//lib/karafka/web/ui/base.rb#0
module Karafka::Web::Ui::Helpers; end

# Helper for generating general alerts
#
# source://karafka-web//lib/karafka/web/ui/helpers/alerts_helper.rb#8
module Karafka::Web::Ui::Helpers::AlertsHelper
  # @param message [String] alert message
  # @return [String] html with alert info
  #
  # source://karafka-web//lib/karafka/web/ui/helpers/alerts_helper.rb#11
  def alert_info(message); end
end

# Main application helper
#
# source://karafka-web//lib/karafka/web/ui/helpers/application_helper.rb#10
module Karafka::Web::Ui::Helpers::ApplicationHelper
  # @param hash [Hash] we want to flatten
  # @param parent_key [String] key for recursion
  # @param result [Hash] result for recursion
  # @return [Hash]
  #
  # source://karafka-web//lib/karafka/web/ui/helpers/application_helper.rb#258
  def flat_hash(hash, parent_key = T.unsafe(nil), result = T.unsafe(nil)); end

  # @param mem_kb [Integer] memory used in KB
  # @return [String] formatted memory usage
  #
  # source://karafka-web//lib/karafka/web/ui/helpers/application_helper.rb#115
  def format_memory(mem_kb); end

  # Takes a kafka report state and recommends background style color
  #
  # @param state [String] state
  # @return [String] background style
  #
  # source://karafka-web//lib/karafka/web/ui/helpers/application_helper.rb#103
  def kafka_state_bg(state); end

  # Takes the lag trend and gives it appropriate background style color for badge
  #
  # @param trend [Numeric] lag trend
  # @return [String] bg classes
  #
  # source://karafka-web//lib/karafka/web/ui/helpers/application_helper.rb#83
  def lag_trend_bg(trend); end

  # @param lag [Integer] lag
  # @return [String] lag if correct or `N/A` with labeled explanation
  # @see #offset_with_label
  #
  # source://karafka-web//lib/karafka/web/ui/helpers/application_helper.rb#191
  def lag_with_label(lag); end

  # @param details [::Karafka::Web::Ui::Models::Partition] partition information with
  #   lso risk state info
  # @return [String] background classes for row marking
  #
  # source://karafka-web//lib/karafka/web/ui/helpers/application_helper.rb#222
  def lso_risk_state_bg(details); end

  # Adds active class to the current location in the nav if needed
  #
  # @param location [Hash]
  #
  # source://karafka-web//lib/karafka/web/ui/helpers/application_helper.rb#36
  def nav_class(location); end

  # Converts number to a more friendly delimiter based version
  #
  # @param number [Numeric]
  # @param delimiter [String] delimiter (comma by default)
  # @return [String] number with delimiter
  #
  # source://karafka-web//lib/karafka/web/ui/helpers/application_helper.rb#131
  def number_with_delimiter(number, delimiter = T.unsafe(nil)); end

  # Converts object into a string and for objects that would anyhow return their
  # stringified instance value, it replaces it with the class name instead.
  # Useful for deserializers, etc presentation.
  #
  # @param object [Object]
  # @return [String]
  #
  # source://karafka-web//lib/karafka/web/ui/helpers/application_helper.rb#49
  def object_value_to_s(object); end

  # @param topic_name [String] name of the topic for explorer path
  # @param partition_id [Integer] partition for the explorer path
  # @param offset [Integer] offset
  # @param explore [Boolean] should we generate (when allowed) a link to message explorer
  # @return [String] offset if correct or `N/A` with labeled explanation for offsets
  #   that are less than 0. Offset with less than 0 indicates, that the offset was not
  #   yet committed and there is no value we know of
  #
  # source://karafka-web//lib/karafka/web/ui/helpers/application_helper.rb#207
  def offset_with_label(topic_name, partition_id, offset, explore: T.unsafe(nil)); end

  # @param state [String] poll state
  # @param state_ch [Integer] time until next change of the poll state
  #   (from paused to active)
  # @return [String] span tag with label and title with change time if present
  #
  # source://karafka-web//lib/karafka/web/ui/helpers/application_helper.rb#158
  def poll_state_with_change_time_label(state, state_ch); end

  # @param time [Float] UTC time float
  # @return [String] relative time tag for timeago.js
  #
  # source://karafka-web//lib/karafka/web/ui/helpers/application_helper.rb#141
  def relative_time(time); end

  # Renders per scope breadcrumbs
  #
  # source://karafka-web//lib/karafka/web/ui/helpers/application_helper.rb#54
  def render_breadcrumbs; end

  # @param name [String] link value
  # @param attribute [Symbol, nil] sorting attribute or nil if we provide only symbol name
  # @param rev [Boolean] when set to true, arrows will be in the reverse position. This is
  #   used when the description in the link is reverse to data we sort. For example we have
  #   order on when processes were started and we display "x hours" ago but we sort on
  #   their age, meaning that it looks like it is the other way around. This flag allows
  #   us to reverse just he arrow making it look consistent with the presented data order
  # @return [String] html link for sorting with arrow when attribute sort enabled
  #
  # source://karafka-web//lib/karafka/web/ui/helpers/application_helper.rb#283
  def sort_link(name, attribute = T.unsafe(nil), rev: T.unsafe(nil)); end

  # Takes a status and recommends background style color
  #
  # @param status [String] status
  # @return [String] background style
  #
  # source://karafka-web//lib/karafka/web/ui/helpers/application_helper.rb#64
  def status_bg(status); end

  # Renders tags one after another
  #
  # @param tags_array [Array<String>]
  # @return [String] tags badges
  #
  # source://karafka-web//lib/karafka/web/ui/helpers/application_helper.rb#94
  def tags(tags_array); end

  # @param time [Time] time object we want to present with detailed ms label
  # @return [String] span tag with raw timestamp as a title and time as a value
  #
  # source://karafka-web//lib/karafka/web/ui/helpers/application_helper.rb#148
  def time_with_label(time); end

  # Truncates given text if it is too long and wraps it with a title with full text.
  # Can use a middle-based strategy that keeps beginning and ending of a string instead of
  # keeping just the beginning.
  #
  # The `:middle` strategy is useful when we have strings such as really long process names
  # that have important beginning and end but middle can be removed without risk of not
  # allowing user to recognize the content.
  #
  # @param string [String] string we want to truncate
  # @param length [Integer] max length of the final string that we accept before truncating
  # @param omission [String] truncation omission
  # @param strategy [Symbol] `:default` or `:middle` how should we truncate
  # @return [String] HTML span tag with truncated content and full content title
  #
  # source://karafka-web//lib/karafka/web/ui/helpers/application_helper.rb#324
  def truncate(string, length: T.unsafe(nil), omission: T.unsafe(nil), strategy: T.unsafe(nil)); end

  # Returns the view title html code
  #
  # @param title [String] page title
  # @param hr [Boolean] should we add the hr tag at the end
  # @return [String] title html
  #
  # source://karafka-web//lib/karafka/web/ui/helpers/application_helper.rb#240
  def view_title(title, hr: T.unsafe(nil)); end
end

# Default attribute names mapped from the attributes themselves
# It makes it easier as we do not have to declare those all the time
#
# source://karafka-web//lib/karafka/web/ui/helpers/application_helper.rb#13
Karafka::Web::Ui::Helpers::ApplicationHelper::SORT_NAMES = T.let(T.unsafe(nil), Hash)

# Helper for web ui paths builders
#
# source://karafka-web//lib/karafka/web/ui/helpers/paths_helper.rb#8
module Karafka::Web::Ui::Helpers::PathsHelper
  # Generates a full path to any asset with our web-ui version. We ship all assets with
  # the version in the url to prevent those assets from being used after update. After
  # each web-ui update, assets are going to be re-fetched as the url will change
  #
  # @param local_path [String] local path to the asset
  # @return [String] full path to the asst including correct root path
  #
  # source://karafka-web//lib/karafka/web/ui/helpers/paths_helper.rb#27
  def asset_path(local_path); end

  # Helps build explorer paths. We often link offsets to proper messages, etc so this
  # allows us to short-track this
  #
  # @param topic_name [String, nil] name of the topic where we want to go within the
  #   explorer or nil if we want to just go to the explorer root
  # @param partition_id [Integer, nil] partition we want to display in the explorer or nil
  #   if we want to go to the topic root
  # @param offset [Integer, nil] offset of particular message or nil of we want to just go
  #   to the partition root
  # @param action [String, nil] specific routed action or nil
  # @return [String] path to the expected location
  #
  # source://karafka-web//lib/karafka/web/ui/helpers/paths_helper.rb#41
  def explorer_path(topic_name = T.unsafe(nil), partition_id = T.unsafe(nil), offset = T.unsafe(nil), action = T.unsafe(nil)); end

  # Generates a full path with the root path out of the provided arguments
  #
  # @note This needs to be done that way with the `#root_path` because the web UI can be
  #   mounted in a sub-path and we need to make sure our all paths are relative to "our"
  #   root, not the root of the app in which it was mounted.
  # @param args [Array<String, Numeric>] arguments that will make the path
  # @return [String] path from the root
  #
  # source://karafka-web//lib/karafka/web/ui/helpers/paths_helper.rb#17
  def root_path(*args); end
end

# Non info related extra components used in the UI
#
# source://karafka-web//lib/karafka/web/config.rb#0
module Karafka::Web::Ui::Lib; end

# Wrapper around Karafka Admin that alters its behaviours or injects Web UI interface
# specific settings that optimize the responsiveness of the UI when operating on topics
#
# @note Not all commands need those optimizations, hence we alter only those that need
#   that and we only expose those admin commands that are used in the Web-UI interface
#   component.
# @note We expose here only admin methods used in the Web UI interface. Processing uses the
#   `Karafka::Admin` with the defaults
#
# source://karafka-web//lib/karafka/web/ui/lib/admin.rb#16
class Karafka::Web::Ui::Lib::Admin
  class << self
    # source://forwardable/1.3.3/forwardable.rb#231
    def cluster_info(*args, **_arg1, &block); end

    # Allows us to read messages from the topic
    #
    # @param name [String, Symbol] topic name
    # @param partition [Integer] partition
    # @param count [Integer] how many messages we want to get at most
    # @param start_offset [Integer, Time] offset from which we should start. If -1 is provided
    #   (default) we will start from the latest offset. If time is provided, the appropriate
    #   offset will be resolved. If negative beyond -1 is provided, we move backwards more.
    # @param settings [Hash] kafka extra settings (optional)
    # @return [Array<Karafka::Messages::Message>] array with messages
    #
    # source://karafka-web//lib/karafka/web/ui/lib/admin.rb#33
    def read_topic(name, partition, count, start_offset = T.unsafe(nil), settings = T.unsafe(nil)); end

    # source://forwardable/1.3.3/forwardable.rb#231
    def read_watermark_offsets(*args, **_arg1, &block); end

    private

    # @note It does **not** affect tracking or processing
    # @return [Hash] kafka config for Web UI interface.
    #
    # source://karafka-web//lib/karafka/web/ui/lib/admin.rb#48
    def config; end
  end
end

# Proxy for hashes we use across UI.
# Often we have nested values we want to extract or just values we want to reference and
# this object drastically simplifies that.
#
# It is mostly used for flat hashes.
#
# It is in a way similar to openstruct but has abilities to dive deep into objects
#
# It is not super fast but it is enough for the UI and how deep structures we have.
#
# source://karafka-web//lib/karafka/web/ui/lib/hash_proxy.rb#17
class Karafka::Web::Ui::Lib::HashProxy
  extend ::Forwardable

  # @param hash [Hash] hash we want to convert to a proxy
  # @return [HashProxy] a new instance of HashProxy
  #
  # source://karafka-web//lib/karafka/web/ui/lib/hash_proxy.rb#23
  def initialize(hash); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def [](*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def []=(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def each(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def find(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def key?(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def keys(*args, **_arg1, &block); end

  # @param method_name [String] method name
  # @param args [Object] all the args of the method
  # @param block [Proc] block for the method
  #
  # source://karafka-web//lib/karafka/web/ui/lib/hash_proxy.rb#41
  def method_missing(method_name, *args, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def select(*args, **_arg1, &block); end

  # @return [Original hash]
  #
  # source://karafka-web//lib/karafka/web/ui/lib/hash_proxy.rb#34
  def to_h; end

  # source://forwardable/1.3.3/forwardable.rb#231
  def values(*args, **_arg1, &block); end

  private

  # @param obj [Object] local scope of iterating
  # @param key [Symbol, String] key we are looking for
  #
  # source://karafka-web//lib/karafka/web/ui/lib/hash_proxy.rb#74
  def deep_find(obj, key); end

  # @param method_name [String] method name
  # @param include_private [Boolean]
  # @return [Boolean]
  #
  # source://karafka-web//lib/karafka/web/ui/lib/hash_proxy.rb#56
  def respond_to_missing?(method_name, include_private = T.unsafe(nil)); end
end

# Namespace for all the types of pagination engines we want to support
#
# source://karafka-web//lib/karafka/web/ui/controllers/base_controller.rb#0
module Karafka::Web::Ui::Lib::Paginations; end

# Abstraction on top of pagination, so we can alter pagination key and other things
# for non-standard pagination views (non page based, etc)
#
# @note We do not use `_page` explicitly to indicate, that the page scope may not operate
#   on numerable pages (1,2,3,4) but can operate on offsets or times, etc. `_offset` is
#   more general and may refer to many types of pagination.
#
# source://karafka-web//lib/karafka/web/ui/lib/paginations/base.rb#15
class Karafka::Web::Ui::Lib::Paginations::Base
  # Returns the value of attribute current_offset.
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/base.rb#16
  def current_offset; end

  # @raise [NotImplementedError]
  # @return [Boolean] Should we show current offset. If false, the current offset link
  #   will not be visible at all. Useful for non-linear pagination.
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/base.rb#42
  def current_offset?; end

  # @raise [NotImplementedError]
  # @return [String] first offset url value
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/base.rb#30
  def first_offset; end

  # @raise [NotImplementedError]
  # @return [Boolean] Should first offset link be active. If false, the first offset link
  #   will be disabled
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/base.rb#25
  def first_offset?; end

  # Returns the value of attribute next_offset.
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/base.rb#16
  def next_offset; end

  # @raise [NotImplementedError]
  # @return [Boolean] Should we show next offset pagination. If false, next offset link
  #   will be disabled.
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/base.rb#48
  def next_offset?; end

  # @raise [NotImplementedError]
  # @return [String] the url offset key
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/base.rb#53
  def offset_key; end

  # @raise [NotImplementedError]
  # @return [Boolean] Should we show pagination at all
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/base.rb#19
  def paginate?; end

  # Returns the value of attribute previous_offset.
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/base.rb#16
  def previous_offset; end

  # @raise [NotImplementedError]
  # @return [Boolean] Should previous offset link be active. If false, the previous
  #   offset link will be disabled
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/base.rb#36
  def previous_offset?; end
end

# Kafka offset based pagination backend
#
# Allows us to support paginating over offsets
#
# source://karafka-web//lib/karafka/web/ui/lib/paginations/offset_based.rb#11
class Karafka::Web::Ui::Lib::Paginations::OffsetBased < ::Karafka::Web::Ui::Lib::Paginations::Base
  # @param previous_offset [Integer, false] previous offset or false if should not be
  #   presented
  # @param current_offset [Integer] current offset
  # @param next_offset [Integer, Boolean] should we show next offset page button. If
  #   false it will not be presented.
  # @param visible_offsets [Array<Integer>] offsets that are visible in the paginated
  #   view. It is needed for the current page label
  # @return [OffsetBased] a new instance of OffsetBased
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/offset_based.rb#19
  def initialize(previous_offset, current_offset, next_offset, visible_offsets); end

  # @return [String] label of the current page. It is combined out of the first and
  #   the last offsets to show the range where we are. It will be empty if no offsets
  #   but this is not a problem as then we should not display pagination at all
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/offset_based.rb#80
  def current_label; end

  # @return [Boolean] We show current label with offsets that are present on the given
  #   page
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/offset_based.rb#60
  def current_offset?; end

  # @return [Integer] -1 because it will then select the highest offsets
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/offset_based.rb#49
  def first_offset; end

  # @return [Boolean] active only when we are not on the first page. First page is always
  #   indicated by the current offset being -1. If there is someone that sets up the
  #   current offset to a value equal to the last message in the topic partition, we do
  #   not consider it as a first page and we allow to "reset" to -1 via the first page
  #   button
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/offset_based.rb#44
  def first_offset?; end

  # If there is no next offset, we point to 0 as there should be no smaller offset than
  # that in Kafka ever
  #
  # @return [Integer]
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/offset_based.rb#73
  def next_offset; end

  # @return [Boolean] move to the next page if not false. False indicates, that there is
  #   no next page to move to
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/offset_based.rb#66
  def next_offset?; end

  # @return [String] for offset based pagination we use the offset param name
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/offset_based.rb#87
  def offset_key; end

  # Show pagination only when there is more than one page of results to be presented
  #
  # @return [Boolean]
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/offset_based.rb#35
  def paginate?; end

  # @return [Boolean] Active previous page link when it is not the first page
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/offset_based.rb#54
  def previous_offset?; end
end

# Regular page-based pagination engine
#
# source://karafka-web//lib/karafka/web/ui/lib/paginations/page_based.rb#9
class Karafka::Web::Ui::Lib::Paginations::PageBased < ::Karafka::Web::Ui::Lib::Paginations::Base
  # @param current_offset [Integer] current page
  # @param show_next_offset [Boolean] should we show next page
  #   (value is computed automatically)
  # @return [PageBased] a new instance of PageBased
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/page_based.rb#13
  def initialize(current_offset, show_next_offset); end

  # @return [String] label of the current page
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/page_based.rb#51
  def current_label; end

  # @return [Boolean] always show current offset pagination value
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/page_based.rb#46
  def current_offset?; end

  # @return [Boolean] first page for page based pagination is always empty as it moves us
  #   to the initial page so we do not include any page info
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/page_based.rb#36
  def first_offset; end

  # @return [Boolean] active the first page link when we are not on the first page
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/page_based.rb#30
  def first_offset?; end

  # @return [Boolean] move to the next page if not false. False indicates, that there is
  #   no next page to move to
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/page_based.rb#57
  def next_offset?; end

  # @return [String] for page pages pagination, always use page as the url value
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/page_based.rb#62
  def offset_key; end

  # Show pagination only when there is more than one page
  #
  # @return [Boolean]
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/page_based.rb#25
  def paginate?; end

  # @return [Boolean] Active previous page link when it is not the first page
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/page_based.rb#41
  def previous_offset?; end
end

# Namespace for commands that build paginated resources based on the provided page
#
# source://karafka-web//lib/karafka/web/ui/models/message.rb#0
module Karafka::Web::Ui::Lib::Paginations::Paginators; end

# A simple wrapper for paginating array related data structures
# We call this with plural (same with `Sets`) to avoid confusion with Ruby classes
#
# source://karafka-web//lib/karafka/web/ui/lib/paginations/paginators/arrays.rb#12
class Karafka::Web::Ui::Lib::Paginations::Paginators::Arrays < ::Karafka::Web::Ui::Lib::Paginations::Paginators::Base
  class << self
    # @param array [Array] array we want to paginate
    # @param current_page [Integer] page we want to be on
    # @return [Array<Array, Boolean>] Array with two elements: first is the array with
    #   data of the given page and second is a boolean flag with info if the elements we got
    #   are from the last page
    #
    # source://karafka-web//lib/karafka/web/ui/lib/paginations/paginators/arrays.rb#19
    def call(array, current_page); end
  end
end

# Base paginator
#
# source://karafka-web//lib/karafka/web/ui/lib/paginations/paginators/base.rb#10
class Karafka::Web::Ui::Lib::Paginations::Paginators::Base
  class << self
    # @return [Integer] number of elements per page
    #
    # source://karafka-web//lib/karafka/web/ui/lib/paginations/paginators/base.rb#13
    def per_page; end
  end
end

# Paginator for selecting proper range of partitions for each page
# For topics with a lot of partitions we cannot get all the data efficiently, that
# is why we limit number of partitions per page and reduce the operations
# that way. This allows us to effectively display more while not having to fetch
# more partitions then the number of messages per page.
# In cases like this we distribute partitions evenly part of partitions on each of
# the pages. This may become unreliable for partitions that are not evenly
# distributed but this allows us to display data for as many partitions as we want
# without overloading the system
#
# source://karafka-web//lib/karafka/web/ui/lib/paginations/paginators/partitions.rb#18
class Karafka::Web::Ui::Lib::Paginations::Paginators::Partitions < ::Karafka::Web::Ui::Lib::Paginations::Paginators::Base
  class << self
    # Computers the partitions slice, materialized page and the limitations status
    # for a given page
    #
    # @param partitions_count [Integer] number of partitions for a given topic
    # @param current_page [Integer] current page
    # @return [Array<Array<Integer>, Integer, Boolean>] list of partitions that should
    #   be active on a given page, materialized page for them and info if we had to
    #   limit the partitions number on a given page
    #
    # source://karafka-web//lib/karafka/web/ui/lib/paginations/paginators/partitions.rb#27
    def call(partitions_count, current_page); end
  end
end

# Paginator that allows us to take several lists/sets and iterate over them in a
# round-robin fashion.
#
# It does not have to iterate over all the elements from each set for higher pages
# making it much more effective than the naive implementation.
#
# source://karafka-web//lib/karafka/web/ui/lib/paginations/paginators/sets.rb#14
class Karafka::Web::Ui::Lib::Paginations::Paginators::Sets < ::Karafka::Web::Ui::Lib::Paginations::Paginators::Base
  class << self
    # @param counts [Array<Integer>] sets elements counts
    # @param current_page [Integer] page number
    # @return [Hash<Integer, Range>] hash with integer keys indicating the count
    #   location and the range needed to be taken of elements (counting backwards) for
    #   each partition
    #
    # source://karafka-web//lib/karafka/web/ui/lib/paginations/paginators/sets.rb#21
    def call(counts, current_page); end
  end
end

# Watermark offsets single message pagination engine
#
# It is used to provide pagination for single message displays (explorer, errors)
#
# source://karafka-web//lib/karafka/web/ui/lib/paginations/watermark_offsets_based.rb#11
class Karafka::Web::Ui::Lib::Paginations::WatermarkOffsetsBased < ::Karafka::Web::Ui::Lib::Paginations::Base
  # @param current_offset [Integer] current message offset
  # @param low_watermark_offset [Integer]
  # @param high_watermark_offset [Integer]
  # @return [WatermarkOffsetsBased] a new instance of WatermarkOffsetsBased
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/watermark_offsets_based.rb#15
  def initialize(current_offset, low_watermark_offset, high_watermark_offset); end

  # @return [String] shows as current page pagination the offset
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/watermark_offsets_based.rb#57
  def current_label; end

  # @return [Boolean] We always show current offset
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/watermark_offsets_based.rb#52
  def current_offset?; end

  # @return [Integer] highest available offset
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/watermark_offsets_based.rb#42
  def first_offset; end

  # @return [Boolean] provide link to the first (newest)
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/watermark_offsets_based.rb#37
  def first_offset?; end

  # @return [Boolean] if not lowest, show
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/watermark_offsets_based.rb#62
  def next_offset?; end

  # @return [String] params offset key
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/watermark_offsets_based.rb#67
  def offset_key; end

  # @return [Boolean] show pagination only when there are other things to present
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/watermark_offsets_based.rb#29
  def paginate?; end

  # @return [Boolean]
  #
  # source://karafka-web//lib/karafka/web/ui/lib/paginations/watermark_offsets_based.rb#47
  def previous_offset?; end
end

# Class used to execute code that can fail but we do not want to fail the whole operation.
# The primary use-case is for displaying deserialized data. We always need to assume, that
# part of the data can be corrupted and it should not crash the whole UI.
#
# It caches the result and does not run the code twice (only once)
#
# source://karafka-web//lib/karafka/web/ui/lib/safe_runner.rb#12
class Karafka::Web::Ui::Lib::SafeRunner
  # @param block [Proc] code we want to safe-guard
  # @return [SafeRunner] a new instance of SafeRunner
  #
  # source://karafka-web//lib/karafka/web/ui/lib/safe_runner.rb#16
  def initialize(&block); end

  # Runs the execution and returns block result
  #
  # source://karafka-web//lib/karafka/web/ui/lib/safe_runner.rb#39
  def call; end

  # Returns the value of attribute error.
  #
  # source://karafka-web//lib/karafka/web/ui/lib/safe_runner.rb#13
  def error; end

  # @return [Boolean] was the code executed already or not yet
  #
  # source://karafka-web//lib/karafka/web/ui/lib/safe_runner.rb#52
  def executed?; end

  # @return [Boolean] was the code execution failed or not
  #
  # source://karafka-web//lib/karafka/web/ui/lib/safe_runner.rb#34
  def failure?; end

  # Returns the value of attribute result.
  #
  # source://karafka-web//lib/karafka/web/ui/lib/safe_runner.rb#13
  def result; end

  # @return [Boolean] was the code execution successful or not
  #
  # source://karafka-web//lib/karafka/web/ui/lib/safe_runner.rb#25
  def success?; end
end

# Sorting engine for deep in-memory structures
# It supports hashes, arrays and hash proxies.
#
# @note It handles sorting in place by mutating appropriate resources and sub-components
#
# source://karafka-web//lib/karafka/web/ui/lib/sorter.rb#11
class Karafka::Web::Ui::Lib::Sorter
  # @param sort_query [String] query for sorting or empty string if no sorting needed
  # @param allowed_attributes [Array<String>] attributes on which we allow to sort. Since
  #   we can sort on method invocations, this needs to be limited and provided on a per
  #   controller basis.
  # @return [Sorter] a new instance of Sorter
  #
  # source://karafka-web//lib/karafka/web/ui/lib/sorter.rb#24
  def initialize(sort_query, allowed_attributes:); end

  # Sorts the structure and returns it sorted.
  #
  # @param resource [Hash, Array, Lib::HashProxy] structure we want to sort
  # @param current_depth []
  #
  # source://karafka-web//lib/karafka/web/ui/lib/sorter.rb#44
  def call(resource, current_depth = T.unsafe(nil)); end

  private

  # @return [Boolean] true if we sort in desc, otherwise false
  #
  # source://karafka-web//lib/karafka/web/ui/lib/sorter.rb#137
  def desc?; end

  # Sorts an array in-place based on a specified attribute.
  #
  # The method iterates over each element in the array and applies the transformation.
  #
  # @note This method modifies the array in place (mutates the caller).
  # @param array [Array<Object>] The array of elements to be sorted
  # @param current_depth [Integer] The current depth of the sorting operation,
  #   used in the `call` method to handle nested structures or recursion.
  #
  # source://karafka-web//lib/karafka/web/ui/lib/sorter.rb#123
  def sort_array!(array, current_depth); end

  # Sorts the hash in place
  #
  # @param hash [Hash] hash we want to sort
  # @param current_depth [Integer] current depth of sorting from root
  #
  # source://karafka-web//lib/karafka/web/ui/lib/sorter.rb#77
  def sort_hash!(hash, current_depth); end

  # Extracts the attribute based on which we should sort (if present)
  #
  # @param element [Object] takes the element object and depending on its type, tries to
  #   figure out the value based on which we may sort
  # @return [Object, nil] sortable value or nil if nothing to sort
  #
  # source://karafka-web//lib/karafka/web/ui/lib/sorter.rb#146
  def sortable_value(element); end
end

# We can support only two order types
#
# source://karafka-web//lib/karafka/web/ui/lib/sorter.rb#13
Karafka::Web::Ui::Lib::Sorter::ALLOWED_ORDERS = T.let(T.unsafe(nil), Array)

# Max depth for nested sorting
#
# source://karafka-web//lib/karafka/web/ui/lib/sorter.rb#16
Karafka::Web::Ui::Lib::Sorter::MAX_DEPTH = T.let(T.unsafe(nil), Integer)

# Ttl Cache for caching things in-memory
#
# @note It **is** thread-safe
#
# source://karafka-web//lib/karafka/web/ui/lib/ttl_cache.rb#10
class Karafka::Web::Ui::Lib::TtlCache
  include ::Karafka::Core::Helpers::Time

  # @param ttl [Integer] time in ms how long should this cache keep data
  # @return [TtlCache] a new instance of TtlCache
  #
  # source://karafka-web//lib/karafka/web/ui/lib/ttl_cache.rb#14
  def initialize(ttl); end

  # Clears the whole cache
  #
  # source://karafka-web//lib/karafka/web/ui/lib/ttl_cache.rb#60
  def clear; end

  # Reads from the cache and if value not present, will run the block and store its result
  # in the cache
  #
  # @param key [String, Symbol] key for the cache read
  # @return [Object] anything that was cached or yielded
  #
  # source://karafka-web//lib/karafka/web/ui/lib/ttl_cache.rb#49
  def fetch(key); end

  # Reads data from the cache
  #
  # @param key [String, Symbol] key for the cache read
  # @return [Object] anything that was cached
  #
  # source://karafka-web//lib/karafka/web/ui/lib/ttl_cache.rb#25
  def read(key); end

  # Writes to the cache
  #
  # @param key [String, Symbol] key for the cache
  # @param value [Object] value we want to cache
  # @return [Object] value we have written
  #
  # source://karafka-web//lib/karafka/web/ui/lib/ttl_cache.rb#37
  def write(key, value); end

  private

  # Removes expired elements from the cache
  #
  # source://karafka-web//lib/karafka/web/ui/lib/ttl_cache.rb#70
  def evict; end
end

# Namespace for models representing pieces of data about Karafka setup
#
# source://karafka-web//lib/karafka/web/config.rb#0
module Karafka::Web::Ui::Models; end

# Represents a single broker data within the cluster
#
# source://karafka-web//lib/karafka/web/ui/models/broker.rb#8
class Karafka::Web::Ui::Models::Broker < ::Karafka::Web::Ui::Lib::HashProxy
  # @return [Array<Karafka::Admin::Configs::Config>] all broker configs
  #
  # source://karafka-web//lib/karafka/web/ui/models/broker.rb#53
  def configs; end

  # @return [String] full broker name for presentation
  #
  # source://karafka-web//lib/karafka/web/ui/models/broker.rb#48
  def full_name; end

  # @return [Integer]
  #
  # source://karafka-web//lib/karafka/web/ui/models/broker.rb#33
  def id; end

  # @return [String]
  #
  # source://karafka-web//lib/karafka/web/ui/models/broker.rb#38
  def name; end

  # @return [Integer]
  #
  # source://karafka-web//lib/karafka/web/ui/models/broker.rb#43
  def port; end

  class << self
    # @return [Array<Broker>] all brokers in the cluster
    #
    # source://karafka-web//lib/karafka/web/ui/models/broker.rb#11
    def all; end

    # Finds requested broker
    #
    # @param broker_id [String, Integer] id of the broker
    # @raise [::Karafka::Web::Errors::Ui::NotFoundError]
    # @return [Broker]
    #
    # source://karafka-web//lib/karafka/web/ui/models/broker.rb#23
    def find(broker_id); end
  end
end

# Wraps around the `Lib::Admin#cluster_info` with caching and some additional aliases
# so we can reference relevant information easily
#
# source://karafka-web//lib/karafka/web/ui/models/cluster_info.rb#9
class Karafka::Web::Ui::Models::ClusterInfo
  class << self
    # Gets us all the cluster metadata info
    #
    # @param cached [Boolean] should we use cached data (true by default)
    # @return [Rdkafka::Metadata] cluster metadata info
    #
    # source://karafka-web//lib/karafka/web/ui/models/cluster_info.rb#15
    def fetch(cached: T.unsafe(nil)); end

    # @param topic_name [String] name of the topic we are looking for
    # @param cached [Boolean] should we use cached data (true by default)
    # @return [Integer] number of partitions in a given topic
    #
    # source://karafka-web//lib/karafka/web/ui/models/cluster_info.rb#51
    def partitions_count(topic_name, cached: T.unsafe(nil)); end

    # Fetches us details about particular topic
    #
    # @param topic_name [String] name of the topic we are looking for
    # @param cached [Boolean] should we use cached data (true by default)
    # @return [Ui::Models::Topic] topic details
    #
    # source://karafka-web//lib/karafka/web/ui/models/cluster_info.rb#42
    def topic(topic_name, cached: T.unsafe(nil)); end

    # Returns us all the info about available topics from the cluster
    #
    # @param cached [Boolean] should we use cached data (true by default)
    # @return [Array<Ui::Models::Topic>] topics details
    #
    # source://karafka-web//lib/karafka/web/ui/models/cluster_info.rb#31
    def topics(cached: T.unsafe(nil)); end
  end
end

# Representation of data of a Karafka consumer group
#
# source://karafka-web//lib/karafka/web/ui/models/consumer_group.rb#9
class Karafka::Web::Ui::Models::ConsumerGroup < ::Karafka::Web::Ui::Lib::HashProxy
  # @return [Array<SubscriptionGroup>] Data of topics belonging to this consumer group
  #
  # source://karafka-web//lib/karafka/web/ui/models/consumer_group.rb#11
  def subscription_groups; end
end

# Model representing the current consumers metrics most recent state
#
# source://karafka-web//lib/karafka/web/ui/models/consumers_metrics.rb#8
class Karafka::Web::Ui::Models::ConsumersMetrics < ::Karafka::Web::Ui::Lib::HashProxy
  class << self
    # @return [State, false] current consumers metrics or false if not found
    #
    # source://karafka-web//lib/karafka/web/ui/models/consumers_metrics.rb#11
    def current; end

    # @raise [::Karafka::Web::Errors::Ui::NotFoundError] raised when there is no metrics.
    # @return [State] current consumers metrics
    #
    # source://karafka-web//lib/karafka/web/ui/models/consumers_metrics.rb#27
    def current!; end

    private

    # @return [::Karafka::Messages::Message, nil] most recent state or nil if none
    #
    # source://karafka-web//lib/karafka/web/ui/models/consumers_metrics.rb#34
    def fetch; end
  end
end

# Represents the current consumer processes aggregated state
# This state is the core of Karafka reporting. It holds the most important aggregated data
# as well as pointers to states of particular consumers and their details.
#
# source://karafka-web//lib/karafka/web/ui/models/consumers_state.rb#10
class Karafka::Web::Ui::Models::ConsumersState < ::Karafka::Web::Ui::Lib::HashProxy
  extend ::Karafka::Core::Helpers::Time

  class << self
    # @note Current state may contain expired data, for example of processes that were
    #   forcefully killed, etc. We clean this prior to returning the state.
    # @return [State, false] current aggregated state or false if not found
    #
    # source://karafka-web//lib/karafka/web/ui/models/consumers_state.rb#17
    def current; end

    # @raise [::Karafka::Web::Errors::Ui::NotFoundError] raised when there is no current
    #   state.
    # @return [State] current aggregated state
    #
    # source://karafka-web//lib/karafka/web/ui/models/consumers_state.rb#36
    def current!; end

    private

    # Evicts (removes) details about processes that are beyond our TTL on liveliness
    #
    # @param state_hash [Hash] raw message state hash
    #
    # source://karafka-web//lib/karafka/web/ui/models/consumers_state.rb#55
    def evict_expired_processes(state_hash); end

    # @return [::Karafka::Messages::Message, nil] most recent state or nil if none
    #
    # source://karafka-web//lib/karafka/web/ui/models/consumers_state.rb#43
    def fetch; end

    # Sorts the processes based on their unique ids, so they are always in order
    #
    # @param state_hash [Hash] raw message state hash
    #
    # source://karafka-web//lib/karafka/web/ui/models/consumers_state.rb#65
    def sort_processes(state_hash); end
  end
end

# Represents the top counters bar values on the consumers view
#
# source://karafka-web//lib/karafka/web/ui/models/counters.rb#8
class Karafka::Web::Ui::Models::Counters < ::Karafka::Web::Ui::Lib::HashProxy
  # @param state [Hash]
  # @return [Counters] a new instance of Counters
  #
  # source://karafka-web//lib/karafka/web/ui/models/counters.rb#15
  def initialize(state); end

  # @return [Integer] number of jobs that are not yet running. This includes jobs on the
  #   workers queue as well as jobs in the scheduling
  #
  # source://karafka-web//lib/karafka/web/ui/models/counters.rb#22
  def pending; end

  private

  # Estimates the number of errors present in the errors topic.
  #
  # source://karafka-web//lib/karafka/web/ui/models/counters.rb#29
  def estimate_errors_count; end
end

# Max errors partitions we support for estimations
#
# source://karafka-web//lib/karafka/web/ui/models/counters.rb#10
Karafka::Web::Ui::Models::Counters::MAX_ERROR_PARTITIONS = T.let(T.unsafe(nil), Integer)

# Aggregated health data statistics representation
#
# source://karafka-web//lib/karafka/web/ui/models/health.rb#8
class Karafka::Web::Ui::Models::Health
  class << self
    # @return [Hash] hash with cluster lag data
    #
    # source://karafka-web//lib/karafka/web/ui/models/health.rb#22
    def cluster_lags_with_offsets; end

    # @param state [State] current system state
    # @return [Hash] hash with aggregated statistics
    #
    # source://karafka-web//lib/karafka/web/ui/models/health.rb#12
    def current(state); end

    private

    # Aggregates rebalances ages data
    #
    # @param state [Hash]
    # @param stats [Hash] hash where we will store all the aggregated data
    #
    # source://karafka-web//lib/karafka/web/ui/models/health.rb#68
    def fetch_rebalance_ages(state, stats); end

    # Aggregates data on a per topic basis (in the context of a consumer group)
    #
    # @param state [Hash]
    # @param stats [Hash] hash where we will store all the aggregated data
    #
    # source://karafka-web//lib/karafka/web/ui/models/health.rb#52
    def fetch_topics_data(state, stats); end

    # Iterates over all partitions, yielding with extra expanded details
    #
    # @param state [State]
    #
    # source://karafka-web//lib/karafka/web/ui/models/health.rb#90
    def iterate_partitions(state); end

    # Sorts data so we always present it in an alphabetical order
    #
    # @param stats [Hash] stats hash
    # @return [Hash] sorted data
    #
    # source://karafka-web//lib/karafka/web/ui/models/health.rb#114
    def sort_structure(stats); end
  end
end

# Single job data representation model
#
# source://karafka-web//lib/karafka/web/ui/models/job.rb#8
class Karafka::Web::Ui::Models::Job < ::Karafka::Web::Ui::Lib::HashProxy; end

# Model representing group of jobs
#
# It simplifies filtering on running jobs and others, etc
#
# source://karafka-web//lib/karafka/web/ui/models/jobs.rb#10
class Karafka::Web::Ui::Models::Jobs
  include ::Enumerable
  extend ::Forwardable

  # @param jobs_array [Array<Job>] all jobs we want to enclose
  # @return [Jobs] a new instance of Jobs
  #
  # source://karafka-web//lib/karafka/web/ui/models/jobs.rb#18
  def initialize(jobs_array); end

  # Allows for iteration over jobs
  #
  # @param block [Proc] block to call for each job
  #
  # source://karafka-web//lib/karafka/web/ui/models/jobs.rb#41
  def each(&block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def empty?(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def map!(*args, **_arg1, &block); end

  # @return [Jobs] pending jobs
  #
  # source://karafka-web//lib/karafka/web/ui/models/jobs.rb#28
  def pending; end

  # source://forwardable/1.3.3/forwardable.rb#231
  def reverse!(*args, **_arg1, &block); end

  # @return [Jobs] running jobs
  #
  # source://karafka-web//lib/karafka/web/ui/models/jobs.rb#23
  def running; end

  # Creates a new Jobs object with selected jobs
  #
  # @param block [Proc] select proc
  # @return [Jobs] selected jobs enclosed with the Jobs object
  #
  # source://karafka-web//lib/karafka/web/ui/models/jobs.rb#35
  def select(&block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def size(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def sort_by!(*args, **_arg1, &block); end
end

# A proxy between `::Karafka::Messages::Message` and web UI
# We work with the Karafka messages but use this model to wrap the work needed.
#
# source://karafka-web//lib/karafka/web/ui/models/message.rb#9
class Karafka::Web::Ui::Models::Message
  extend ::Karafka::Web::Ui::Lib::Paginations::Paginators

  class << self
    # Looks for a message from a given topic partition
    #
    # @param topic_id [String]
    # @param partition_id [Integer]
    # @param offset [Integer]
    # @raise [::Karafka::Web::Errors::Ui::NotFoundError] when not found
    #
    # source://karafka-web//lib/karafka/web/ui/models/message.rb#19
    def find(topic_id, partition_id, offset); end

    # Fetches requested `page_count` number of Kafka messages starting from the oldest
    # requested `start_offset`. If `start_offset` is `-1`, will fetch the most recent
    # results
    #
    # @param topic_id [String]
    # @param partition_id [Integer]
    # @param start_offset [Integer] oldest offset from which we want to get the data
    # @param watermark_offsets [Ui::Models::WatermarkOffsets] watermark offsets
    # @return [Array] We return page data as well as all the details needed to build
    #   the pagination details.
    #
    # source://karafka-web//lib/karafka/web/ui/models/message.rb#45
    def offset_page(topic_id, partition_id, start_offset, watermark_offsets); end

    # Fetches requested `page_count` number of Kafka messages from the topic partitions
    # and merges the results. Ensures, that pagination works as expected.
    #
    # @param topic_id [String]
    # @param partitions_ids [Array<Integer>] for which of the partitions we want to
    #   get the data. This is a limiting factor because of the fact that we have to
    #   query the watermark offsets independently
    # @param page [Integer] which page we want to get
    #
    # source://karafka-web//lib/karafka/web/ui/models/message.rb#129
    def topic_page(topic_id, partitions_ids, page); end

    private

    # Since we paginate with compacted offsets visible but we do not get compacted messages
    # we need to fill those with  just the missing offset and handle this on the UI.
    #
    # @param messages [Array<Karafka::Messages::Message>] selected messages
    # @param partition_id [Integer] number of partition for which we fill message gap
    # @param start_offset [Integer] offset of the first message (lowest) that we received
    # @param count [Integer] how many messages we wanted - we need that to fill spots to
    #   have exactly the number that was  requested and not more
    # @param high_offset [Integer] high watermark offset
    # @return [Array<Karafka::Messages::Message, Integer>] array with gaps filled with the
    #   missing offset
    #
    # source://karafka-web//lib/karafka/web/ui/models/message.rb#227
    def fill_compacted(messages, partition_id, start_offset, count, high_offset); end

    # @return [Integer] elements per page
    #
    # source://karafka-web//lib/karafka/web/ui/models/message.rb#212
    def per_page; end

    # @param args [Object] anything required by the admin `#read_topic`
    # @return [Array<Karafka::Messages::Message>, false] topic partition messages or false
    #   in case we hit a non-existing offset
    #
    # source://karafka-web//lib/karafka/web/ui/models/message.rb#203
    def read_topic(*args); end
  end
end

# Namespace for metrics related models
#
# source://karafka-web//lib/karafka/web.rb#0
module Karafka::Web::Ui::Models::Metrics; end

# Materializes the aggregated data and computes the expected diffs out of the snapshots
# We do some pre-processing to make sure, we do not have bigger gaps and to compensate
# for reporting drifting
#
# source://karafka-web//lib/karafka/web/ui/models/metrics/aggregated.rb#12
class Karafka::Web::Ui::Models::Metrics::Aggregated < ::Karafka::Web::Ui::Lib::HashProxy
  include ::Karafka::Core::Helpers::Time

  # Builds the Web-UI historicals representation that includes deltas
  #
  # @param aggregated [Hash] aggregated historical metrics
  # @return [Aggregated] a new instance of Aggregated
  #
  # source://karafka-web//lib/karafka/web/ui/models/metrics/aggregated.rb#39
  def initialize(aggregated); end

  # @return [Boolean] do we have enough data to draw any basic charts
  #
  # source://karafka-web//lib/karafka/web/ui/models/metrics/aggregated.rb#50
  def sufficient?; end

  private

  # Computes deltas for all the relevant keys for which we want to have deltas
  #
  # @param previous [Hash]
  # @param current [Hash]
  # @return [Hash] delta computed values
  #
  # source://karafka-web//lib/karafka/web/ui/models/metrics/aggregated.rb#183
  def compute_deltas(previous, current); end

  # Batch size is a match between number of messages and number of batches
  # It is derived out of the data we have so we compute it on the fly
  #
  # @param historicals [Hash] all historicals for all the ranges
  #
  # source://karafka-web//lib/karafka/web/ui/models/metrics/aggregated.rb#149
  def enrich_with_batch_size(historicals); end

  # Takes the historical hash, iterates over all the samples and enriches them with the
  # delta computed values
  #
  # @param historicals [Hash] all historicals for all the ranges
  # @return [Hash] historicals with delta based data
  #
  # source://karafka-web//lib/karafka/web/ui/models/metrics/aggregated.rb#123
  def enrich_with_deltas(historicals); end

  # Adds an average RSS on a per process basis
  #
  # @param historicals [Hash] all historicals for all the ranges
  #
  # source://karafka-web//lib/karafka/web/ui/models/metrics/aggregated.rb#165
  def enrich_with_process_rss(historicals); end

  # In case of a positive drift, we may have gaps bigger than few seconds in reporting.
  # This can create a false sense of spikes that do not reflect the reality. We compensate
  # this by extrapolating the delta values and using the rest as they are.
  #
  # This problems only affects our near real-time metrics with seconds precision
  #
  # @param historicals [Hash] all historicals for all the ranges
  #
  # source://karafka-web//lib/karafka/web/ui/models/metrics/aggregated.rb#90
  def fill_gaps(historicals); end

  # Since our reporting is not ms precise, there are cases where sampling can drift.
  # If drifting gets us close to one side, for delta metrics it would create sudden
  # artificial drops in metrics that would not match the reality. We reject drifters like
  # this as we can compensate this later.
  #
  # This problems only affects our near real-time metrics with seconds precision
  #
  # @param historicals [Hash] all historicals for all the ranges
  #
  # source://karafka-web//lib/karafka/web/ui/models/metrics/aggregated.rb#64
  def reject_drifters(historicals); end
end

# For which keys we should compute the delta in reference to the previous period
# Metrics we get from the processes are always absolute, hence we need a reference point
# to compute the deltas
#
# If at least two elements do not exist for given delta range, we keep it empty
#
# source://karafka-web//lib/karafka/web/ui/models/metrics/aggregated.rb#26
Karafka::Web::Ui::Models::Metrics::Aggregated::DELTA_KEYS = T.let(T.unsafe(nil), Array)

# If samples are further away than that, we will inject an artificial sample in-between
#
# source://karafka-web//lib/karafka/web/ui/models/metrics/aggregated.rb#19
Karafka::Web::Ui::Models::Metrics::Aggregated::MAX_ACCEPTED_DRIFT = T.let(T.unsafe(nil), Integer)

# If samples are closer than that, sample will be rejected
#
# source://karafka-web//lib/karafka/web/ui/models/metrics/aggregated.rb#16
Karafka::Web::Ui::Models::Metrics::Aggregated::MIN_ACCEPTED_DRIFT = T.let(T.unsafe(nil), Integer)

# Namespace for models related to presentation of charts
#
# source://karafka-web//lib/karafka/web.rb#0
module Karafka::Web::Ui::Models::Metrics::Charts; end

# Model for formatting aggregated metrics data for charts
#
# source://karafka-web//lib/karafka/web/ui/models/metrics/charts/aggregated.rb#11
class Karafka::Web::Ui::Models::Metrics::Charts::Aggregated < ::Karafka::Web::Ui::Lib::HashProxy
  # @param aggregated [Hash] all aggregated for all periods
  # @param period [Symbol] period that we are interested in
  # @return [Aggregated] a new instance of Aggregated
  #
  # source://karafka-web//lib/karafka/web/ui/models/metrics/charts/aggregated.rb#14
  def initialize(aggregated, period); end

  # @return [Array<Array<Symbol, Integer>>] active listeners statistics
  #
  # source://karafka-web//lib/karafka/web/ui/models/metrics/charts/aggregated.rb#47
  def active_listeners; end

  # @return [String] JSON with bytes sent and bytes received metrics
  #
  # source://karafka-web//lib/karafka/web/ui/models/metrics/charts/aggregated.rb#20
  def data_transfers; end

  # Handles delegation to fetch appropriate historical metrics based on their name
  #
  # @param method_name [String]
  # @param arguments [Array] missing method call arguments
  #
  # source://karafka-web//lib/karafka/web/ui/models/metrics/charts/aggregated.rb#70
  def method_missing(method_name, *arguments); end

  # @return [Array<Array<Symbol, Integer>>] standby listeners statistics
  #
  # source://karafka-web//lib/karafka/web/ui/models/metrics/charts/aggregated.rb#54
  def standby_listeners; end

  # @param args [Array<String>] names of aggregated we want to show
  # @return [String] JSON with data about all the charts we were interested in
  #
  # source://karafka-web//lib/karafka/web/ui/models/metrics/charts/aggregated.rb#39
  def with(*args); end

  private

  # @param method_name [String]
  # @param include_private [Boolean]
  # @return [Boolean]
  #
  # source://karafka-web//lib/karafka/web/ui/models/metrics/charts/aggregated.rb#62
  def respond_to_missing?(method_name, include_private = T.unsafe(nil)); end
end

# Model for preparing data about topics states
#
# source://karafka-web//lib/karafka/web/ui/models/metrics/charts/topics.rb#10
class Karafka::Web::Ui::Models::Metrics::Charts::Topics
  # @param topics_data [Hash] topics aggregated metrics data
  # @param period [Symbol] period that we are interested in
  # @return [Topics] a new instance of Topics
  #
  # source://karafka-web//lib/karafka/web/ui/models/metrics/charts/topics.rb#13
  def initialize(topics_data, period); end

  # @return [String] JSON with lags of each of the topics + total lag of all the topics
  #   from all the consumer groups.
  #
  # source://karafka-web//lib/karafka/web/ui/models/metrics/charts/topics.rb#19
  def lags_hybrid; end

  # @return [String] JSON with per-topic, highest LSO freeze duration. Useful for
  #   debugging of issues arising from hanging transactions
  #
  # source://karafka-web//lib/karafka/web/ui/models/metrics/charts/topics.rb#72
  def max_lso_time; end

  # @return [String] JSON with producers pace that represents high-watermarks sum for
  #   each topic
  #
  # source://karafka-web//lib/karafka/web/ui/models/metrics/charts/topics.rb#52
  def topics_pace; end
end

# Representation of topics historical metrics based on the aggregated metrics data
# We do some pre-processing to align and normalize all the data
#
# source://karafka-web//lib/karafka/web/ui/models/metrics/topics.rb#10
class Karafka::Web::Ui::Models::Metrics::Topics < ::Karafka::Web::Ui::Lib::HashProxy
  # @param consumers_groups [Hash] historical metrics for consumers groups
  # @return [Topics] a new instance of Topics
  #
  # source://karafka-web//lib/karafka/web/ui/models/metrics/topics.rb#12
  def initialize(consumers_groups); end

  private

  # Extracts and aggregates data on a per-topic basis in a hash. Because in theory same
  # topic can be consumed by multiple consumer groups, we include consumer group in the
  # hash keys.
  #
  # @param consumers_groups [Hash] consumers groups initial hash with metrics
  # @return [Hash] remapped hash with range including extracted topics details
  #
  # source://karafka-web//lib/karafka/web/ui/models/metrics/topics.rb#26
  def aggregate_topics_data(consumers_groups); end

  # Nullifies gaps within data with metrics with nil values. This is needed for us to be
  # able to provide consistent charts even with gaps in reporting.
  #
  # @note This modifies the original data in place
  # @note We nullify both gaps in metrics as well as gaps in times (no values for time)
  # @param topics_metrics [Hash] flattened topics data
  #
  # source://karafka-web//lib/karafka/web/ui/models/metrics/topics.rb#60
  def nulify_gaps(topics_metrics); end
end

# Single topic partition data representation model
#
# source://karafka-web//lib/karafka/web/ui/models/partition.rb#8
class Karafka::Web::Ui::Models::Partition < ::Karafka::Web::Ui::Lib::HashProxy
  # @param args [Object] anything hash proxy accepts
  # @return [Partition] a new instance of Partition
  #
  # source://karafka-web//lib/karafka/web/ui/models/partition.rb#10
  def initialize(*args); end

  # Because `lag_stored` is not available until first marking, we fallback to the lag
  #   value that may be behind but is always available until stored lag is available.
  #
  # @return [Integer] hybrid log value
  #
  # source://karafka-web//lib/karafka/web/ui/models/partition.rb#22
  def lag_hybrid; end

  # @return [Integer] hybrid log delta
  #
  # source://karafka-web//lib/karafka/web/ui/models/partition.rb#27
  def lag_hybrid_d; end

  # @note States descriptions:
  #   - `:active` all good. No hanging transactions, processing is ok
  #   - `:at_risk` - there may be hanging transactions but they do not affect processing
  #   before being stuck. This means, that the transaction still may be finished
  #   without affecting the processing, hence not having any impact.
  #   - `:stopped` - we have reached a hanging LSO and we cannot move forward despite more
  #   data being available. Unless the hanging transaction is killed or it finishes,
  #   we will not move forward.
  # @return [Symbol] one of three states in which LSO can be in the correlation to given
  #   partition in the context of a consumer group.
  #
  # source://karafka-web//lib/karafka/web/ui/models/partition.rb#42
  def lso_risk_state; end
end

# Single consumer process representation
#
# source://karafka-web//lib/karafka/web/ui/models/process.rb#8
class Karafka::Web::Ui::Models::Process < ::Karafka::Web::Ui::Lib::HashProxy
  # @return [Array<ConsumerGroup>] consumer groups to which this process is subscribed in
  #   an alphabetical order
  #
  # source://karafka-web//lib/karafka/web/ui/models/process.rb#23
  def consumer_groups; end

  # Jobs sorted from longest running to youngest
  #
  # @return [Array<Job>] current jobs of this process
  #
  # source://karafka-web//lib/karafka/web/ui/models/process.rb#32
  def jobs; end

  # @return [Integer] collective hybrid lag on this process
  #
  # source://karafka-web//lib/karafka/web/ui/models/process.rb#50
  def lag_hybrid; end

  # @return [Integer] number of pending jobs on a process
  #
  # source://karafka-web//lib/karafka/web/ui/models/process.rb#45
  def pending_jobs_count; end

  # @return [Integer] number of running jobs on a process
  #
  # source://karafka-web//lib/karafka/web/ui/models/process.rb#40
  def running_jobs_count; end

  # @return [Boolean] true if there are any active subscriptions, otherwise false.
  #
  # source://karafka-web//lib/karafka/web/ui/models/process.rb#61
  def subscribed?; end

  # @return [Integer] number of partitions to which we are currently subscribed
  #
  # source://karafka-web//lib/karafka/web/ui/models/process.rb#70
  def subscribed_partitions_count; end

  class << self
    # Looks for a given process based on its id
    #
    # @param state [State] state of the system based on which we will do the lookup
    # @param process_id [String] id of the process we are looking for
    # @raise [::Karafka::Web::Errors::Ui::NotFoundError] raised if process not found
    # @return [Process] selected process or error raised
    #
    # source://karafka-web//lib/karafka/web/ui/models/process.rb#15
    def find(state, process_id); end
  end
end

# Represents the active processes data
#
# @note Active also includes processes stopped recently. We use it to provide better
#   visibility via UI.
#
# source://karafka-web//lib/karafka/web/ui/models/processes.rb#10
module Karafka::Web::Ui::Models::Processes
  extend ::Karafka::Core::Helpers::Time

  class << self
    # Returns the active processes in an array and alongside of that the current state of
    # the system. We use those together in the UI and it would be expensive to pick it up
    # while we've already had it.
    #
    # @param state [State] current system state from which we can get processes metadata
    # @return [Array<Process>]
    #
    # source://karafka-web//lib/karafka/web/ui/models/processes.rb#20
    def active(state); end

    private

    # Removes processes that are no longer active. They may still be here because the state
    # may have a small lag but we want to compensate for it that way.
    #
    # @param processes [Array<Hash>]
    # @return [Array<Hash>] only active processes data
    #
    # source://karafka-web//lib/karafka/web/ui/models/processes.rb#73
    def evict_expired_processes(processes); end

    # Fetches the relevant processes reports from the reports topic
    #
    # @param state [State]
    # @return [Array<Hash>] array with deserialized processes reports
    #
    # source://karafka-web//lib/karafka/web/ui/models/processes.rb#35
    def fetch_reports(state); end

    # Ensures that we always return processes sorted by their id
    #
    # @param processes [Array<Hash>]
    # @return [Array<Hash>] sorted processes data
    #
    # source://karafka-web//lib/karafka/web/ui/models/processes.rb#85
    def sort_processes(processes); end

    # Collapses processes data and only keeps the most recent report for give process
    #
    # @param processes [Array<Hash>]
    # @return [Array<Hash>] unique processes data
    #
    # source://karafka-web//lib/karafka/web/ui/models/processes.rb#62
    def squash_processes_data(processes); end
  end
end

# Model that represents the general status of the Web UI.
#
# We use this data to display a status page that helps with debugging on what is missing
# in the overall setup of the Web UI.
#
# People have various problems like too many partitions, not created topics, etc. and this
# data and view aims to help them with understanding the current status of the setup
#
# source://karafka-web//lib/karafka/web/ui/models/status.rb#14
class Karafka::Web::Ui::Models::Status
  # Some people try to work with Kafka over the internet with really high latency and this
  # should be highlighted in the UI as often the connection just becomes unstable
  #
  # @return [Status::Step] were we able to connect to Kafka or not and how fast.
  #
  # source://karafka-web//lib/karafka/web/ui/models/status.rb#59
  def connection; end

  # @return [Status::Step] could we read and operate on the current processes data (if any)
  #
  # source://karafka-web//lib/karafka/web/ui/models/status.rb#184
  def consumers_reports; end

  # @return [Status::Step] Are we able to actually digest the consumers reports with the
  #   consumer that is consuming them.
  #
  # source://karafka-web//lib/karafka/web/ui/models/status.rb#234
  def consumers_reports_schema_state; end

  # Is karafka-web enabled in the `karafka.rb`
  # Checks if the consumer group for web-ui is injected.
  # It does **not** check if the group is active because this may depend on the
  # configuration details, but for the Web-UI web app to work, the routing needs to be
  # aware of the deserializer, etc
  #
  # source://karafka-web//lib/karafka/web/ui/models/status.rb#45
  def enabled; end

  # @return [Status::Step] Is the initial consumers metrics record present in Kafka and
  #   that they can be deserialized
  #
  # source://karafka-web//lib/karafka/web/ui/models/status.rb#162
  def initial_consumers_metrics; end

  # @return [Status::Step] Is the initial consumers state present in Kafka and that they
  #   can be deserialized
  #
  # source://karafka-web//lib/karafka/web/ui/models/status.rb#139
  def initial_consumers_state; end

  # @return [Status::Step] Is there at least one active karafka server reporting to the
  #   Web UI
  #
  # source://karafka-web//lib/karafka/web/ui/models/status.rb#199
  def live_reporting; end

  # @return [Status::Step] do we have all topics with expected number of partitions
  #
  # source://karafka-web//lib/karafka/web/ui/models/status.rb#98
  def partitions; end

  # @note It's not an error not to have it but we want to warn, that some of the features
  #   may not work without Pro.
  # @return [Status::Step] is Pro enabled with all of its features.
  #
  # source://karafka-web//lib/karafka/web/ui/models/status.rb#272
  def pro_subscription; end

  # @return [Status::Step] do we have correct replication for given env
  #
  # source://karafka-web//lib/karafka/web/ui/models/status.rb#117
  def replication; end

  # @return [Status::Step] are there any active topics in the routing that are not present
  #   in the cluster (does not apply to patterns)
  #
  # source://karafka-web//lib/karafka/web/ui/models/status.rb#249
  def routing_topics_presence; end

  # @return [Status::Step] is there a subscription to our reports topic that is being
  #   consumed actively.
  #
  # source://karafka-web//lib/karafka/web/ui/models/status.rb#214
  def state_calculation; end

  # @return [Status::Step] do all the needed topics exist
  #
  # source://karafka-web//lib/karafka/web/ui/models/status.rb#82
  def topics; end

  private

  # Tries connecting with the cluster and saves the cluster info and the connection time
  #
  # @note If fails, `connection_time` will be 1_000_000
  #
  # source://karafka-web//lib/karafka/web/ui/models/status.rb#329
  def connect; end

  # @return [String] consumers metrics topic name
  #
  # source://karafka-web//lib/karafka/web/ui/models/status.rb#292
  def topics_consumers_metrics; end

  # @return [String] consumers reports topic name
  #
  # source://karafka-web//lib/karafka/web/ui/models/status.rb#287
  def topics_consumers_reports; end

  # @return [String] consumers states topic name
  #
  # source://karafka-web//lib/karafka/web/ui/models/status.rb#282
  def topics_consumers_states; end

  # @return [Hash] hash with topics with which we work details (even if don't exist)
  #
  # source://karafka-web//lib/karafka/web/ui/models/status.rb#302
  def topics_details; end

  # @return [String] errors topic name
  #
  # source://karafka-web//lib/karafka/web/ui/models/status.rb#297
  def topics_errors; end
end

# Status of a single step of setup
#
# source://karafka-web//lib/karafka/web/ui/models/status.rb#16
class Karafka::Web::Ui::Models::Status::Step < ::Struct
  # Returns the value of attribute details
  #
  # @return [Object] the current value of details
  def details; end

  # Sets the attribute details
  #
  # @param value [Object] the value to set the attribute details to.
  # @return [Object] the newly set value
  def details=(_); end

  # @return [String] local namespace for partial of a given type
  #
  # source://karafka-web//lib/karafka/web/ui/models/status.rb#23
  def partial_namespace; end

  # Returns the value of attribute status
  #
  # @return [Object] the current value of status
  def status; end

  # Sets the attribute status
  #
  # @param value [Object] the value to set the attribute status to.
  # @return [Object] the newly set value
  def status=(_); end

  # @return [Boolean] is the given step successfully configured and working
  #
  # source://karafka-web//lib/karafka/web/ui/models/status.rb#18
  def success?; end

  # @return [String] stringified status
  #
  # source://karafka-web//lib/karafka/web/ui/models/status.rb#35
  def to_s; end

  class << self
    def [](*_arg0); end
    def inspect; end
    def keyword_init?; end
    def members; end
    def new(*_arg0); end
  end
end

# Representation of data of a Karafka subscription group
#
# source://karafka-web//lib/karafka/web/ui/models/subscription_group.rb#9
class Karafka::Web::Ui::Models::SubscriptionGroup < ::Karafka::Web::Ui::Lib::HashProxy
  # @return [Array<Topic>] Data of topics belonging to this subscription group
  #
  # source://karafka-web//lib/karafka/web/ui/models/subscription_group.rb#11
  def topics; end
end

# Single topic data representation model
#
# source://karafka-web//lib/karafka/web/ui/models/topic.rb#8
class Karafka::Web::Ui::Models::Topic < ::Karafka::Web::Ui::Lib::HashProxy
  # @return [Array<Karafka::Admin::Configs::Config>] all topic configs
  #
  # source://karafka-web//lib/karafka/web/ui/models/topic.rb#41
  def configs; end

  # Generates info about estimated messages distribution in partitions, allowing for
  # inspection and detection of imbalances
  #
  # @param partitions [Array<Integer>] partitions we're interested in
  # @return [Array<HashProxy, Array<HashProxy>>] array where first value contains
  #   aggregated statistics and then the second value is an array with per partition data
  #
  # source://karafka-web//lib/karafka/web/ui/models/topic.rb#57
  def distribution(partitions); end

  # @return [Array<Partition>] All topic partitions data
  #
  # source://karafka-web//lib/karafka/web/ui/models/topic.rb#32
  def partitions; end

  class << self
    # @return [Array<Broker>] all topics in the cluster
    #
    # source://karafka-web//lib/karafka/web/ui/models/topic.rb#11
    def all; end

    # Finds requested topic
    #
    # @param topic_name [String] name of the topic
    # @raise [::Karafka::Web::Errors::Ui::NotFoundError]
    # @return [Topic]
    #
    # source://karafka-web//lib/karafka/web/ui/models/topic.rb#22
    def find(topic_name); end
  end
end

# Allows for a granular control over what parts of messages are being displayed
# There are scenarios where payload or other parts of messages should not be presented
# because they may contain sensitive data. This API allows to manage that on a per message
# basis.
#
# source://karafka-web//lib/karafka/web/ui/models/visibility_filter.rb#11
class Karafka::Web::Ui::Models::VisibilityFilter
  # Should it be allowed to download this message raw payload
  #
  # @param message [::Karafka::Messages::Message]
  # @return [Boolean] true if downloads allowed
  #
  # source://karafka-web//lib/karafka/web/ui/models/visibility_filter.rb#34
  def download?(message); end

  # Should it be allowed to download the deserialized and sanitized payload as JSON
  #
  # @param message [::Karafka::Messages::Message]
  # @return [Boolean] true if exports allowed
  #
  # source://karafka-web//lib/karafka/web/ui/models/visibility_filter.rb#42
  def export?(message); end

  # @param _message [::Karafka::Messages::Message]
  # @return [Boolean] should message headers be visible
  #
  # source://karafka-web//lib/karafka/web/ui/models/visibility_filter.rb#20
  def headers?(_message); end

  # @param _message [::Karafka::Messages::Message]
  # @return [Boolean] should message key be visible
  #
  # source://karafka-web//lib/karafka/web/ui/models/visibility_filter.rb#14
  def key?(_message); end

  # @param message [::Karafka::Messages::Message]
  # @return [Boolean] should message payload be visible
  #
  # source://karafka-web//lib/karafka/web/ui/models/visibility_filter.rb#26
  def payload?(message); end
end

# Model used for accessing watermark offsets
#
# source://karafka-web//lib/karafka/web/ui/models/watermark_offsets.rb#8
class Karafka::Web::Ui::Models::WatermarkOffsets < ::Karafka::Web::Ui::Lib::HashProxy
  # @return [Boolean] true if given partition had data but all of it was removed due
  #   to log retention and compaction policies
  #
  # source://karafka-web//lib/karafka/web/ui/models/watermark_offsets.rb#32
  def cleaned?; end

  # @return [Boolean] true if given partition never had any messages and is empty
  #
  # source://karafka-web//lib/karafka/web/ui/models/watermark_offsets.rb#26
  def empty?; end

  class << self
    # Retrieve watermark offsets for given topic partition
    #
    # @param topic_id [String]
    # @param partition_id [Integer]
    # @return [WatermarkOffsets]
    #
    # source://karafka-web//lib/karafka/web/ui/models/watermark_offsets.rb#15
    def find(topic_id, partition_id); end
  end
end

# Current gem version
#
# source://karafka-web//lib/karafka/web/version.rb#6
Karafka::Web::VERSION = T.let(T.unsafe(nil), String)
