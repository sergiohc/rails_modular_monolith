# typed: true

# DO NOT EDIT MANUALLY
# This is an autogenerated file for types exported from the `karafka` gem.
# Please instead update this file by running `bin/tapioca gem karafka`.


# ActiveJob components to allow for jobs consumption with Karafka
#
# source://karafka//lib/active_job/queue_adapters/karafka_adapter.rb#4
module ActiveJob
  class << self
    # source://activejob/7.1.3.4/lib/active_job/queue_adapter.rb#7
    def adapter_name(adapter); end

    # source://activejob/7.1.3.4/lib/active_job/deprecator.rb#4
    def deprecator; end

    # source://activejob/7.1.3.4/lib/active_job/gem_version.rb#5
    def gem_version; end

    # source://activejob/7.1.3.4/lib/active_job/enqueuing.rb#16
    def perform_all_later(*jobs); end

    # source://activejob/7.1.3.4/lib/active_job.rb#53
    def use_big_decimal_serializer; end

    # source://activejob/7.1.3.4/lib/active_job.rb#53
    def use_big_decimal_serializer=(_arg0); end

    # source://activejob/7.1.3.4/lib/active_job.rb#61
    def verbose_enqueue_logs; end

    # source://activejob/7.1.3.4/lib/active_job.rb#61
    def verbose_enqueue_logs=(_arg0); end

    # source://activejob/7.1.3.4/lib/active_job/version.rb#7
    def version; end

    private

    # source://activejob/7.1.3.4/lib/active_job/instrumentation.rb#6
    def instrument_enqueue_all(queue_adapter, jobs); end
  end
end

# source://karafka//lib/active_job/karafka.rb#12
module ActiveJob::Karafka; end

# ActiveJob queue adapters
#
# source://karafka//lib/active_job/queue_adapters/karafka_adapter.rb#6
module ActiveJob::QueueAdapters
  class << self
    # source://activejob/7.1.3.4/lib/active_job/queue_adapters.rb#137
    def lookup(name); end
  end
end

# Karafka adapter for enqueuing jobs
# This is here for ease of integration with ActiveJob.
#
# source://karafka//lib/active_job/queue_adapters/karafka_adapter.rb#9
class ActiveJob::QueueAdapters::KarafkaAdapter
  # Enqueues the job using the configured dispatcher
  #
  # @param job [Object] job that should be enqueued
  #
  # source://karafka//lib/active_job/queue_adapters/karafka_adapter.rb#13
  def enqueue(job); end

  # Enqueues multiple jobs in one go
  #
  # @param jobs [Array<Object>] jobs that we want to enqueue
  # @return [Integer] number of jobs enqueued (required by Rails)
  #
  # source://karafka//lib/active_job/queue_adapters/karafka_adapter.rb#20
  def enqueue_all(jobs); end

  # Raises info, that Karafka backend does not support scheduling jobs
  #
  # @param _job [Object] job we cannot enqueue
  # @param _timestamp [Time] time when job should run
  # @raise [NotImplementedError]
  #
  # source://karafka//lib/active_job/queue_adapters/karafka_adapter.rb#29
  def enqueue_at(_job, _timestamp); end
end

# Karafka framework main namespace
#
# source://karafka//lib/karafka.rb#22
module Karafka
  class << self
    # @example Standard only-Karafka case
    #   Karafka.boot_file #=> '/home/app_path/karafka.rb'
    # @example Non standard case
    #   KARAFKA_BOOT_FILE='/home/app_path/app.rb'
    #   Karafka.boot_file #=> '/home/app_path/app.rb'
    # @note By default it is a file called 'karafka.rb' but it can be specified as you wish if you
    #   have Karafka that is merged into a Sinatra/Rails app and karafka.rb is taken.
    #   It will be used for console/consumers/etc
    # @return [String] path to a default file that contains booting procedure etc
    #
    # source://karafka//lib/karafka.rb#94
    def boot_file; end

    # @return [String] path to Karafka gem root core
    #
    # source://karafka//lib/karafka.rb#63
    def core_root; end

    # @return [Karafka::Env] env instance that allows us to check environment
    #
    # source://karafka//lib/karafka.rb#25
    def env; end

    # @example Assign new environment to Karafka::App
    #   Karafka::App.env = :production
    # @param environment [String, Symbol] new environment that we want to set
    # @return [Karafka::Env] env instance
    #
    # source://karafka//lib/karafka.rb#33
    def env=(environment); end

    # @return [String] root path of this gem
    #
    # source://karafka//lib/karafka.rb#53
    def gem_root; end

    # @return [Logger] logger that we want to use. Will use ::Karafka::Logger by default
    #
    # source://karafka//lib/karafka.rb#38
    def logger; end

    # @return [::Karafka::Monitor] monitor that we want to use
    #
    # source://karafka//lib/karafka.rb#48
    def monitor; end

    # @return [Boolean] true if there is a valid pro token present
    #
    # source://karafka//lib/karafka.rb#68
    def pro?; end

    # @return [WaterDrop::Producer] waterdrop messages producer
    #
    # source://karafka//lib/karafka.rb#43
    def producer; end

    # @return [Boolean] Do we run within/with Rails. We use this to initialize Railtie and proxy
    #   the console invocation to Rails
    #
    # source://karafka//lib/karafka.rb#74
    def rails?; end

    # We need to be able to overwrite both monitor and logger after the configuration in case they
    # would be changed because those two (with defaults) can be used prior to the setup and their
    # state change should be reflected in the updated setup
    #
    # This method refreshes the things that might have been altered by the configuration
    #
    # source://karafka//lib/karafka.rb#103
    def refresh!; end

    # @return [String] Karafka app root path (user application path)
    #
    # source://karafka//lib/karafka.rb#58
    def root; end
  end
end

# Namespace for all the ActiveJob related things from within Karafka
#
# source://karafka//lib/karafka.rb#0
module Karafka::ActiveJob; end

# This is the consumer for ActiveJob that eats the messages enqueued with it one after another.
# It marks the offset after each message, so we make sure, none of the jobs is executed twice
#
# source://karafka//lib/karafka/active_job/consumer.rb#8
class Karafka::ActiveJob::Consumer < ::Karafka::BaseConsumer
  # Executes the ActiveJob logic
  #
  # @note ActiveJob does not support batches, so we just run one message after another
  #
  # source://karafka//lib/karafka/active_job/consumer.rb#11
  def consume; end

  private

  # Consumes a message with the job and runs needed instrumentation
  #
  # @param job_message [Karafka::Messages::Message] message with active job
  #
  # source://karafka//lib/karafka/active_job/consumer.rb#26
  def consume_job(job_message); end

  # @param job_message [Karafka::Messages::Message] message with active job
  # @yield [::ActiveSupport::JSON.decode(job_message.raw_payload)]
  #
  # source://karafka//lib/karafka/active_job/consumer.rb#41
  def with_deserialized_job(job_message); end
end

# Dispatcher that sends the ActiveJob job to a proper topic based on the queue name
#
# source://karafka//lib/karafka/active_job/dispatcher.rb#6
class Karafka::ActiveJob::Dispatcher
  # @param job [ActiveJob::Base] job
  #
  # source://karafka//lib/karafka/active_job/dispatcher.rb#17
  def dispatch(job); end

  # Bulk dispatches multiple jobs using the Rails 7.1+ API
  #
  # @param jobs [Array<ActiveJob::Base>] jobs we want to dispatch
  #
  # source://karafka//lib/karafka/active_job/dispatcher.rb#27
  def dispatch_many(jobs); end

  private

  # @param job [ActiveJob::Base] job
  # @param key [Symbol] key we want to fetch
  # @param defaults [Hash]
  # @return [Object] options we are interested in
  #
  # source://karafka//lib/karafka/active_job/dispatcher.rb#55
  def fetch_option(job, key, defaults); end

  # @param job [ActiveJob::Base] job
  # @return [Hash] json representation of the job
  #
  # source://karafka//lib/karafka/active_job/dispatcher.rb#64
  def serialize_job(job); end
end

# Defaults for dispatching
# The can be updated by using `#karafka_options` on the job
#
# source://karafka//lib/karafka/active_job/dispatcher.rb#9
Karafka::ActiveJob::Dispatcher::DEFAULTS = T.let(T.unsafe(nil), Hash)

# Allows for setting karafka specific options in ActiveJob jobs
#
# source://karafka//lib/karafka/active_job/job_extensions.rb#6
module Karafka::ActiveJob::JobExtensions
  # @param new_options [Hash] additional options that allow for jobs Karafka related options
  #   customization
  # @return [Hash] karafka options
  #
  # source://karafka//lib/karafka/active_job/job_extensions.rb#19
  def karafka_options(new_options = T.unsafe(nil)); end

  class << self
    # Defines all the needed accessors and sets defaults
    #
    # @param klass [ActiveJob::Base] active job base
    #
    # source://karafka//lib/karafka/active_job/job_extensions.rb#10
    def extended(klass); end
  end
end

# Contract for validating the options that can be altered with `#karafka_options` per job class
#
# @note We keep this in the `Karafka::ActiveJob` namespace instead of `Karafka::Contracts` as
#   we want to keep ActiveJob related Karafka components outside of the core Karafka code and
#   all in the same place
#
# source://karafka//lib/karafka/active_job/job_options_contract.rb#9
class Karafka::ActiveJob::JobOptionsContract < ::Karafka::Contracts::Base; end

# Admin actions that we can perform via Karafka on our Kafka cluster
#
# @note It always initializes a new admin instance as we want to ensure it is always closed
#   Since admin actions are not performed that often, that should be ok.
# @note It always uses the primary defined cluster and does not support multi-cluster work.
#   Cluster on which operations are performed can be changed via `admin.kafka` config, however
#   there is no multi-cluster runtime support.
#
# source://karafka//lib/karafka/admin.rb#12
module Karafka::Admin
  class << self
    # @return [Rdkafka::Metadata] cluster metadata info
    #
    # source://karafka//lib/karafka/admin.rb#412
    def cluster_info; end

    # Creates more partitions for a given topic
    #
    # @param name [String] topic name
    # @param partitions [Integer] total number of partitions we expect to end up with
    #
    # source://karafka//lib/karafka/admin.rb#135
    def create_partitions(name, partitions); end

    # Creates Kafka topic with given settings
    #
    # @param name [String] topic name
    # @param partitions [Integer] number of partitions we expect
    # @param replication_factor [Integer] number of replicas
    # @param topic_config [Hash] topic config details as described here:
    #   https://kafka.apache.org/documentation/#topicconfigs
    #
    # source://karafka//lib/karafka/admin.rb#106
    def create_topic(name, partitions, replication_factor, topic_config = T.unsafe(nil)); end

    # Removes given consumer group (if exists)
    #
    # @note This method should not be used on a running consumer group as it will not yield any
    #   results.
    # @param consumer_group_id [String] consumer group name
    #
    # source://karafka//lib/karafka/admin.rb#283
    def delete_consumer_group(consumer_group_id); end

    # Deleted a given topic
    #
    # @param name [String] topic name
    #
    # source://karafka//lib/karafka/admin.rb#120
    def delete_topic(name); end

    # Reads lags and offsets for given topics in the context of consumer groups defined in the
    #   routing
    #
    # @note For topics that do not exist, topic details will be set to an empty hash
    # @note For topics that exist but were never consumed by a given CG we set `-1` as lag and
    #   the offset on each of the partitions that were not consumed.
    # @note This lag reporting is for committed lags and is "Kafka-centric", meaning that this
    #   represents lags from Kafka perspective and not the consumer. They may differ.
    # @param consumer_groups_with_topics [Hash<String, Array<String>>] hash with consumer groups
    #   names with array of topics to query per consumer group inside
    # @param active_topics_only [Boolean] if set to false, when we use routing topics, will
    #   select also topics that are marked as inactive in routing
    # @return [Hash<String, Hash<Integer, <Hash<Integer>>>>] hash where the top level keys are
    #   the consumer groups and values are hashes with topics and inside partitions with lags
    #   and offsets
    #
    # source://karafka//lib/karafka/admin.rb#318
    def read_lags_with_offsets(consumer_groups_with_topics = T.unsafe(nil), active_topics_only: T.unsafe(nil)); end

    # Allows us to read messages from the topic
    #
    # @param name [String, Symbol] topic name
    # @param partition [Integer] partition
    # @param count [Integer] how many messages we want to get at most
    # @param start_offset [Integer, Time] offset from which we should start. If -1 is provided
    #   (default) we will start from the latest offset. If time is provided, the appropriate
    #   offset will be resolved. If negative beyond -1 is provided, we move backwards more.
    # @param settings [Hash] kafka extra settings (optional)
    # @return [Array<Karafka::Messages::Message>] array with messages
    #
    # source://karafka//lib/karafka/admin.rb#31
    def read_topic(name, partition, count, start_offset = T.unsafe(nil), settings = T.unsafe(nil)); end

    # Fetches the watermark offsets for a given topic partition
    #
    # @param name [String, Symbol] topic name
    # @param partition [Integer] partition
    # @return [Array<Integer, Integer>] low watermark offset and high watermark offset
    #
    # source://karafka//lib/karafka/admin.rb#295
    def read_watermark_offsets(name, partition); end

    # Moves the offset on a given consumer group and provided topic to the requested location
    #
    # @example Move a single topic partition nr 1 offset to 100
    #   Karafka::Admin.seek_consumer_group('group-id', { 'topic' => { 1 => 100 } })
    # @example Move offsets on all partitions of a topic to 100
    #   Karafka::Admin.seek_consumer_group('group-id', { 'topic' => 100 })
    # @example Move offset to 5 seconds ago on partition 2
    #   Karafka::Admin.seek_consumer_group('group-id', { 'topic' => { 2 => 5.seconds.ago } })
    # @example Move to the earliest offset on all the partitions of a topic
    #   Karafka::Admin.seek_consumer_group('group-id', { 'topic' => 'earliest' })
    # @example Move to the latest (high-watermark) offset on all the partitions of a topic
    #   Karafka::Admin.seek_consumer_group('group-id', { 'topic' => 'latest' })
    # @example Move offset of a single partition to earliest
    #   Karafka::Admin.seek_consumer_group('group-id', { 'topic' => { 1 => 'earliest' } })
    # @example Move offset of a single partition to latest
    #   Karafka::Admin.seek_consumer_group('group-id', { 'topic' => { 1 => 'latest' } })
    # @note This method should **not** be executed on a running consumer group as it creates a
    #   "fake" consumer and uses it to move offsets.
    # @param consumer_group_id [String] id of the consumer group for which we want to move the
    #   existing offset
    # @param topics_with_partitions_and_offsets [Hash] Hash with list of topics and settings to
    #   where to move given consumer. It allows us to move particular partitions or whole topics
    #   if we want to reset all partitions to for example a point in time.
    #
    # source://karafka//lib/karafka/admin.rb#177
    def seek_consumer_group(consumer_group_id, topics_with_partitions_and_offsets); end

    # Returns basic topic metadata
    #
    # @note This query is much more efficient than doing a full `#cluster_info` + topic lookup
    #   because it does not have to query for all the topics data but just the topic we're
    #   interested in
    # @param topic_name [String] name of the topic we're interested in
    # @raise [Rdkafka::RdkafkaError] `unknown_topic_or_part` if requested topic is not found
    # @return [Hash] topic metadata info hash
    #
    # source://karafka//lib/karafka/admin.rb#425
    def topic_info(topic_name); end

    # Creates admin instance and yields it. After usage it closes the admin instance
    #
    # source://karafka//lib/karafka/admin.rb#467
    def with_admin; end

    # Creates consumer instance and yields it. After usage it closes the consumer instance
    # This API can be used in other pieces of code and allows for low-level consumer usage
    #
    # @note We always ship and yield a proxied consumer because admin API performance is not
    #   that relevant. That is, there are no high frequency calls that would have to be delegated
    # @param settings [Hash] extra settings to customize consumer
    #
    # source://karafka//lib/karafka/admin.rb#441
    def with_consumer(settings = T.unsafe(nil)); end

    private

    # @return [Karafka::Core::Configurable::Node] root node config
    #
    # source://karafka//lib/karafka/admin.rb#582
    def app_config; end

    # Adds a new callback for given rdkafka instance for oauth token refresh (if needed)
    #
    # @param id [String, Symbol] unique (for the lifetime of instance) id that we use for
    #   callback referencing
    # @param instance [Rdkafka::Consumer, Rdkafka::Admin] rdkafka instance to be used to set
    #   appropriate oauth token when needed
    #
    # source://karafka//lib/karafka/admin.rb#490
    def bind_oauth(id, instance); end

    # @param type [Symbol] type of config we want
    # @param settings [Hash] extra settings for config (if needed)
    # @return [::Rdkafka::Config] rdkafka config
    #
    # source://karafka//lib/karafka/admin.rb#541
    def config(type, settings); end

    # Resolves the offset if offset is in a time format. Otherwise returns the offset without
    # resolving.
    #
    # @param consumer [::Rdkafka::Consumer]
    # @param name [String, Symbol] expected topic name
    # @param partition [Integer]
    # @param offset [Integer, Time]
    # @return [Integer] expected offset
    #
    # source://karafka//lib/karafka/admin.rb#562
    def resolve_offset(consumer, name, partition, offset); end

    # @return [Array<String>] topics names
    #
    # source://karafka//lib/karafka/admin.rb#508
    def topics_names; end

    # Removes the callback from no longer used instance
    #
    # @param id [String, Symbol] unique (for the lifetime of instance) id that we use for
    #   callback referencing
    #
    # source://karafka//lib/karafka/admin.rb#503
    def unbind_oauth(id); end

    # There are some cases where rdkafka admin operations finish successfully but without the
    # callback being triggered to materialize the post-promise object. Until this is fixed we
    # can figure out, that operation we wanted to do finished successfully by checking that the
    # effect of the command (new topic, more partitions, etc) is handled. Exactly for that we
    # use the breaker. It we get a timeout, we can check that what we wanted to achieve has
    # happened via the breaker check, hence we do not need to wait any longer.
    #
    # @param handler [Proc] the wait handler operation
    # @param breaker [Proc] extra condition upon timeout that indicates things were finished ok
    #
    # source://karafka//lib/karafka/admin.rb#521
    def with_re_wait(handler, breaker); end
  end
end

# Struct and set of operations for ACLs management that simplifies their usage.
# It allows to use Ruby symbol based definitions instead of usage of librdkafka types
# (it allows to use rdkafka numerical types as well out of the box)
#
# We map the numerical values because they are less descriptive and harder to follow.
#
# This API works based on ability to create a `Karafka:Admin::Acl` object that can be then used
# using `#create`, `#delete` and `#describe` class API.
#
# source://karafka//lib/karafka/admin/acl.rb#13
class Karafka::Admin::Acl
  # Initializes a new Acl instance with specified attributes.
  #
  # Each parameter is mapped to its corresponding value in the respective *_MAP constant,
  # allowing usage of more descriptive Ruby symbols instead of numerical types.
  #
  # @param resource_type [Symbol, Integer] Specifies the type of Kafka resource
  #   (like :topic, :consumer_group).
  #   Accepts either a symbol from RESOURCE_TYPES_MAP or a direct rdkafka numerical type.
  # @param resource_name [String, nil] The name of the Kafka resource
  #   (like a specific topic name). Can be nil for certain types of resource pattern types.
  # @param resource_pattern_type [Symbol, Integer] Determines how the ACL is applied to the
  #   resource. Uses a symbol from RESOURCE_PATTERNS_TYPE_MAP or a direct rdkafka numerical
  #   type.
  # @param principal [String, nil] Specifies the principal (user or client) for which the ACL
  #   is being defined. Can be nil if not applicable.
  # @param host [String] (default: '*') Defines the host from which the principal can access
  #   the resource. Defaults to '*' for all hosts.
  # @param operation [Symbol, Integer] Indicates the operation type allowed or denied by the
  #   ACL. Uses a symbol from OPERATIONS_MAP or a direct rdkafka numerical type.
  # @param permission_type [Symbol, Integer] Specifies whether to allow or deny the specified
  #   operation. Uses a symbol from PERMISSION_TYPES_MAP or a direct rdkafka numerical type.
  # @return [Acl] a new instance of Acl
  #
  # source://karafka//lib/karafka/admin/acl.rb#212
  def initialize(resource_type:, resource_name:, resource_pattern_type:, principal:, operation:, permission_type:, host: T.unsafe(nil)); end

  # Returns the value of attribute host.
  #
  # source://karafka//lib/karafka/admin/acl.rb#188
  def host; end

  # Returns the value of attribute operation.
  #
  # source://karafka//lib/karafka/admin/acl.rb#188
  def operation; end

  # Returns the value of attribute permission_type.
  #
  # source://karafka//lib/karafka/admin/acl.rb#188
  def permission_type; end

  # Returns the value of attribute principal.
  #
  # source://karafka//lib/karafka/admin/acl.rb#188
  def principal; end

  # Returns the value of attribute resource_name.
  #
  # source://karafka//lib/karafka/admin/acl.rb#188
  def resource_name; end

  # Returns the value of attribute resource_pattern_type.
  #
  # source://karafka//lib/karafka/admin/acl.rb#188
  def resource_pattern_type; end

  # Returns the value of attribute resource_type.
  #
  # source://karafka//lib/karafka/admin/acl.rb#188
  def resource_type; end

  # Converts the Acl into a hash with native rdkafka types
  #
  # @return [Hash] hash with attributes matching rdkafka numerical types
  #
  # source://karafka//lib/karafka/admin/acl.rb#233
  def to_native_hash; end

  private

  # Maps the provided attribute based on the mapping hash and if not found returns the
  # attribute itself. Useful when converting from Acl symbol based representation to the
  # rdkafka one.
  #
  # @param value [Symbol, Integer] The value to be mapped.
  # @param mappings [Hash] The hash containing the mapping data.
  # @return [Integer, Symbol] The mapped value or the original value if not found in mappings.
  #
  # source://karafka//lib/karafka/admin/acl.rb#254
  def map(value, mappings); end

  # Remaps the provided attribute based on the mapping hash and if not found returns the
  # attribute itself. Useful when converting from Acl symbol based representation to the
  # rdkafka one.
  #
  # @param value [Symbol, Integer] The value to be mapped.
  # @param mappings [Hash] The hash containing the mapping data.
  # @return [Integer, Symbol] The mapped value or the original value if not found in mappings.
  #
  # source://karafka//lib/karafka/admin/acl.rb#267
  def remap(value, mappings); end

  # Validates that the attribute exists in any of the ACL mappings.
  # Raises an error if the attribute is not supported.
  #
  # @param attribute [Symbol, Integer] The attribute to be validated.
  # @raise [Karafka::Errors::UnsupportedCaseError] raised if attribute not found
  #
  # source://karafka//lib/karafka/admin/acl.rb#277
  def validate_attribute!(attribute); end

  class << self
    # Returns all acls on a cluster level
    #
    # @return [Array<Acl>] all acls
    #
    # source://karafka//lib/karafka/admin/acl.rb#145
    def all; end

    # Creates (unless already present) a given ACL rule in Kafka
    #
    # @param acl [Acl]
    # @return [Array<Acl>] created acls
    #
    # source://karafka//lib/karafka/admin/acl.rb#108
    def create(acl); end

    # Removes acls matching provide acl pattern.
    #
    # @note More than one Acl may be removed if rules match that way
    # @param acl [Acl]
    # @return [Array<Acl>] deleted acls
    #
    # source://karafka//lib/karafka/admin/acl.rb#120
    def delete(acl); end

    # Takes an Acl definition and describes all existing Acls matching its criteria
    #
    # @param acl [Acl]
    # @return [Array<Acl>] described acls
    #
    # source://karafka//lib/karafka/admin/acl.rb#133
    def describe(acl); end

    private

    # Takes a rdkafka Acl result and converts it into our local Acl representation. Since the
    # rdkafka Acl object is an integer based on on types, etc we remap it into our "more" Ruby
    # form.
    #
    # return [Acl] mapped acl
    #
    # @param rdkafka_acl [Rdkafka::Admin::AclBindingResult]
    #
    # source://karafka//lib/karafka/admin/acl.rb#175
    def from_rdkafka(rdkafka_acl); end

    # Yields admin instance, allows to run Acl operations and awaits on the final result
    # Makes sure that admin is closed afterwards.
    #
    # source://karafka//lib/karafka/admin/acl.rb#163
    def with_admin_wait; end
  end
end

# Array with all maps used for the Acls support
#
# source://karafka//lib/karafka/admin/acl.rb#92
Karafka::Admin::Acl::ALL_MAPS = T.let(T.unsafe(nil), Array)

# ACL operations define the actions that can be performed on Kafka resources. Each operation
# represents a specific type of access or action that can be allowed or denied.
#
# source://karafka//lib/karafka/admin/acl.rb#51
Karafka::Admin::Acl::OPERATIONS_MAP = T.let(T.unsafe(nil), Hash)

# ACL permission types specify the nature of the access control applied to Kafka resources.
# These types are used to either grant or deny specified operations.
#
# source://karafka//lib/karafka/admin/acl.rb#82
Karafka::Admin::Acl::PERMISSION_TYPES_MAP = T.let(T.unsafe(nil), Hash)

# Resource pattern types define how ACLs (Access Control Lists) are applied to resources,
# specifying the scope and applicability of access rules.
# They determine whether an ACL should apply to a specific named resource, a prefixed group
# of resources, or all resources of a particular type.
#
# source://karafka//lib/karafka/admin/acl.rb#37
Karafka::Admin::Acl::RESOURCE_PATTERNS_TYPE_MAP = T.let(T.unsafe(nil), Hash)

# Types of resources for which we can assign permissions.
#
# Resource refers to any entity within the Kafka ecosystem for which access control can be
# managed using ACLs (Access Control Lists).
# These resources represent different components of Kafka, such as topics, consumer groups,
# and the Kafka cluster itself. ACLs can be applied to these resources to control and
# restrict reading, writing, and administrative operations, ensuring secure and authorized
# access to Kafka's functionalities.
#
# source://karafka//lib/karafka/admin/acl.rb#22
Karafka::Admin::Acl::RESOURCE_TYPES_MAP = T.let(T.unsafe(nil), Hash)

# Namespace for admin operations related to configuration management
#
# At the moment Karafka supports configuration management for brokers and topics
#
# You can describe configuration as well as alter it.
#
# Altering is done in the incremental way.
#
# source://karafka//lib/karafka/admin/configs.rb#12
module Karafka::Admin::Configs
  class << self
    # Alters given resources based on the alteration operations accumulated in the provided
    # resources
    #
    # @example Alter the `delete.retention.ms` and set it to 8640001
    #   resource = Karafka::Admin::Configs::Resource.new(type: :topic, name: 'example')
    #   resource.set('delete.retention.ms', '8640001')
    #   Karafka::Admin::Configs.alter(resource)
    # @note This operation is not transactional and can work only partially if some config
    #   options are not valid. Always make sure, your alterations are correct.
    # @note We call it `#alter` despite using the Kafka incremental alter API because the
    #   regular alter is deprecated.
    # @param resources [Resource, Array<Resource>] single resource we want to alter or
    #   list of resources.
    #
    # source://karafka//lib/karafka/admin/configs.rb#54
    def alter(*resources); end

    # Fetches given resources configurations from Kafka
    #
    # @example Describe topic named "example" and print its config
    #   resource = Karafka::Admin::Configs::Resource.new(type: :topic, name: 'example')
    #   results = Karafka::Admin::Configs.describe(resource)
    #   results.first.configs.each do |config|
    #   puts "#{config.name} - #{config.value}"
    #   end
    # @note Even if you request one resource, result will always be an array with resources
    # @param resources [Resource, Array<Resource>] single resource we want to describe or
    #   list of resources we are interested in. It is useful to provide multiple resources
    #   when you need data from multiple topics, etc. Karafka will make one query for all the
    #   data instead of doing one per topic.
    # @return [Array<Resource>] array with resources containing their configuration details
    #
    # source://karafka//lib/karafka/admin/configs.rb#31
    def describe(*resources); end

    private

    # @param action [Symbol] runs given action via Rdkafka Admin
    # @param resources [Array<Resource>] resources on which we want to operate
    #
    # source://karafka//lib/karafka/admin/configs.rb#65
    def operate_on_resources(action, resources); end

    # Yields admin instance, allows to run Acl operations and awaits on the final result
    # Makes sure that admin is closed afterwards.
    #
    # source://karafka//lib/karafka/admin/configs.rb#95
    def with_admin_wait; end
  end
end

# Represents a single config entry that is related to a resource
#
# source://karafka//lib/karafka/admin/configs/config.rb#7
class Karafka::Admin::Configs::Config
  # Creates new config instance either for reading or as part of altering operation
  #
  # @note For alter operations only `name` and `value` are needed
  # @param name [String] config name
  # @param value [String] config value
  # @param default [Integer] 1 if default
  # @param read_only [Integer] 1 if read only
  # @param sensitive [Integer] 1 if sensitive
  # @param synonym [Integer] 1 if synonym
  # @param synonyms [Array] given config synonyms (if any)
  # @return [Config] a new instance of Config
  #
  # source://karafka//lib/karafka/admin/configs/config.rb#41
  def initialize(name:, value:, default: T.unsafe(nil), read_only: T.unsafe(nil), sensitive: T.unsafe(nil), synonym: T.unsafe(nil), synonyms: T.unsafe(nil)); end

  # @return [Boolean] Is the config property is set to its default value on the broker
  #
  # source://karafka//lib/karafka/admin/configs/config.rb#61
  def default?; end

  # Returns the value of attribute name.
  #
  # source://karafka//lib/karafka/admin/configs/config.rb#8
  def name; end

  # @return [Boolean] Is the config property is read-only on the broker
  #
  # source://karafka//lib/karafka/admin/configs/config.rb#64
  def read_only?; end

  # @return [Boolean] if the config property contains sensitive information (such as
  #   security configuration
  #
  # source://karafka//lib/karafka/admin/configs/config.rb#68
  def sensitive?; end

  # @return [Boolean] is this entry is a synonym
  #
  # source://karafka//lib/karafka/admin/configs/config.rb#71
  def synonym?; end

  # Returns the value of attribute synonyms.
  #
  # source://karafka//lib/karafka/admin/configs/config.rb#8
  def synonyms; end

  # @return [Hash] hash that we can use to operate with rdkafka
  #
  # source://karafka//lib/karafka/admin/configs/config.rb#74
  def to_native_hash; end

  # Returns the value of attribute value.
  #
  # source://karafka//lib/karafka/admin/configs/config.rb#8
  def value; end

  class << self
    # Creates a single config entry from the Rdkafka config result entry
    #
    # @param rd_kafka_config [Rdkafka::Admin::ConfigBindingResult]
    # @return [Config]
    #
    # source://karafka//lib/karafka/admin/configs/config.rb#15
    def from_rd_kafka(rd_kafka_config); end
  end
end

# Represents a single resource in the context of configuration management
#
# source://karafka//lib/karafka/admin/configs/resource.rb#7
class Karafka::Admin::Configs::Resource
  # @param type [Symbol, Integer] type of resource as a symbol for mapping or integer
  # @param name [String] name of the resource. It's the broker id or topic name
  # @return [Resource]
  #
  # source://karafka//lib/karafka/admin/configs/resource.rb#31
  def initialize(type:, name:); end

  # source://karafka//lib/karafka/admin/configs/resource.rb#46
  def append(name, value); end

  # Returns the value of attribute configs.
  #
  # source://karafka//lib/karafka/admin/configs/resource.rb#26
  def configs; end

  # source://karafka//lib/karafka/admin/configs/resource.rb#46
  def delete(name, value = T.unsafe(nil)); end

  # Returns the value of attribute name.
  #
  # source://karafka//lib/karafka/admin/configs/resource.rb#26
  def name; end

  # source://karafka//lib/karafka/admin/configs/resource.rb#46
  def set(name, value); end

  # source://karafka//lib/karafka/admin/configs/resource.rb#46
  def subtract(name, value); end

  # @note Configs include the operation type and are expected to be used only for the
  #   incremental alter API.
  # @return [Hash] resource converted to a hash that rdkafka can work with
  #
  # source://karafka//lib/karafka/admin/configs/resource.rb#55
  def to_native_hash; end

  # Returns the value of attribute type.
  #
  # source://karafka//lib/karafka/admin/configs/resource.rb#26
  def type; end

  private

  # Recognizes whether the type is provided and remaps it to a symbol representation if
  # needed
  #
  # @param type [Symbol, Integer]
  # @return [Symbol]
  #
  # source://karafka//lib/karafka/admin/configs/resource.rb#78
  def map_type(type); end
end

# Map for operations we may perform on the resource configs
#
# source://karafka//lib/karafka/admin/configs/resource.rb#17
Karafka::Admin::Configs::Resource::OPERATIONS_TYPES_MAP = T.let(T.unsafe(nil), Hash)

# Types of resources that have workable configs.
#
# source://karafka//lib/karafka/admin/configs/resource.rb#9
Karafka::Admin::Configs::Resource::RESOURCE_TYPES_MAP = T.let(T.unsafe(nil), Hash)

# More or less number of seconds of 1 hundred years
# Used for time referencing that does not have to be accurate but needs to be big
#
# source://karafka//lib/karafka/admin.rb#15
Karafka::Admin::HUNDRED_YEARS = T.let(T.unsafe(nil), Float)

# App class
#
# source://karafka//lib/karafka/app.rb#5
class Karafka::App
  extend ::Karafka::Setup::Dsl

  class << self
    # Returns current assignments of this process. Both topics and partitions
    #
    # @return [Hash<Karafka::Routing::Topic, Array<Integer>>]
    #
    # source://karafka//lib/karafka/app.rb#56
    def assignments; end

    # @return [Karafka::Routing::Builder] consumers builder instance alias
    #
    # source://karafka//lib/karafka/app.rb#24
    def consumer_groups; end

    # @note It is a meta status from the status object
    # @return [Boolean] true if we should be done in general with processing anything
    #
    # source://karafka//lib/karafka/app.rb#79
    def done?; end

    # source://karafka//lib/karafka/app.rb#93
    def env; end

    # source://karafka//lib/karafka/app.rb#71
    def initialize!; end

    # source://karafka//lib/karafka/app.rb#63
    def initialized; end

    # source://karafka//lib/karafka/app.rb#71
    def initialized!; end

    # source://karafka//lib/karafka/app.rb#67
    def initialized?; end

    # source://karafka//lib/karafka/app.rb#63
    def initializing; end

    # source://karafka//lib/karafka/app.rb#67
    def initializing?; end

    # source://karafka//lib/karafka/app.rb#93
    def logger; end

    # source://karafka//lib/karafka/app.rb#93
    def monitor; end

    # source://karafka//lib/karafka/app.rb#93
    def pro?; end

    # source://karafka//lib/karafka/app.rb#93
    def producer; end

    # source://karafka//lib/karafka/app.rb#63
    def quiet; end

    # source://karafka//lib/karafka/app.rb#71
    def quiet!; end

    # source://karafka//lib/karafka/app.rb#67
    def quiet?; end

    # source://karafka//lib/karafka/app.rb#71
    def quieted!; end

    # source://karafka//lib/karafka/app.rb#63
    def quieting; end

    # source://karafka//lib/karafka/app.rb#67
    def quieting?; end

    # source://karafka//lib/karafka/app.rb#93
    def root; end

    # Just a nicer name for the consumer groups
    #
    # @return [Karafka::Routing::Builder] consumers builder instance alias
    #
    # source://karafka//lib/karafka/app.rb#24
    def routes; end

    # source://karafka//lib/karafka/app.rb#71
    def run!; end

    # source://karafka//lib/karafka/app.rb#63
    def running; end

    # source://karafka//lib/karafka/app.rb#67
    def running?; end

    # source://karafka//lib/karafka/app.rb#71
    def stop!; end

    # source://karafka//lib/karafka/app.rb#63
    def stopped; end

    # source://karafka//lib/karafka/app.rb#71
    def stopped!; end

    # source://karafka//lib/karafka/app.rb#67
    def stopped?; end

    # source://karafka//lib/karafka/app.rb#63
    def stopping; end

    # source://karafka//lib/karafka/app.rb#67
    def stopping?; end

    # @return [Hash] active subscription groups grouped based on consumer group in a hash
    #
    # source://karafka//lib/karafka/app.rb#32
    def subscription_groups; end

    # source://karafka//lib/karafka/app.rb#71
    def supervise!; end

    # source://karafka//lib/karafka/app.rb#63
    def supervising; end

    # source://karafka//lib/karafka/app.rb#67
    def supervising?; end

    # source://karafka//lib/karafka/app.rb#71
    def terminate!; end

    # source://karafka//lib/karafka/app.rb#63
    def terminated; end

    # source://karafka//lib/karafka/app.rb#67
    def terminated?; end

    # Notifies the Ruby virtual machine that the boot sequence is finished, and that now is a
    # good time to optimize the application. In case of older Ruby versions, runs compacting,
    # which is part of the full warmup introduced in Ruby 3.3.
    #
    # source://karafka//lib/karafka/app.rb#12
    def warmup; end
  end
end

# Base consumer from which all Karafka consumers should inherit
#
# source://karafka//lib/karafka/base_consumer.rb#6
class Karafka::BaseConsumer
  include ::Karafka::Core::Taggable
  extend ::Forwardable

  # Creates new consumer and assigns it an id
  #
  # @return [BaseConsumer] a new instance of BaseConsumer
  #
  # source://karafka//lib/karafka/base_consumer.rb#29
  def initialize; end

  # @return [Karafka::Connection::Client] kafka connection client
  #
  # source://karafka//lib/karafka/base_consumer.rb#22
  def client; end

  # @return [Karafka::Connection::Client] kafka connection client
  #
  # source://karafka//lib/karafka/base_consumer.rb#22
  def client=(_arg0); end

  # @return [Karafka::Processing::Coordinator] coordinator
  #
  # source://karafka//lib/karafka/base_consumer.rb#24
  def coordinator; end

  # @return [Karafka::Processing::Coordinator] coordinator
  #
  # source://karafka//lib/karafka/base_consumer.rb#24
  def coordinator=(_arg0); end

  # @return [String] id of the current consumer
  #
  # source://karafka//lib/karafka/base_consumer.rb#18
  def id; end

  # @return [Karafka::Routing::Topic] topic to which a given consumer is subscribed
  #
  # source://karafka//lib/karafka/base_consumer.rb#20
  def messages; end

  # @return [Karafka::Routing::Topic] topic to which a given consumer is subscribed
  #
  # source://karafka//lib/karafka/base_consumer.rb#20
  def messages=(_arg0); end

  # @note This should not be used by the end users as it is part of the lifecycle of things but
  #   not as part of the public api.
  # @note We handle and report errors here because of flows that could fail. For example a DLQ
  #   flow could fail if it was not able to dispatch the DLQ message. Other "non-user" based
  #   flows do not interact with external systems and their errors are expected to bubble up
  # @private
  #
  # source://karafka//lib/karafka/base_consumer.rb#89
  def on_after_consume; end

  # Can be used to run preparation code in the worker
  #
  # @note This should not be used by the end users as it is part of the lifecycle of things and
  #   not as part of the public api. This can act as a hook when creating non-blocking
  #   consumers and doing other advanced stuff
  # @private
  #
  # source://karafka//lib/karafka/base_consumer.rb#51
  def on_before_consume; end

  # Can be used to run preparation code prior to the job being enqueued
  #
  # @note This should not be used by the end users as it is part of the lifecycle of things and
  #   not as a part of the public api. This should not perform any extensive operations as it is
  #   blocking and running in the listener thread.
  # @private
  #
  # source://karafka//lib/karafka/base_consumer.rb#40
  def on_before_schedule_consume; end

  # Can be used to run code prior to scheduling of idle execution
  #
  # @private
  #
  # source://karafka//lib/karafka/base_consumer.rb#106
  def on_before_schedule_idle; end

  # Can be used to run code prior to scheduling of revoked execution
  #
  # @private
  #
  # source://karafka//lib/karafka/base_consumer.rb#120
  def on_before_schedule_revoked; end

  # Can be used to run code prior to scheduling of revoked execution
  #
  # @private
  #
  # source://karafka//lib/karafka/base_consumer.rb#141
  def on_before_schedule_shutdown; end

  # Executes the default consumer flow.
  #
  # @note We keep the seek offset tracking, and use it to compensate for async offset flushing
  #   that may not yet kick in when error occurs. That way we pause always on the last processed
  #   message.
  # @private
  # @return [Boolean] true if there was no exception, otherwise false.
  #
  # source://karafka//lib/karafka/base_consumer.rb#69
  def on_consume; end

  # Trigger method for running on idle runs without messages
  #
  # @private
  #
  # source://karafka//lib/karafka/base_consumer.rb#113
  def on_idle; end

  # Trigger method for running on partition revocation.
  #
  # @private
  #
  # source://karafka//lib/karafka/base_consumer.rb#127
  def on_revoked; end

  # Trigger method for running on shutdown.
  #
  # @private
  #
  # source://karafka//lib/karafka/base_consumer.rb#148
  def on_shutdown; end

  # source://forwardable/1.3.3/forwardable.rb#231
  def partition(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def produce_async(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def produce_many_async(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def produce_many_sync(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def produce_sync(*args, **_arg1, &block); end

  # @return [Waterdrop::Producer] producer instance
  #
  # source://karafka//lib/karafka/base_consumer.rb#26
  def producer; end

  # @return [Waterdrop::Producer] producer instance
  #
  # source://karafka//lib/karafka/base_consumer.rb#26
  def producer=(_arg0); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def topic(*args, **_arg1, &block); end

  private

  # @return [Integer] attempt of processing given batch. 1 if this is the first attempt or higher
  #   in case it is a retry
  #
  # source://karafka//lib/karafka/base_consumer.rb#282
  def attempt; end

  # Method that will perform business logic and on data received from Kafka (it will consume
  #   the data)
  #
  # @note This method needs to be implemented in a subclass. We stub it here as a failover if
  #   someone forgets about it or makes on with typo
  # @raise [NotImplementedError]
  #
  # source://karafka//lib/karafka/base_consumer.rb#165
  def consume; end

  # Pauses processing on a given offset or consecutive offset for the current topic partition
  #
  # After given partition is resumed, it will continue processing from the given offset
  #
  # @note It is **critical** to understand how pause with `:consecutive` offset operates. While
  #   it provides benefit of not purging librdkafka buffer, in case of usage of filters, retries
  #   or other advanced options the consecutive offset may not be the one you want to pause on.
  #   Test it well to ensure, that this behaviour is expected by you.
  # @param offset [Integer, Symbol] offset from which we want to restart the processing or
  #   `:consecutive` if we want to pause and continue without changing the consecutive offset
  #   (cursor position)
  # @param timeout [Integer, nil] how long in milliseconds do we want to pause or nil to use the
  #   default exponential pausing strategy defined for retries
  # @param manual_pause [Boolean] Flag to differentiate between user pause and system/strategy
  #   based pause. While they both pause in exactly the same way, the strategy application
  #   may need to differentiate between them.
  #
  # source://karafka//lib/karafka/base_consumer.rb#201
  def pause(offset, timeout = T.unsafe(nil), manual_pause = T.unsafe(nil)); end

  # Resumes processing of the current topic partition
  #
  # source://karafka//lib/karafka/base_consumer.rb#225
  def resume; end

  # Pauses the processing from the last offset to retry on given message
  #
  # @private
  #
  # source://karafka//lib/karafka/base_consumer.rb#288
  def retry_after_pause; end

  # @return [Boolean] are we retrying processing after an error. This can be used to provide a
  #   different flow after there is an error, for example for resources cleanup, small manual
  #   backoff or different instrumentation tracking.
  #
  # source://karafka//lib/karafka/base_consumer.rb#276
  def retrying?; end

  # Method that will be executed when a given topic partition is revoked. You can use it for
  # some teardown procedures (closing file handler, etc).
  #
  # source://karafka//lib/karafka/base_consumer.rb#171
  def revoked; end

  # @note There are two "levels" on which we can know that partition was revoked. First one is
  #   when we loose the assignment involuntarily and second is when coordinator gets this info
  #   after we poll with the rebalance callbacks. The first check allows us to get this notion
  #   even before we poll but it gets reset when polling happens, hence we also need to switch
  #   the coordinator state after the revocation (but prior to running more jobs)
  # @return [Boolean] true if partition was revoked from the current consumer
  #
  # source://karafka//lib/karafka/base_consumer.rb#264
  def revoked?; end

  # Seeks in the context of current topic and partition
  #
  # @note Please note, that if you are seeking to a time offset, getting the offset is blocking
  # @param offset [Integer, Time] offset where we want to seek or time of the offset where we
  #   want to seek.
  # @param manual_seek [Boolean] Flag to differentiate between user seek and system/strategy
  #   based seek. User seek operations should take precedence over system actions, hence we need
  #   to know who invoked it.
  # @param reset_offset [Boolean] should we reset offset when seeking backwards. It is false by
  #   default to prevent marking in the offset that was earlier than the highest marked offset
  #   for given consumer group. It can be set to true if we want to reprocess data once again and
  #   want to make sure that the marking starts from where we moved to.
  #
  # source://karafka//lib/karafka/base_consumer.rb#245
  def seek(offset, manual_seek = T.unsafe(nil), reset_offset: T.unsafe(nil)); end

  # Method that will be executed when the process is shutting down. You can use it for
  # some teardown procedures (closing file handler, etc).
  #
  # source://karafka//lib/karafka/base_consumer.rb#175
  def shutdown; end

  # @return [Boolean] was this consumer in active use. Active use means running `#consume` at
  #   least once. Consumer may have to run `#revoked` or `#shutdown` despite not running
  #   `#consume` previously in delayed job cases and other cases that potentially involve running
  #   the `Jobs::Idle` for house-keeping
  #
  # source://karafka//lib/karafka/base_consumer.rb#181
  def used?; end
end

# Karafka framework Cli
#
# If you want to add/modify command that belongs to CLI, please review all commands
# available in cli/ directory inside Karafka source code.
#
# source://karafka//lib/karafka/cli.rb#8
class Karafka::Cli
  class << self
    # Starts the CLI
    #
    # source://karafka//lib/karafka/cli.rb#11
    def start; end

    private

    # @return [Array<Class>] command classes
    #
    # source://karafka//lib/karafka/cli.rb#35
    def commands; end
  end
end

# Base class for all the command that we want to define
# This base class provides an interface to easier separate single independent commands
#
# source://karafka//lib/karafka/cli/base.rb#7
class Karafka::Cli::Base
  include ::Karafka::Helpers::Colorize

  # Creates new CLI command instance
  #
  # @return [Base] a new instance of Base
  #
  # source://karafka//lib/karafka/cli/base.rb#14
  def initialize; end

  # This method should implement proper cli action
  #
  # @raise [NotImplementedError]
  #
  # source://karafka//lib/karafka/cli/base.rb#20
  def call; end

  # @return [Hash] given command cli options
  #
  # source://karafka//lib/karafka/cli/base.rb#11
  def options; end

  private

  # Prints marketing info
  #
  # source://karafka//lib/karafka/cli/base.rb#27
  def print_marketing_info; end

  class << self
    # Allows to set aliases for a given cli command
    #
    # @param args [Array] list of aliases that we can use to run given cli command
    #
    # source://karafka//lib/karafka/cli/base.rb#78
    def aliases(*args); end

    # @return [Array<Class>] available commands
    #
    # source://karafka//lib/karafka/cli/base.rb#105
    def commands; end

    # Allows to set description of a given cli command
    #
    # @param desc [String] Description of a given cli command
    #
    # source://karafka//lib/karafka/cli/base.rb#72
    def desc(desc = T.unsafe(nil)); end

    # Loads proper environment with what is needed to run the CLI
    #
    # source://karafka//lib/karafka/cli/base.rb#43
    def load; end

    # @example for Karafka::Cli::Install
    #   name #=> 'install'
    # @return [String] downcased current class name that we use to define name for
    #   given Cli command
    #
    # source://karafka//lib/karafka/cli/base.rb#117
    def name; end

    # @return [Array<String>] names and aliases for command matching
    #
    # source://karafka//lib/karafka/cli/base.rb#122
    def names; end

    # Allows to set options for Thor cli
    #
    # @param option Single option details
    # @see https://github.com/erikhuda/thor
    #
    # source://karafka//lib/karafka/cli/base.rb#65
    def option(*option); end

    # Parses the CLI options
    #
    # @return [Hash] hash with parsed values
    #
    # source://karafka//lib/karafka/cli/base.rb#85
    def parse_options; end
  end
end

# Console Karafka Cli action
#
# source://karafka//lib/karafka/cli/console.rb#7
class Karafka::Cli::Console < ::Karafka::Cli::Base
  # Start the Karafka console
  #
  # source://karafka//lib/karafka/cli/console.rb#28
  def call; end

  class << self
    # @example
    #   Karafka::Cli::Console.command #=> 'KARAFKA_CONSOLE=true bundle exec irb...'
    # @return [String] Console executing command for non-Rails setup
    #
    # source://karafka//lib/karafka/cli/console.rb#16
    def console; end

    # @note In case of Rails, it has its own console, hence we can just defer to it
    # @return [String] Console executing command for Rails setup
    #
    # source://karafka//lib/karafka/cli/console.rb#22
    def rails_console; end
  end
end

# Prints info with list of commands available
#
# source://karafka//lib/karafka/cli/help.rb#7
class Karafka::Cli::Help < ::Karafka::Cli::Base
  # Print available commands
  #
  # source://karafka//lib/karafka/cli/help.rb#11
  def call; end
end

# Info Karafka Cli action
#
# source://karafka//lib/karafka/cli/info.rb#7
class Karafka::Cli::Info < ::Karafka::Cli::Base
  # Print configuration details and other options of your application
  #
  # source://karafka//lib/karafka/cli/info.rb#29
  def call; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def client_id; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def concurrency; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def license; end

  private

  # @return [Array<String>] core framework related info
  #
  # source://karafka//lib/karafka/cli/info.rb#37
  def core_info; end

  # @return [Array<String>] license related info
  #
  # source://karafka//lib/karafka/cli/info.rb#54
  def license_info; end
end

# Nice karafka banner
#
# source://karafka//lib/karafka/cli/info.rb#17
Karafka::Cli::Info::BANNER = T.let(T.unsafe(nil), String)

# Install Karafka Cli action
#
# source://karafka//lib/karafka/cli/install.rb#9
class Karafka::Cli::Install < ::Karafka::Cli::Base
  # @return [Install] a new instance of Install
  #
  # source://karafka//lib/karafka/cli/install.rb#29
  def initialize; end

  # Install all required things for Karafka application in current directory
  #
  # source://karafka//lib/karafka/cli/install.rb#42
  def call; end

  # This allows us to generate customized karafka.rb template with some tweaks specific for
  # Rails
  #
  # @return [Boolean] true if we have Rails loaded
  #
  # source://karafka//lib/karafka/cli/install.rb#71
  def rails?; end
end

# Directories created by default
#
# source://karafka//lib/karafka/cli/install.rb#15
Karafka::Cli::Install::INSTALL_DIRS = T.let(T.unsafe(nil), Array)

# Where should we map proper files from templates
#
# source://karafka//lib/karafka/cli/install.rb#23
Karafka::Cli::Install::INSTALL_FILES_MAP = T.let(T.unsafe(nil), Hash)

# Server Karafka Cli action
#
# source://karafka//lib/karafka/cli/server.rb#7
class Karafka::Cli::Server < ::Karafka::Cli::Base
  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def activity_manager; end

  # Start the Karafka server
  #
  # source://karafka//lib/karafka/cli/server.rb#85
  def call; end

  # Registers things we want to exclude (if defined)
  #
  # source://karafka//lib/karafka/cli/server.rb#105
  def register_exclusions; end

  # Registers things we want to include (if defined)
  #
  # source://karafka//lib/karafka/cli/server.rb#96
  def register_inclusions; end
end

# Those options can also be used when in swarm mode, hence we re-use
#
# source://karafka//lib/karafka/cli/server.rb#22
Karafka::Cli::Server::OPTIONS_BUILDER = T.let(T.unsafe(nil), Proc)

# Types of things we can include / exclude from the routing via the CLI options
#
# source://karafka//lib/karafka/cli/server.rb#13
Karafka::Cli::Server::SUPPORTED_TYPES = T.let(T.unsafe(nil), Array)

# Starts swarm of consumers forked from the supervisor
#
# source://karafka//lib/karafka/cli/swarm.rb#7
class Karafka::Cli::Swarm < ::Karafka::Cli::Base
  # Starts the swarm
  #
  # source://karafka//lib/karafka/cli/swarm.rb#15
  def call; end
end

# CLI actions related to Kafka cluster topics management
#
# source://karafka//lib/karafka/cli/topics.rb#6
class Karafka::Cli::Topics < ::Karafka::Cli::Base
  # @param action [String] action we want to take
  #
  # source://karafka//lib/karafka/cli/topics.rb#14
  def call(action = T.unsafe(nil)); end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def kafka_config; end
end

# Aligns configuration of all the declarative topics that exist based on the declarative
# topics definitions.
#
# Takes into consideration already existing settings, so will only align what is needed.
#
# Keep in mind, this is NOT transactional. Kafka topic changes are not transactional so
# it is highly recommended to test it before running in prod.
#
# @note This command does NOT repartition and does NOT create new topics. It only aligns
#   configuration of existing topics.
#
# source://karafka//lib/karafka/cli/topics/align.rb#16
class Karafka::Cli::Topics::Align < ::Karafka::Cli::Topics::Base
  # @return [Boolean] true if there were any changes applied, otherwise false
  #
  # source://karafka//lib/karafka/cli/topics/align.rb#18
  def call; end

  private

  # Iterates over configs of all the candidate topics and prepares alignment resources for
  # a single request to Kafka
  #
  # @return [Array<Karafka::Admin::Configs::Resource>] all topics with config change requests
  #
  # source://karafka//lib/karafka/cli/topics/align.rb#67
  def build_resources_to_migrate; end

  # Selects topics that exist and potentially may have config to align
  #
  # @return [Set<Karafka::Routing::Topic>]
  #
  # source://karafka//lib/karafka/cli/topics/align.rb#46
  def candidate_topics; end
end

# Base class for all the topics related operations
#
# source://karafka//lib/karafka/cli/topics/base.rb#7
class Karafka::Cli::Topics::Base
  include ::Karafka::Helpers::Colorize

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def kafka_config; end

  private

  # @note If topic is defined in multiple consumer groups, first config will be used. This
  #   means, that this CLI will not work for simultaneous management of multiple clusters
  #   from a single CLI command execution flow.
  # @return [Array<Karafka::Routing::Topic>] all available topics that can be managed
  #
  # source://karafka//lib/karafka/cli/topics/base.rb#19
  def declaratives_routing_topics; end

  # @return [Array<Hash>] existing topics details
  #
  # source://karafka//lib/karafka/cli/topics/base.rb#40
  def existing_topics; end

  # @return [Array<String>] names of already existing topics
  #
  # source://karafka//lib/karafka/cli/topics/base.rb#45
  def existing_topics_names; end

  # Waits with a message, that we are waiting on topics
  # This is not doing much, just waiting as there are some cases that it takes a bit of time
  # for Kafka to actually propagate new topics knowledge across the cluster. We give it that
  # bit of time just in case.
  #
  # source://karafka//lib/karafka/cli/topics/base.rb#53
  def wait; end
end

# Creates topics based on the routing setup and configuration
#
# source://karafka//lib/karafka/cli/topics/create.rb#7
class Karafka::Cli::Topics::Create < ::Karafka::Cli::Topics::Base
  # @return [Boolean] true if any topic was created, otherwise false
  #
  # source://karafka//lib/karafka/cli/topics/create.rb#9
  def call; end
end

# Deletes routing based topics
#
# source://karafka//lib/karafka/cli/topics/delete.rb#7
class Karafka::Cli::Topics::Delete < ::Karafka::Cli::Topics::Base
  # @return [Boolean] true if any topic was deleted, otherwise false
  #
  # source://karafka//lib/karafka/cli/topics/delete.rb#9
  def call; end
end

# Creates missing topics and aligns the partitions count
#
# source://karafka//lib/karafka/cli/topics/migrate.rb#7
class Karafka::Cli::Topics::Migrate < ::Karafka::Cli::Topics::Base
  # Runs the migration
  #
  # @return [Boolean] true if there were any changes applied
  #
  # source://karafka//lib/karafka/cli/topics/migrate.rb#10
  def call; end
end

# Plans the migration process and prints what changes are going to be applied if migration
# would to be executed
#
# source://karafka//lib/karafka/cli/topics/plan.rb#8
class Karafka::Cli::Topics::Plan < ::Karafka::Cli::Topics::Base
  # Figures out scope of changes that need to happen
  #
  # @return [Boolean] true if running migrate would change anything, false otherwise
  #
  # source://karafka//lib/karafka/cli/topics/plan.rb#11
  def call; end

  private

  # @return [Hash] Hash where keys are topics to alter and values are configs that will
  #   be altered.
  #
  # source://karafka//lib/karafka/cli/topics/plan.rb#110
  def topics_to_alter; end

  # @return [Array<Karafka::Routing::Topic>] topics that will be created
  #
  # source://karafka//lib/karafka/cli/topics/plan.rb#74
  def topics_to_create; end

  # @return [Array<Array<Karafka::Routing::Topic, Integer>>] array with topics that will
  #   be repartitioned and current number of partitions
  #
  # source://karafka//lib/karafka/cli/topics/plan.rb#86
  def topics_to_repartition; end
end

# Increases number of partitions on topics that have less partitions than defined
# Will **not** create topics if missing.
#
# source://karafka//lib/karafka/cli/topics/repartition.rb#8
class Karafka::Cli::Topics::Repartition < ::Karafka::Cli::Topics::Base
  # @return [Boolean] true if anything was repartitioned, otherwise false
  #
  # source://karafka//lib/karafka/cli/topics/repartition.rb#10
  def call; end
end

# Deletes routing based topics and re-creates them
#
# source://karafka//lib/karafka/cli/topics/reset.rb#7
class Karafka::Cli::Topics::Reset < ::Karafka::Cli::Topics::Base
  # @return [true] since it is a reset, always changes so `true` always
  #
  # source://karafka//lib/karafka/cli/topics/reset.rb#9
  def call; end
end

# Namespace for Kafka connection related logic
#
# source://karafka//lib/karafka.rb#0
module Karafka::Connection; end

# An abstraction layer on top of the rdkafka consumer.
#
# It is threadsafe and provides some security measures so we won't end up operating on a
# closed consumer instance as it causes Ruby VM process to crash.
#
# source://karafka//lib/karafka/connection/client.rb#10
class Karafka::Connection::Client
  # Creates a new consumer instance.
  #
  # @param subscription_group [Karafka::Routing::SubscriptionGroup] subscription group
  #   with all the configuration details needed for us to create a client
  # @param batch_poll_breaker [Proc] proc that when evaluated to false will cause the batch
  #   poll loop to finish early. This improves the shutdown and dynamic multiplication as it
  #   allows us to early break on long polls.
  # @return [Karafka::Connection::Client]
  #
  # source://karafka//lib/karafka/connection/client.rb#37
  def initialize(subscription_group, batch_poll_breaker); end

  # @return [Rdkafka::Consumer::TopicPartitionList] current active assignment
  #
  # source://karafka//lib/karafka/connection/client.rb#141
  def assignment; end

  # @return [Boolean] true if our current assignment has been lost involuntarily.
  #
  # source://karafka//lib/karafka/connection/client.rb#136
  def assignment_lost?; end

  # Fetches messages within boundaries defined by the settings (time, size, topics, etc).
  #
  # Also periodically runs the events polling to trigger events callbacks.
  #
  # @note This method should not be executed from many threads at the same time
  # @return [Karafka::Connection::MessagesBuffer] messages buffer that holds messages per topic
  #   partition
  #
  # source://karafka//lib/karafka/connection/client.rb#75
  def batch_poll; end

  # Commits the offset on a current consumer in a non-blocking or blocking way.
  #
  # @note This will commit all the offsets for the whole consumer. In order to achieve
  #   granular control over where the offset should be for particular topic partitions, the
  #   store_offset should be used to only store new offset when we want them to be flushed
  # @note This method for async may return `true` despite involuntary partition revocation as
  #   it does **not** resolve to `lost_assignment?`. It returns only the commit state operation
  #   result.
  # @param async [Boolean] should the commit happen async or sync (async by default)
  # @return [Boolean] did committing was successful. It may be not, when we no longer own
  #   given partition.
  #
  # source://karafka//lib/karafka/connection/client.rb#158
  def commit_offsets(async: T.unsafe(nil)); end

  # Commits offset in a synchronous way.
  #
  # @see `#commit_offset` for more details
  #
  # source://karafka//lib/karafka/connection/client.rb#165
  def commit_offsets!; end

  # Return the current committed offset per partition for this consumer group.
  # The offset field of each requested partition will either be set to stored offset or to
  # -1001 in case there was no stored offset for that partition.
  #
  # @note It is recommended to use this only on rebalances to get positions with metadata
  #   when working with metadata as this is synchronous
  # @param tpl [Rdkafka::Consumer::TopicPartitionList] for which we want to get committed
  # @raise [Rdkafka::RdkafkaError] When getting the committed positions fails.
  # @return [Rdkafka::Consumer::TopicPartitionList]
  #
  # source://karafka//lib/karafka/connection/client.rb#357
  def committed(tpl = T.unsafe(nil)); end

  # Returns pointer to the consumer group metadata. It is used only in the context of
  # exactly-once-semantics in transactions, this is why it is never remapped to Ruby
  #
  # @return [FFI::Pointer]
  #
  # source://karafka//lib/karafka/connection/client.rb#344
  def consumer_group_metadata_pointer; end

  # Triggers the rdkafka main queue events by consuming this queue. This is not the consumer
  # consumption queue but the one with:
  #   - error callbacks
  #   - stats callbacks
  #   - OAUTHBEARER token refresh callbacks
  #
  # @note It is non-blocking when timeout 0 and will not wait if queue empty. It costs up to
  #   2ms when no callbacks are triggered.
  # @param timeout [Integer] number of milliseconds to wait on events or 0 not to wait.
  #
  # source://karafka//lib/karafka/connection/client.rb#337
  def events_poll(timeout = T.unsafe(nil)); end

  # @return [String] id of the client
  #
  # source://karafka//lib/karafka/connection/client.rb#22
  def id; end

  # Marks given message as consumed.
  #
  # @note This method won't trigger automatic offsets commits, rather relying on the offset
  #   check-pointing trigger that happens with each batch processed. It will however check the
  #   `librdkafka` assignment ownership to increase accuracy for involuntary revocations.
  # @param message [Karafka::Messages::Message] message that we want to mark as processed
  # @param metadata [String, nil] offset storage metadata or nil if none
  # @return [Boolean] true if successful. False if we no longer own given partition
  #
  # source://karafka//lib/karafka/connection/client.rb#285
  def mark_as_consumed(message, metadata = T.unsafe(nil)); end

  # Marks a given message as consumed and commits the offsets in a blocking way.
  #
  # @param message [Karafka::Messages::Message] message that we want to mark as processed
  # @param metadata [String, nil] offset storage metadata or nil if none
  # @return [Boolean] true if successful. False if we no longer own given partition
  #
  # source://karafka//lib/karafka/connection/client.rb#294
  def mark_as_consumed!(message, metadata = T.unsafe(nil)); end

  # @note Consumer name may change in case we regenerate it
  # @return [String] underlying consumer name
  #
  # source://karafka//lib/karafka/connection/client.rb#19
  def name; end

  # Pauses given partition and moves back to last successful offset processed.
  #
  # @note This will pause indefinitely and requires manual `#resume`
  # @note When `#internal_seek` is not involved (when offset is `nil`) we will not purge the
  #   librdkafka buffers and continue from the last cursor offset
  # @param topic [String] topic name
  # @param partition [Integer] partition
  # @param offset [Integer, nil] offset of the message on which we want to pause (this message
  #   will be reprocessed after getting back to processing) or nil if we want to pause and
  #   resume from the consecutive offset (+1 from the last message passed to us by librdkafka)
  #
  # source://karafka//lib/karafka/connection/client.rb#189
  def pause(topic, partition, offset = T.unsafe(nil)); end

  # Runs a single poll on the main queue and consumer queue ignoring all the potential errors
  # This is used as a keep-alive in the shutdown stage and any errors that happen here are
  # irrelevant from the shutdown process perspective
  #
  # This is used only to trigger rebalance callbacks and other callbacks
  #
  # source://karafka//lib/karafka/connection/client.rb#320
  def ping; end

  # Returns the value of attribute rebalance_manager.
  #
  # source://karafka//lib/karafka/connection/client.rb#11
  def rebalance_manager; end

  # Closes and resets the client completely.
  #
  # source://karafka//lib/karafka/connection/client.rb#301
  def reset; end

  # Resumes processing of a give topic partition after it was paused.
  #
  # @param topic [String] topic name
  # @param partition [Integer] partition
  #
  # source://karafka//lib/karafka/connection/client.rb#230
  def resume(topic, partition); end

  # Seek to a particular message. The next poll on the topic/partition will return the
  # message at the given offset.
  #
  # @note Please note, that if you are seeking to a time offset, getting the offset is blocking
  # @param message [Messages::Message, Messages::Seek] message to which we want to seek to.
  #   It can have the time based offset.
  #
  # source://karafka//lib/karafka/connection/client.rb#175
  def seek(message); end

  # Gracefully stops topic consumption.
  #
  # source://karafka//lib/karafka/connection/client.rb#259
  def stop; end

  # Stores offset for a given partition of a given topic based on the provided message.
  #
  # @param message [Karafka::Messages::Message]
  # @param offset_metadata [String, nil] offset storage metadata or nil if none
  #
  # source://karafka//lib/karafka/connection/client.rb#131
  def store_offset(message, offset_metadata = T.unsafe(nil)); end

  # @return [Karafka::Routing::SubscriptionGroup] subscription group to which this client
  #   belongs to
  #
  # source://karafka//lib/karafka/connection/client.rb#15
  def subscription_group; end

  private

  # Builds a new rdkafka consumer instance based on the subscription group configuration
  #
  # @return [Rdkafka::Consumer]
  #
  # source://karafka//lib/karafka/connection/client.rb#600
  def build_consumer; end

  # Commits the stored offsets in a sync way and closes the consumer.
  #
  # source://karafka//lib/karafka/connection/client.rb#427
  def close; end

  # Non thread-safe message committing method
  #
  # @note We do **not** consider `no_offset` as any problem and we allow to commit offsets
  #   even when no stored, because with sync commit, it refreshes the ownership state of the
  #   consumer in a sync way.
  # @param async [Boolean] should the commit happen async or sync (async by default)
  # @return [Boolean] true if offset commit worked, false if we've lost the assignment
  #
  # source://karafka//lib/karafka/connection/client.rb#379
  def internal_commit_offsets(async: T.unsafe(nil)); end

  # Non-mutexed seek that should be used only internally. Outside we expose `#seek` that is
  # wrapped with a mutex.
  #
  # @note Will not invoke seeking if the desired seek would lead us to the current position.
  #   This prevents us from flushing librdkafka buffer when it is not needed.
  # @param message [Messages::Message, Messages::Seek] message to which we want to seek to.
  #   It can have the time based offset.
  #
  # source://karafka//lib/karafka/connection/client.rb#391
  def internal_seek(message); end

  # When we cannot store an offset, it means we no longer own the partition
  #
  # Non thread-safe offset storing method
  #
  # @param message [Karafka::Messages::Message]
  # @param metadata [String, nil] offset storage metadata or nil if none
  # @return [Boolean] true if we could store the offset (if we still own the partition)
  #
  # source://karafka//lib/karafka/connection/client.rb#369
  def internal_store_offset(message, metadata); end

  # @return [Rdkafka::Consumer] librdkafka consumer instance
  #
  # source://karafka//lib/karafka/connection/client.rb#673
  def kafka; end

  # Performs a single poll operation and handles retries and errors
  #
  # Keep in mind, that this timeout will be limited by a tick interval value, because we cannot
  # block on a single poll longer than that. Otherwise our events polling would not be able to
  # run frequently enough. This means, that even if you provide big value, it will not block
  # for that long. This is anyhow compensated by the `#batch_poll` that can run for extended
  # period of time but will run events polling frequently while waiting for the requested total
  # time.
  #
  # @param timeout [Integer] timeout for a single poll.
  # @return [Rdkafka::Consumer::Message, nil, Symbol] fetched message, nil if nothing polled
  #   within the time we had or symbol indicating the early return reason
  #
  # source://karafka//lib/karafka/connection/client.rb#505
  def poll(timeout); end

  # We may have a case where in the middle of data polling, we've lost a partition.
  # In a case like this we should remove all the pre-buffered messages from list partitions as
  # we are no longer responsible in a given process for processing those messages and they
  # should have been picked up by a different process.
  #
  # source://karafka//lib/karafka/connection/client.rb#662
  def remove_revoked_and_duplicated_messages; end

  # @param topic [String]
  # @param partition [Integer]
  # @return [Rdkafka::Consumer::TopicPartitionList]
  #
  # source://karafka//lib/karafka/connection/client.rb#470
  def topic_partition_list(topic, partition); end

  # @param topic [String]
  # @param partition [Integer]
  # @return [Integer] current position within topic partition or `-1` if it could not be
  #   established. It may be `-1` in case we lost the assignment or we did not yet fetch data
  #   for this topic partition
  #
  # source://karafka//lib/karafka/connection/client.rb#486
  def topic_partition_position(topic, partition); end

  # Unsubscribes from all the subscriptions
  #
  # @note This is a private API to be used only on shutdown
  # @note We do not re-raise since this is supposed to be only used on close and can be safely
  #   ignored. We do however want to instrument on it
  #
  # source://karafka//lib/karafka/connection/client.rb#456
  def unsubscribe; end
end

# How many times should we retry polling in case of a failure
#
# source://karafka//lib/karafka/connection/client.rb#25
Karafka::Connection::Client::MAX_POLL_RETRIES = T.let(T.unsafe(nil), Integer)

# Conductor is responsible for time orchestration of listeners manager.
# It blocks when manager is not needed as there were no state changes that could cause any
# listeners config changes and unblocks when things change or when certain time passed.
# The time based unblocking allows for building of complex managers that could be state aware
#
# source://karafka//lib/karafka/connection/conductor.rb#9
class Karafka::Connection::Conductor
  # @param max_interval [Integer] after how many milliseconds of doing nothing should we wake
  #   up the manager despite no state changes
  # @return [Conductor] a new instance of Conductor
  #
  # source://karafka//lib/karafka/connection/conductor.rb#12
  def initialize(max_interval = T.unsafe(nil)); end

  # Releases wait lock on state change
  #
  # source://karafka//lib/karafka/connection/conductor.rb#23
  def signal; end

  # Waits in a blocking way until it is time to manage listeners
  #
  # source://karafka//lib/karafka/connection/conductor.rb#18
  def wait; end
end

# A single listener that listens to incoming messages from a single subscription group.
# It polls the messages and then enqueues jobs. It also takes care of potential recovery from
# critical errors by restarting everything in a safe manner.
#
# This is the heart of the consumption process.
#
# It provides async API for managing, so all status changes are expected to be async.
#
# source://karafka//lib/karafka/connection/listener.rb#12
class Karafka::Connection::Listener
  include ::Karafka::Helpers::Async
  extend ::Forwardable

  # @param subscription_group [Karafka::Routing::SubscriptionGroup]
  # @param jobs_queue [Karafka::Processing::JobsQueue] queue where we should push work
  # @param scheduler [Karafka::Processing::Scheduler] scheduler we want to use
  # @return [Karafka::Connection::Listener] listener instance
  #
  # source://karafka//lib/karafka/connection/listener.rb#32
  def initialize(subscription_group, jobs_queue, scheduler); end

  # @return [Boolean] is this listener active (not stopped and not pending)
  #
  # source://karafka//lib/karafka/connection/listener.rb#99
  def active?; end

  # source://forwardable/1.3.3/forwardable.rb#231
  def alive?(*args, **_arg1, &block); end

  # Runs the main listener fetch loop.
  #
  # @note Prefetch callbacks can be used to seek offset or do other things before we actually
  #   start consuming data
  #
  # source://karafka//lib/karafka/connection/listener.rb#67
  def call; end

  # Can be useful for logging
  #
  # @return [String] id of this listener
  #
  # source://karafka//lib/karafka/connection/listener.rb#17
  def id; end

  # source://forwardable/1.3.3/forwardable.rb#231
  def join(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def name(*args, **_arg1, &block); end

  # source://karafka//lib/karafka/connection/listener.rb#93
  def pending!; end

  # source://karafka//lib/karafka/connection/listener.rb#88
  def pending?; end

  # source://karafka//lib/karafka/connection/listener.rb#93
  def quiet!; end

  # source://karafka//lib/karafka/connection/listener.rb#88
  def quiet?; end

  # source://karafka//lib/karafka/connection/listener.rb#93
  def quieted!; end

  # source://karafka//lib/karafka/connection/listener.rb#88
  def quieting?; end

  # source://karafka//lib/karafka/connection/listener.rb#93
  def running!; end

  # source://karafka//lib/karafka/connection/listener.rb#88
  def running?; end

  # Stops the jobs queue, triggers shutdown on all the executors (sync), commits offsets and
  # stops kafka client.
  #
  # @note This method is not private despite being part of the fetch loop because in case of
  #   a forceful shutdown, it may be invoked from a separate thread
  # @note We wrap it with a mutex exactly because of the above case of forceful shutdown
  #
  # source://karafka//lib/karafka/connection/listener.rb#124
  def shutdown; end

  # We overwrite the state `#start` because on start we need to also start running listener in
  # the async thread. While other state transitions happen automatically and status state
  # change is enough, here we need to run the background threads
  #
  # source://karafka//lib/karafka/connection/listener.rb#106
  def start!; end

  # source://karafka//lib/karafka/connection/listener.rb#88
  def starting?; end

  # source://karafka//lib/karafka/connection/listener.rb#93
  def stop!; end

  # source://karafka//lib/karafka/connection/listener.rb#93
  def stopped!; end

  # source://karafka//lib/karafka/connection/listener.rb#88
  def stopped?; end

  # source://karafka//lib/karafka/connection/listener.rb#88
  def stopping?; end

  # @return [Karafka::Routing::SubscriptionGroup] subscription group that this listener handles
  #
  # source://karafka//lib/karafka/connection/listener.rb#20
  def subscription_group; end

  # source://forwardable/1.3.3/forwardable.rb#231
  def terminate(*args, **_arg1, &block); end

  private

  # Takes the messages per topic partition and enqueues processing jobs in threads using
  # given scheduler. It also handles the idle jobs when filtering API removed all messages
  # and we need to run house-keeping
  #
  # source://karafka//lib/karafka/connection/listener.rb#332
  def build_and_schedule_flow_jobs; end

  # Builds and schedules periodic jobs for topics partitions for which no messages were
  # received recently. In case `Idle` job is invoked, we do not run periodic. Idle means that
  # a complex flow kicked in and it was a user choice not to run consumption but messages were
  # shipped.
  #
  # source://karafka//lib/karafka/connection/listener.rb#376
  def build_and_schedule_periodic_jobs; end

  # Enqueues revoking jobs for partitions that were taken away from the running process.
  #
  # source://karafka//lib/karafka/connection/listener.rb#278
  def build_and_schedule_revoked_jobs_for_revoked_partitions; end

  # Enqueues the shutdown jobs for all the executors that exist in our subscription group
  #
  # source://karafka//lib/karafka/connection/listener.rb#314
  def build_and_schedule_shutdown_jobs; end

  # Fetches the data and adds it to the jobs queue.
  #
  # @note We catch all the errors here, so they don't affect other listeners (or this one)
  #   so we will be able to listen and consume other incoming messages.
  #   Since it is run inside Karafka::Connection::Runner thread - catching all the exceptions
  #   won't crash the whole process. Here we mostly focus on catching the exceptions related to
  #   Kafka connections / Internet connection issues / Etc. Business logic problems should not
  #   propagate this far.
  #
  # source://karafka//lib/karafka/connection/listener.rb#148
  def fetch_loop; end

  # Polls messages within the time and amount boundaries defined in the settings and then
  # builds karafka messages based on the raw rdkafka messages buffer returned by the
  # `#batch_poll` method.
  #
  # @note There are two buffers, one for raw messages and one for "built" karafka messages
  #
  # source://karafka//lib/karafka/connection/listener.rb#271
  def poll_and_remap_messages; end

  # We can stop client without a problem, as it will reinitialize itself when running the
  # `#fetch_loop` again. We just need to remember to also reset the runner as it is a long
  # running one, so with a new connection to Kafka, we need to initialize the state of the
  # runner and underlying consumers once again.
  #
  # source://karafka//lib/karafka/connection/listener.rb#459
  def reset; end

  # Resumes processing of partitions that were paused due to an error.
  #
  # source://karafka//lib/karafka/connection/listener.rb#260
  def resume_paused_partitions; end

  # Waits for all the jobs from a given subscription group to finish before moving forward
  #
  # source://karafka//lib/karafka/connection/listener.rb#429
  def wait; end

  # Waits without blocking the polling
  #
  # This should be used only when we no longer plan to use any incoming messages data and we
  # can safely discard it. We can however use the rebalance information if needed.
  #
  # @note Performance of this is not relevant (in regards to blocks) because it is used only
  #   on shutdown and quiet, hence not in the running mode
  # @param wait_until [Proc] until this evaluates to true, we will poll data
  # @param after_ping [Proc] code that we want to run after each ping (if any)
  #
  # source://karafka//lib/karafka/connection/listener.rb#445
  def wait_pinging(wait_until:, after_ping: T.unsafe(nil)); end
end

# How long to wait in the initial events poll. Increases chances of having the initial events
# immediately available
#
# source://karafka//lib/karafka/connection/listener.rb#24
Karafka::Connection::Listener::INITIAL_EVENTS_POLL_TIMEOUT = T.let(T.unsafe(nil), Integer)

# Abstraction layer around listeners batch.
#
# source://karafka//lib/karafka/connection/listeners_batch.rb#6
class Karafka::Connection::ListenersBatch
  include ::Enumerable

  # @param jobs_queue [JobsQueue]
  # @return [ListenersBatch]
  #
  # source://karafka//lib/karafka/connection/listeners_batch.rb#11
  def initialize(jobs_queue); end

  # @return [Array<Listener>] active listeners
  #
  # source://karafka//lib/karafka/connection/listeners_batch.rb#34
  def active; end

  # Iterates over available listeners and yields each listener
  #
  # @param block [Proc] block we want to run
  #
  # source://karafka//lib/karafka/connection/listeners_batch.rb#29
  def each(&block); end
end

# Connections manager responsible for starting and managing listeners connections
#
# In the OSS version it starts listeners as they are without any connection management or
# resources utilization supervision and shuts them down or quiets  when time has come
#
# source://karafka//lib/karafka/connection/manager.rb#10
class Karafka::Connection::Manager
  # @return [Manager] a new instance of Manager
  #
  # source://karafka//lib/karafka/connection/manager.rb#11
  def initialize; end

  # Controls the state of listeners upon shutdown and quiet requests
  # In both cases (quieting and shutdown) we first need to stop processing more work and tell
  # listeners to become quiet (connected but not yielding messages) and then depending on
  # whether we want to stop fully or just keep quiet we apply different flow.
  #
  # @note It is important to ensure, that all listeners from the same consumer group are always
  #   all quiet before we can fully shutdown given consumer group. Skipping this can cause
  #   `Timed out LeaveGroupRequest in flight` and other errors. For the simplification, we just
  #   quiet all and only then move forward.
  # @note This manager works with the assumption, that all listeners are executed on register.
  #
  # source://karafka//lib/karafka/connection/manager.rb#39
  def control; end

  # @return [Boolean] true if all listeners are stopped
  #
  # source://karafka//lib/karafka/connection/manager.rb#24
  def done?; end

  # Registers provided listeners and starts all of them
  #
  # @param listeners [Connection::ListenersBatch]
  #
  # source://karafka//lib/karafka/connection/manager.rb#18
  def register(listeners); end

  private

  # Runs code only once and never again
  #
  # @param args [Object] anything we want to use as a set of unique keys for given execution
  #
  # source://karafka//lib/karafka/connection/manager.rb#63
  def once(*args); end
end

# Buffer used to build and store karafka messages built based on raw librdkafka messages.
#
# Why do we have two buffers? `RawMessagesBuffer` is used to store raw messages and to handle
#   cases related to partition revocation and reconnections. It is "internal" to the listening
#   process. `MessagesBuffer` on the other hand is used to "translate" those raw messages that
#   we know that are ok into Karafka messages and to simplify further work with them.
#
# While it adds a bit of overhead, it makes conceptual things much easier and it adds only two
#   simple hash iterations over messages batch.
#
# @note This buffer is NOT thread safe. We do not worry about it as we do not use it outside
#   of the main listener loop. It can be cleared after the jobs are scheduled with messages
#   it stores, because messages arrays are not "cleared" in any way directly and their
#   reference stays.
#
# source://karafka//lib/karafka/connection/messages_buffer.rb#19
class Karafka::Connection::MessagesBuffer
  # @param subscription_group [Karafka::Routing::SubscriptionGroup]
  # @return [MessagesBuffer] a new instance of MessagesBuffer
  #
  # source://karafka//lib/karafka/connection/messages_buffer.rb#23
  def initialize(subscription_group); end

  # Allows to iterate over all the topics and partitions messages
  #
  # @yieldparam topic [String] name
  # @yieldparam partition [Integer] number
  # @yieldparam messages [Array<Karafka::Messages::Message>] from a given topic partition
  #
  # source://karafka//lib/karafka/connection/messages_buffer.rb#62
  def each; end

  # @return [Boolean] is the buffer empty or does it contain any messages
  #
  # source://karafka//lib/karafka/connection/messages_buffer.rb#83
  def empty?; end

  # Checks if there are any messages from a given topic partition in the buffer
  #
  # @param topic [String] topic name
  # @param partition [Integer] partition number
  # @return [Boolean] true if there is at least one message from this topic partition,
  #   otherwise false
  #
  # source://karafka//lib/karafka/connection/messages_buffer.rb#75
  def present?(topic, partition); end

  # Remaps raw messages from the raw messages buffer to Karafka messages
  #
  # @param raw_messages_buffer [RawMessagesBuffer] buffer with raw messages
  #
  # source://karafka//lib/karafka/connection/messages_buffer.rb#35
  def remap(raw_messages_buffer); end

  # Returns the value of attribute size.
  #
  # source://karafka//lib/karafka/connection/messages_buffer.rb#20
  def size; end

  private

  # Clears the buffer completely
  #
  # source://karafka//lib/karafka/connection/messages_buffer.rb#90
  def clear; end
end

# Partitions pauses management abstraction layer.
# It aggregates all the pauses for all the partitions that we're working with.
#
# source://karafka//lib/karafka/connection/pauses_manager.rb#7
class Karafka::Connection::PausesManager
  # @return [Karafka::Connection::PausesManager] pauses manager
  #
  # source://karafka//lib/karafka/connection/pauses_manager.rb#9
  def initialize; end

  # Creates or fetches pause tracker of a given topic partition.
  #
  # @param topic [::Karafka::Routing::Topic] topic
  # @param partition [Integer] partition number
  # @return [Karafka::TimeTrackers::Pause] pause tracker instance
  #
  # source://karafka//lib/karafka/connection/pauses_manager.rb#20
  def fetch(topic, partition); end

  # Resumes processing of partitions for which pause time has ended.
  #
  # @yieldparam topic [Karafka::Routing::Topic]
  # @yieldparam partition [Integer] number
  #
  # source://karafka//lib/karafka/connection/pauses_manager.rb#32
  def resume; end
end

# Usually it is ok to use the `Rdkafka::Consumer` directly because we need 1:1 its
# functionality. There are however cases where we want to have extra recoveries or other
# handling of errors and settings. This is where this module comes in handy.
#
# We do not want to wrap and delegate all via a proxy object for performance reasons, but we
# do still want to be able to alter some functionalities. This wrapper helps us do it when
# it would be needed
#
# source://karafka//lib/karafka/connection/proxy.rb#12
class Karafka::Connection::Proxy < ::SimpleDelegator
  # @param obj [Rdkafka::Consumer, Proxy] rdkafka consumer or consumer wrapped with proxy
  # @return [Proxy] a new instance of Proxy
  #
  # source://karafka//lib/karafka/connection/proxy.rb#30
  def initialize(obj); end

  # Returns the value of attribute wrapped.
  #
  # source://karafka//lib/karafka/connection/proxy.rb#25
  def __getobj__; end

  # Non thread-safe message committing method
  #
  # @note We do **not** consider `no_offset` as any problem and we allow to commit offsets
  #   even when no stored, because with sync commit, it refreshes the ownership state of the
  #   consumer in a sync way.
  # @param tpl [Rdkafka::Consumer::TopicPartitionList, nil] tpl or nil
  # @param async [Boolean] should the commit happen async or sync (async by default)
  # @return [Boolean] true if offset commit worked, false if we've lost the assignment
  #
  # source://karafka//lib/karafka/connection/proxy.rb#119
  def commit_offsets(tpl = T.unsafe(nil), async: T.unsafe(nil)); end

  # Similar to `#query_watermark_offsets`.
  #
  # @param tpl [Rdkafka::Consumer::TopicPartitionList, nil] tpl or nil for full current
  #   assignment tpl usage
  # @return [Rdkafka::Consumer::TopicPartitionList] tpl with committed offsets and metadata
  #
  # source://karafka//lib/karafka/connection/proxy.rb#83
  def committed(tpl = T.unsafe(nil)); end

  # @param tpl [Rdkafka::Consumer::TopicPartitionList] list of topics and partitions for which
  #   we want to get the lag on the defined CG
  # @return [Hash<String, Hash>] hash with topics and their partitions lags
  #
  # source://karafka//lib/karafka/connection/proxy.rb#149
  def lag(tpl); end

  # @param topic_name [String, nil] Name of the topic we're interested in or nil if we want to
  #   get info on all topics
  # @return [Rdkafka::Metadata] rdkafka metadata object with the requested details
  #
  # source://karafka//lib/karafka/connection/proxy.rb#164
  def metadata(topic_name = T.unsafe(nil)); end

  # Similar to `#query_watermark_offsets`, this method can be sensitive to latency. We handle
  # this the same way
  #
  # @param tpl [Rdkafka::Consumer::TopicPartitionList] tpl to get time offsets
  # @return [Rdkafka::Consumer::TopicPartitionList] tpl with time offsets
  #
  # source://karafka//lib/karafka/connection/proxy.rb#66
  def offsets_for_times(tpl); end

  # Proxies the `#query_watermark_offsets` with extra recovery from timeout problems.
  # We impose our own custom timeout to make sure, that high-latency clusters and overloaded
  # clusters can handle our requests.
  #
  # @param topic [String] topic name
  # @param partition [Partition]
  # @return [Array<Integer, Integer>] watermark offsets
  #
  # source://karafka//lib/karafka/connection/proxy.rb#46
  def query_watermark_offsets(topic, partition); end

  # When we cannot store an offset, it means we no longer own the partition
  #
  # Non thread-safe offset storing method
  #
  # @param message [Karafka::Messages::Message]
  # @param metadata [String, nil] offset storage metadata or nil if none
  # @return [Boolean] true if we could store the offset (if we still own the partition)
  #
  # source://karafka//lib/karafka/connection/proxy.rb#101
  def store_offset(message, metadata = T.unsafe(nil)); end

  # Returns the value of attribute wrapped.
  #
  # source://karafka//lib/karafka/connection/proxy.rb#25
  def wrapped; end

  # Sets the attribute wrapped
  #
  # @param value the value to set the attribute wrapped to.
  #
  # source://karafka//lib/karafka/connection/proxy.rb#25
  def wrapped=(_arg0); end

  private

  # Runs expected block of code with few retries on all_brokers_down
  # librdkafka can return `all_brokers_down` for scenarios when broker is overloaded or not
  # reachable due to latency.
  #
  # @param max_attempts [Integer] how many attempts (not retries) should we take before failing
  #   completely.
  # @param wait_time [Integer, Float] how many seconds should we wait. It uses `#sleep` of Ruby
  #   so it needs time in seconds.
  # @param errors [Array<Symbol>] rdkafka errors we want to retry on
  #
  # source://karafka//lib/karafka/connection/proxy.rb#186
  def with_broker_errors_retry(max_attempts:, wait_time: T.unsafe(nil), errors: T.unsafe(nil)); end
end

# Errors on which we want to retry
# Includes temporary errors related to node not being (or not yet being) coordinator or a
# leader to a given set of partitions. Usually goes away after a retry
#
# source://karafka//lib/karafka/connection/proxy.rb#16
Karafka::Connection::Proxy::RETRYABLE_DEFAULT_ERRORS = T.let(T.unsafe(nil), Array)

# Buffer for raw librdkafka messages.
#
# When message is added to this buffer, it gets assigned to an array with other messages from
# the same topic and partition.
#
# @note This buffer is NOT threadsafe.
# @note We store data here in groups per topic partition to handle the revocation case, where
#   we may need to remove messages from a single topic partition.
#
# source://karafka//lib/karafka/connection/raw_messages_buffer.rb#14
class Karafka::Connection::RawMessagesBuffer
  # @return [Karafka::Connection::MessagesBuffer] buffer instance
  #
  # source://karafka//lib/karafka/connection/raw_messages_buffer.rb#18
  def initialize; end

  # Adds a message to the buffer.
  #
  # @param message [Rdkafka::Consumer::Message] raw rdkafka message
  # @return [Array<Rdkafka::Consumer::Message>] given partition topic sub-buffer array
  #
  # source://karafka//lib/karafka/connection/raw_messages_buffer.rb#31
  def <<(message); end

  # Removes all the data from the buffer.
  #
  # @note We do not clear the whole groups hash but rather we clear the partition hashes, so
  #   we save ourselves some objects allocations. We cannot clear the underlying arrays as they
  #   may be used in other threads for data processing, thus if we would clear it, we could
  #   potentially clear a raw messages array for a job that is in the jobs queue.
  #
  # source://karafka//lib/karafka/connection/raw_messages_buffer.rb#86
  def clear; end

  # Removes given topic and partition data out of the buffer
  # This is used when there's a partition revocation
  #
  # @param topic [String] topic we're interested in
  # @param partition [Integer] partition of which data we want to remove
  #
  # source://karafka//lib/karafka/connection/raw_messages_buffer.rb#53
  def delete(topic, partition); end

  # Allows to iterate over all the topics and partitions messages
  #
  # @yieldparam topic [String] name
  # @yieldparam partition [Integer] number
  # @yieldparam topic [Array<Rdkafka::Consumer::Message>] partition aggregated results
  #
  # source://karafka//lib/karafka/connection/raw_messages_buffer.rb#41
  def each; end

  # Returns the value of attribute size.
  #
  # source://karafka//lib/karafka/connection/raw_messages_buffer.rb#15
  def size; end

  # Removes duplicated messages from the same partitions
  # This should be used only when rebalance occurs, as we may get data again we already have
  # due to the processing from the last offset. In cases like this, we may get same data
  # again and we do want to ensure as few duplications as possible
  #
  # source://karafka//lib/karafka/connection/raw_messages_buffer.rb#70
  def uniq!; end

  private

  # Updates the messages count if we performed any operations that could change the state
  #
  # source://karafka//lib/karafka/connection/raw_messages_buffer.rb#94
  def recount!; end
end

# Manager for tracking changes in the partitions assignment after the assignment is done.
#
# We need tracking of those to clean up consumers that will no longer process given partitions
# as they were taken away.
#
# @note Since this does not happen really often, we try to stick with same objects for the
#   empty states most of the time, so we don't create many objects during the manager life
# @note Internally in the rebalance manager we have a notion of lost partitions. Partitions
#   that are lost, are those that got revoked but did not get re-assigned back. We do not
#   expose this concept outside and we normalize to have them revoked, as it is irrelevant
#   from the rest of the code perspective as only those that are lost are truly revoked.
# @note For cooperative-sticky `#assigned_partitions` holds only the recently assigned
#   partitions, not all the partitions that are owned
# @note We have to have the `subscription_group` reference because we have a global pipeline
#   for notifications and we need to make sure we track changes only for things that are of
#   relevance to our subscription group
#
# source://karafka//lib/karafka/connection/rebalance_manager.rb#24
class Karafka::Connection::RebalanceManager
  # @param subscription_group_id [String] subscription group id
  # @return [RebalanceManager]
  #
  # source://karafka//lib/karafka/connection/rebalance_manager.rb#34
  def initialize(subscription_group_id); end

  # @note This method is needed to make sure that when using cooperative-sticky, we do not
  #   close until first rebalance. Otherwise librdkafka may crash.
  # @return [Boolean] true if there was at least one rebalance
  # @see https://github.com/confluentinc/librdkafka/issues/4312
  #
  # source://karafka//lib/karafka/connection/rebalance_manager.rb#63
  def active?; end

  # Returns the value of attribute assigned_partitions.
  #
  # source://karafka//lib/karafka/connection/rebalance_manager.rb#28
  def assigned_partitions; end

  # @return [Boolean] indicates a state change in the partitions assignment
  #
  # source://karafka//lib/karafka/connection/rebalance_manager.rb#55
  def changed?; end

  # Resets the rebalance manager state
  # This needs to be done before each polling loop as during the polling, the state may be
  # changed
  #
  # source://karafka//lib/karafka/connection/rebalance_manager.rb#48
  def clear; end

  # We consider as lost only partitions that were taken away and not re-assigned back to us
  #
  # source://karafka//lib/karafka/connection/rebalance_manager.rb#68
  def lost_partitions; end

  # Callback that kicks in inside of rdkafka, when new partitions were assigned.
  #
  # @param event [Karafka::Core::Monitoring::Event]
  # @private
  #
  # source://karafka//lib/karafka/connection/rebalance_manager.rb#82
  def on_rebalance_partitions_assigned(event); end

  # Callback that kicks in inside of rdkafka, when partitions were revoked.
  #
  # @param event [Karafka::Core::Monitoring::Event]
  # @private
  #
  # source://karafka//lib/karafka/connection/rebalance_manager.rb#95
  def on_rebalance_partitions_revoked(event); end

  # Returns the value of attribute revoked_partitions.
  #
  # source://karafka//lib/karafka/connection/rebalance_manager.rb#28
  def revoked_partitions; end
end

# Empty array for internal usage not to create new objects
#
# source://karafka//lib/karafka/connection/rebalance_manager.rb#26
Karafka::Connection::RebalanceManager::EMPTY_ARRAY = T.let(T.unsafe(nil), Array)

# Listener connection status representation
#
# source://karafka//lib/karafka/connection/status.rb#7
class Karafka::Connection::Status
  # @return [Status] a new instance of Status
  #
  # source://karafka//lib/karafka/connection/status.rb#46
  def initialize; end

  # @return [Boolean] listener is considered active when it has a client reference that may
  #   be active and connected to Kafka
  #
  # source://karafka//lib/karafka/connection/status.rb#82
  def active?; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def conductor; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def monitor; end

  # source://karafka//lib/karafka/connection/status.rb#27
  def pending!; end

  # source://karafka//lib/karafka/connection/status.rb#40
  def pending?; end

  # source://karafka//lib/karafka/connection/status.rb#27
  def quiet!; end

  # source://karafka//lib/karafka/connection/status.rb#40
  def quiet?; end

  # source://karafka//lib/karafka/connection/status.rb#27
  def quieted!; end

  # source://karafka//lib/karafka/connection/status.rb#40
  def quieting?; end

  # Moves status back from stopped to pending (and only that). We should not be able to reset
  # listeners that are not stopped
  #
  # source://karafka//lib/karafka/connection/status.rb#74
  def reset!; end

  # source://karafka//lib/karafka/connection/status.rb#27
  def running!; end

  # source://karafka//lib/karafka/connection/status.rb#40
  def running?; end

  # source://karafka//lib/karafka/connection/status.rb#27
  def start!; end

  # source://karafka//lib/karafka/connection/status.rb#40
  def starting?; end

  # If this listener was not even running, will just move it through states until final.
  # If it was running, will start the stopping procedures.
  # Will do nothing if it was already stopped
  #
  # source://karafka//lib/karafka/connection/status.rb#54
  def stop!; end

  # source://karafka//lib/karafka/connection/status.rb#27
  def stopped!; end

  # source://karafka//lib/karafka/connection/status.rb#40
  def stopped?; end

  # source://karafka//lib/karafka/connection/status.rb#40
  def stopping?; end
end

# Available states and their transitions.
#
# source://karafka//lib/karafka/connection/status.rb#14
Karafka::Connection::Status::STATES = T.let(T.unsafe(nil), Hash)

# Module used to check some constraints that cannot be easily defined by Bundler
# At the moment we use it to ensure, that if Karafka is used, it operates with the expected
# web ui version and that older versions of Web UI that would not be compatible with the API
# changes in karafka are not used.
#
# We can make Web UI require certain karafka version range, but at the moment we do not have a
# strict 1:1 release pattern matching those two.
#
# source://karafka//lib/karafka/constraints.rb#11
module Karafka::Constraints
  class << self
    # Verifies that optional requirements are met.
    #
    # @raise [Errors::DependencyConstraintsError]
    #
    # source://karafka//lib/karafka/constraints.rb#14
    def verify!; end

    private

    # Requires given version file from a gem location
    #
    # @param version_location [String]
    # @return [Boolean] true if it was required or false if not reachable
    #
    # source://karafka//lib/karafka/constraints.rb#33
    def require_version(version_location); end

    # Builds a version object for comparing
    #
    # @param string [String]
    # @return [::Gem::Version]
    #
    # source://karafka//lib/karafka/constraints.rb#44
    def version(string); end
  end
end

# Namespace for all the validation contracts that we use to check input
#
# source://karafka//lib/karafka/contracts.rb#5
module Karafka::Contracts; end

# Base contract for all Karafka contracts
#
# source://karafka//lib/karafka/contracts/base.rb#6
class Karafka::Contracts::Base < ::Karafka::Core::Contractable::Contract
  # @note We use contracts only in the config validation context, so no need to add support
  #   for multiple error classes. It will be added when it will be needed.
  # @param data [Hash] data for validation
  # @raise [Errors::InvalidConfigurationError] invalid configuration error
  # @return [Boolean] true if all good
  #
  # source://karafka//lib/karafka/contracts/base.rb#12
  def validate!(data); end
end

# Contract with validation rules for Karafka configuration details.
#
# @note There are many more configuration options inside of the
#   `Karafka::Setup::Config` model, but we don't validate them here as they are
#   validated per each route (topic + consumer_group) because they can be overwritten,
#   so we validate all of that once all the routes are defined and ready.
#
# source://karafka//lib/karafka/contracts/config.rb#11
class Karafka::Contracts::Config < ::Karafka::Contracts::Base; end

# Contract for single full route (consumer group + topics) validation.
#
# source://karafka//lib/karafka/contracts/consumer_group.rb#6
class Karafka::Contracts::ConsumerGroup < ::Karafka::Contracts::Base
  class << self
    # @param topic [Hash] topic config hash
    # @return [String] topic unique key for validators
    #
    # source://karafka//lib/karafka/contracts/consumer_group.rb#58
    def topic_unique_key(topic); end
  end
end

# Contract for validating correctness of the server cli command options.
#
# source://karafka//lib/karafka/contracts/server_cli_options.rb#6
class Karafka::Contracts::ServerCliOptions < ::Karafka::Contracts::Base; end

# Regexp for validating format of groups and topics
#
# @note It is not nested inside of the contracts, as it is used by couple of them
#
# source://karafka//lib/karafka/contracts.rb#8
Karafka::Contracts::TOPIC_REGEXP = T.let(T.unsafe(nil), Regexp)

# Consumer group topic validation rules.
#
# source://karafka//lib/karafka/contracts/topic.rb#6
class Karafka::Contracts::Topic < ::Karafka::Contracts::Base; end

# Module for all supported by default deserializers.
#
# source://karafka//lib/karafka.rb#0
module Karafka::Deserializers; end

# Default message headers deserializer
#
# source://karafka//lib/karafka/deserializers/headers.rb#6
class Karafka::Deserializers::Headers
  # @param metadata [Karafka::Messages::Metadata] metadata object from which we obtain the
  #   `#raw_headers`
  # @return [Hash] expected message headers hash
  #
  # source://karafka//lib/karafka/deserializers/headers.rb#10
  def call(metadata); end
end

# Default message key deserializer
#
# source://karafka//lib/karafka/deserializers/key.rb#6
class Karafka::Deserializers::Key
  # @param metadata [Karafka::Messages::Metadata] metadata object from which we obtain the
  #   `#raw_key`
  # @return [String, nil] expected message key in a string format or nil if no key
  #
  # source://karafka//lib/karafka/deserializers/key.rb#10
  def call(metadata); end
end

# Default Karafka Json deserializer for loading JSON data in payload.
#
# source://karafka//lib/karafka/deserializers/payload.rb#7
class Karafka::Deserializers::Payload
  # @param message [Karafka::Messages::Message] Message object that we want to deserialize
  # @return [Hash] hash with deserialized JSON data
  #
  # source://karafka//lib/karafka/deserializers/payload.rb#10
  def call(message); end
end

# Allows to start and stop Karafka as part of a different process
#
# source://karafka//lib/karafka/embedded.rb#5
module Karafka::Embedded
  class << self
    # Quiets Karafka upon any event
    #
    # It will trigger the quiet procedure but won't wait.
    #
    # @note This method is not blocking and will not wait for Karafka to fully quiet.
    # @note Please keep in mind you need to `#stop` to actually stop the server anyhow.
    #
    # source://karafka//lib/karafka/embedded.rb#33
    def quiet; end

    # Starts Karafka without supervision and without ownership of signals in a background thread
    # so it won't interrupt other things running
    #
    # source://karafka//lib/karafka/embedded.rb#9
    def start; end

    # Stops Karafka upon any event
    #
    # @note This method is blocking because we want to wait until Karafka is stopped with final
    #   process shutdown
    #
    # source://karafka//lib/karafka/embedded.rb#22
    def stop; end
  end
end

# Env management class to get and set environment for Karafka
#
# source://karafka//lib/karafka/env.rb#5
class Karafka::Env < ::String
  # @note Will load appropriate environment automatically
  # @return [Karafka::Env] env object
  #
  # source://karafka//lib/karafka/env.rb#20
  def initialize; end

  # Reacts to missing methods, from which some might be the env checks.
  # If the method ends with '?' we assume, that it is an env check
  #
  # @param method_name [String] method name for missing or env name with question mark
  # @param arguments [Array] any arguments that we pass to the method
  #
  # source://karafka//lib/karafka/env.rb#42
  def method_missing(method_name, *arguments); end

  private

  # @param method_name [String] method name
  # @param include_private [Boolean] should we include private methods as well
  # @return [Boolean] true if we respond to a given missing method, otherwise false
  #
  # source://karafka//lib/karafka/env.rb#34
  def respond_to_missing?(method_name, include_private = T.unsafe(nil)); end
end

# Default fallback env
#
# source://karafka//lib/karafka/env.rb#14
Karafka::Env::DEFAULT_ENV = T.let(T.unsafe(nil), String)

# Keys where we look for environment details for Karafka
#
# source://karafka//lib/karafka/env.rb#7
Karafka::Env::LOOKUP_ENV_KEYS = T.let(T.unsafe(nil), Array)

# Namespace used to encapsulate all the internal errors of Karafka
#
# source://karafka//lib/karafka/errors.rb#5
module Karafka::Errors; end

# Raised in transactions when we attempt to store offset for a partition that we have lost
# This does not affect producer only transactions, hence we raise it only on offset storage
#
# source://karafka//lib/karafka/errors.rb#82
class Karafka::Errors::AssignmentLostError < ::Karafka::Errors::BaseError; end

# Base class for all the Karafka internal errors
#
# source://karafka//lib/karafka/errors.rb#7
class Karafka::Errors::BaseError < ::StandardError; end

# Raised if optional dependencies like karafka-web are required in a version that is not
# supported by the current framework version.
#
# Because we do not want to require web out of the box and we do not want to lock web with
# karafka 1:1, we do such a sanity check
#
# source://karafka//lib/karafka/errors.rb#89
class Karafka::Errors::DependencyConstraintsError < ::Karafka::Errors::BaseError; end

# Raised when we've waited enough for shutting down a non-responsive process
#
# source://karafka//lib/karafka/errors.rb#28
class Karafka::Errors::ForcefulShutdownError < ::Karafka::Errors::BaseError; end

# Raised when configuration doesn't match with validation contract
#
# source://karafka//lib/karafka/errors.rb#22
class Karafka::Errors::InvalidConfigurationError < ::Karafka::Errors::BaseError; end

# This should never happen. Please open an issue if it does.
#
# source://karafka//lib/karafka/errors.rb#48
class Karafka::Errors::InvalidCoordinatorStateError < ::Karafka::Errors::BaseError; end

# source://karafka//lib/karafka/errors.rb#42
class Karafka::Errors::InvalidLicenseTokenError < ::Karafka::Errors::BaseError; end

# Raised when we want to un-pause listener that was not paused
#
# source://karafka//lib/karafka/errors.rb#78
class Karafka::Errors::InvalidListenerPauseError < ::Karafka::Errors::BaseError; end

# Raised in case a listener that was paused is being resumed
#
# source://karafka//lib/karafka/errors.rb#75
class Karafka::Errors::InvalidListenerResumeError < ::Karafka::Errors::BaseError; end

# This should never happen. Please open an issue if it does.
#
# source://karafka//lib/karafka/errors.rb#54
class Karafka::Errors::InvalidRealOffsetUsageError < ::Karafka::Errors::BaseError; end

# This should never happen. Please open an issue if it does.
#
# source://karafka//lib/karafka/errors.rb#57
class Karafka::Errors::InvalidTimeBasedOffsetError < ::Karafka::Errors::BaseError; end

# Raised when the jobs queue receives a job that should not be received as it would cause
# the processing to go out of sync. We should never process in parallel data from the same
# topic partition (unless virtual partitions apply)
#
# source://karafka//lib/karafka/errors.rb#33
class Karafka::Errors::JobsQueueSynchronizationError < ::Karafka::Errors::BaseError; end

# Raised on attempt to deserializer a cleared message
#
# source://karafka//lib/karafka/errors.rb#45
class Karafka::Errors::MessageClearedError < ::Karafka::Errors::BaseError; end

# Raised when we try to use Karafka CLI commands (except install) without a boot file
#
# source://karafka//lib/karafka/errors.rb#25
class Karafka::Errors::MissingBootFileError < ::Karafka::Errors::BaseError; end

# Raised when router receives topic name which does not correspond with any routes
# This can only happen in a case when:
#   - you've received a message and we cannot match it with a consumer
#   - you've changed the routing, so router can no longer associate your topic to
#     any consumer
#   - or in a case when you do a lot of meta-programming and you change routing/etc on runtime
#
# In case this happens, you will have to create a temporary route that will allow
# you to "eat" everything from the Sidekiq queue.
#
# @see https://github.com/karafka/karafka/issues/135
#
# source://karafka//lib/karafka/errors.rb#19
class Karafka::Errors::NonMatchingRouteError < ::Karafka::Errors::BaseError; end

# Raised when we were not able to open pidfd for given pid
# This should not happen. If you see it, please report.
#
# source://karafka//lib/karafka/errors.rb#93
class Karafka::Errors::PidfdOpenFailedError < ::Karafka::Errors::BaseError; end

# Failed to send signal to a process via pidfd
# This should not happen. If you see it, please report.
#
# source://karafka//lib/karafka/errors.rb#97
class Karafka::Errors::PidfdSignalFailedError < ::Karafka::Errors::BaseError; end

# For internal usage only
# Raised when we run operations that require certain result but despite successfully finishing
# it is not yet available due to some synchronization mechanisms and caches
#
# source://karafka//lib/karafka/errors.rb#62
class Karafka::Errors::ResultNotVisibleError < ::Karafka::Errors::BaseError; end

# This should never happen. Please open an issue if it does.
#
# source://karafka//lib/karafka/errors.rb#51
class Karafka::Errors::StrategyNotFoundError < ::Karafka::Errors::BaseError; end

# Raised when given topic is not found while expected
#
# source://karafka//lib/karafka/errors.rb#36
class Karafka::Errors::TopicNotFoundError < ::Karafka::Errors::BaseError; end

# Raised in case user would want to perform nested transactions.
#
# source://karafka//lib/karafka/errors.rb#72
class Karafka::Errors::TransactionAlreadyInitializedError < ::Karafka::Errors::BaseError; end

# Raised when we attempt to perform operation that is only allowed inside of a transaction and
# there is no transaction around us
#
# source://karafka//lib/karafka/errors.rb#69
class Karafka::Errors::TransactionRequiredError < ::Karafka::Errors::BaseError; end

# Raised when there is an attempt to run an unrecognized CLI command
#
# source://karafka//lib/karafka/errors.rb#65
class Karafka::Errors::UnrecognizedCommandError < ::Karafka::Errors::BaseError; end

# This should never happen. Please open an issue if it does.
#
# source://karafka//lib/karafka/errors.rb#39
class Karafka::Errors::UnsupportedCaseError < ::Karafka::Errors::BaseError; end

# Raised when given option/feature is not supported on a given platform or when given option
# is not supported in a given configuration
#
# source://karafka//lib/karafka/errors.rb#101
class Karafka::Errors::UnsupportedOptionError < ::Karafka::Errors::BaseError; end

# Module containing classes and methods that provide some additional helper functionalities.
#
# source://karafka//lib/karafka/status.rb#0
module Karafka::Helpers; end

# Allows a given class to run async in a separate thread. Provides also few methods we may
# want to use to control the underlying thread
#
# @note Thread running code needs to manage it's own exceptions. If they leak out, they will
#   abort thread on exception.
#
# source://karafka//lib/karafka/helpers/async.rb#10
module Karafka::Helpers::Async
  mixes_in_class_methods ::Forwardable

  # Runs the `#call` method in a new thread
  #
  # @param thread_name [String] name that we want to assign to the thread when we start it
  #
  # source://karafka//lib/karafka/helpers/async.rb#31
  def async_call(thread_name = T.unsafe(nil)); end

  class << self
    # Adds forwardable to redirect thread-based control methods to the underlying thread that
    # runs the async operations
    #
    # @param base [Class] class we're including this module in
    #
    # source://karafka//lib/karafka/helpers/async.rb#22
    def included(base); end
  end
end

# Mutex used to ensure we do not create multiple threads if we decide to run this
# in parallel on multiple threads
#
# source://karafka//lib/karafka/helpers/async.rb#13
Karafka::Helpers::Async::MUTEX = T.let(T.unsafe(nil), Thread::Mutex)

# Simple wrapper for adding colors to strings
#
# source://karafka//lib/karafka/helpers/colorize.rb#6
module Karafka::Helpers::Colorize
  # @param string [String] string we want to have in green
  # @return [String] green string
  #
  # source://karafka//lib/karafka/helpers/colorize.rb#9
  def green(string); end

  # @param string [String] string we want to have in grey
  # @return [String] grey string
  #
  # source://karafka//lib/karafka/helpers/colorize.rb#27
  def grey(string); end

  # @param string [String] string we want to have in red
  # @return [String] red string
  #
  # source://karafka//lib/karafka/helpers/colorize.rb#15
  def red(string); end

  # @param string [String] string we want to have in yellow
  # @return [String] yellow string
  #
  # source://karafka//lib/karafka/helpers/colorize.rb#21
  def yellow(string); end
end

# Module allowing for configuration injections. By default injects whole app config
# Allows for granular config injection
#
# source://karafka//lib/karafka/helpers/config_importer.rb#7
class Karafka::Helpers::ConfigImporter < ::Module
  # @param attributes [Hash<Symbol, Array<Symbol>>] map defining what we want to inject.
  #   The key is the name under which attribute will be visible and the value is the full
  #   path to the attribute
  # @return [ConfigImporter] a new instance of ConfigImporter
  #
  # source://karafka//lib/karafka/helpers/config_importer.rb#11
  def initialize(attributes = T.unsafe(nil)); end

  # @param model [Object] object to which we want to add the config fetcher
  #
  # source://karafka//lib/karafka/helpers/config_importer.rb#17
  def included(model); end
end

# Object responsible for running given code with a given interval. It won't run given code
# more often than with a given interval.
#
# This allows us to execute certain code only once in a while.
#
# This can be used when we have code that could be invoked often due to it being in loops
# or other places but would only slow things down if would run with each tick.
#
# source://karafka//lib/karafka/helpers/interval_runner.rb#12
class Karafka::Helpers::IntervalRunner
  include ::Karafka::Core::Helpers::Time

  # @param interval [Integer] interval in ms for running the provided code. Defaults to the
  #   `internal.tick_interval` value
  # @param block [Proc] block of code we want to run once in a while
  # @return [IntervalRunner] a new instance of IntervalRunner
  #
  # source://karafka//lib/karafka/helpers/interval_runner.rb#18
  def initialize(interval: T.unsafe(nil), &block); end

  # Runs the requested code if it was not executed previously recently
  #
  # source://karafka//lib/karafka/helpers/interval_runner.rb#25
  def call; end

  # Resets the runner, so next `#call` will run the underlying code
  #
  # source://karafka//lib/karafka/helpers/interval_runner.rb#34
  def reset; end
end

# Multidelegator is used to delegate calls to multiple targets.
#
# @note Taken from http://stackoverflow.com/questions/6407141
#
# source://karafka//lib/karafka/helpers/multi_delegator.rb#8
class Karafka::Helpers::MultiDelegator
  # @param targets to which we want to delegate methods
  # @return [MultiDelegator] a new instance of MultiDelegator
  #
  # source://karafka//lib/karafka/helpers/multi_delegator.rb#10
  def initialize(*targets); end

  # source://karafka//lib/karafka/helpers/multi_delegator.rb#20
  def close(*args); end

  # source://karafka//lib/karafka/helpers/multi_delegator.rb#20
  def write(*args); end

  class << self
    # @example Delegate write and close to STDOUT and file
    #   Logger.new MultiDelegator.delegate(:write, :close).to(STDOUT, log_file)
    # @param methods names that should be delegated to
    #
    # source://karafka//lib/karafka/helpers/multi_delegator.rb#18
    def delegate(*methods); end

    def to(*_arg0); end
  end
end

# Namespace for all the things related with Karafka instrumentation process
#
# source://karafka//lib/karafka.rb#0
module Karafka::Instrumentation; end

# Keeps track of active assignments and materializes them by returning the routing topics
# with appropriate partitions that are assigned at a given moment
#
# It is auto-subscribed as part of Karafka itself.
#
# It is not heavy from the computational point of view, as it only operates during rebalances.
#
# We keep assignments as flat topics structure because we can go from topics to both
# subscription and consumer groups if needed.
#
# source://karafka//lib/karafka/instrumentation/assignments_tracker.rb#14
class Karafka::Instrumentation::AssignmentsTracker
  include ::Singleton
  extend ::Singleton::SingletonClassMethods

  # @return [AssignmentsTracker] a new instance of AssignmentsTracker
  #
  # source://karafka//lib/karafka/instrumentation/assignments_tracker.rb#17
  def initialize; end

  # Clears all the assignments
  #
  # source://karafka//lib/karafka/instrumentation/assignments_tracker.rb#42
  def clear; end

  # Returns all the active/current assignments of this given process
  #
  # @note Keep in mind, that those assignments can change any time, especially when working
  #   with multiple consumer groups or subscription groups.
  # @note We return a copy because we modify internals and we do not want user to tamper with
  #   the data accidentally
  # @return [Hash<Karafka::Routing::Topic, Array<Integer>>]
  #
  # source://karafka//lib/karafka/instrumentation/assignments_tracker.rb#31
  def current; end

  # When client is under reset due to critical issues, remove all of its assignments as we will
  #   get a new set of assignments
  #
  # @param event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka//lib/karafka/instrumentation/assignments_tracker.rb#51
  def on_client_reset(event); end

  # # Adds partitions to the current assignments hash
  #
  # @param event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka//lib/karafka/instrumentation/assignments_tracker.rb#82
  def on_rebalance_partitions_assigned(event); end

  # Removes partitions from the current assignments hash
  #
  # @param event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka//lib/karafka/instrumentation/assignments_tracker.rb#64
  def on_rebalance_partitions_revoked(event); end

  class << self
    private

    def allocate; end
    def new(*_arg0); end
  end
end

# Callbacks used to transport things from rdkafka
#
# source://karafka//lib/karafka.rb#0
module Karafka::Instrumentation::Callbacks; end

# Callback that kicks in when consumer error occurs and is published in a background thread
#
# source://karafka//lib/karafka/instrumentation/callbacks/error.rb#8
class Karafka::Instrumentation::Callbacks::Error
  # @param subscription_group_id [String] id of the current subscription group instance
  # @param consumer_group_id [String] id of the current consumer group
  # @param client_name [String] rdkafka client name
  # @return [Error] a new instance of Error
  #
  # source://karafka//lib/karafka/instrumentation/callbacks/error.rb#16
  def initialize(subscription_group_id, consumer_group_id, client_name); end

  # Runs the instrumentation monitor with error
  #
  # @note It will only instrument on errors of the client of our consumer
  # @param client_name [String] rdkafka client name
  # @param error [Rdkafka::Error] error that occurred
  #
  # source://karafka//lib/karafka/instrumentation/callbacks/error.rb#26
  def call(client_name, error); end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def monitor; end
end

# Callback that is triggered when oauth token needs to be refreshed.
#
# source://karafka//lib/karafka/instrumentation/callbacks/oauthbearer_token_refresh.rb#7
class Karafka::Instrumentation::Callbacks::OauthbearerTokenRefresh
  # @param bearer [Rdkafka::Consumer, Rdkafka::Admin] given rdkafka instance. It is needed as
  #   we need to have a reference to call `#oauthbearer_set_token` or
  #   `#oauthbearer_set_token_failure` upon the event.
  # @return [OauthbearerTokenRefresh] a new instance of OauthbearerTokenRefresh
  #
  # source://karafka//lib/karafka/instrumentation/callbacks/oauthbearer_token_refresh.rb#15
  def initialize(bearer); end

  # @param _rd_config [Rdkafka::Config]
  # @param bearer_name [String] name of the bearer for which we refresh
  #
  # source://karafka//lib/karafka/instrumentation/callbacks/oauthbearer_token_refresh.rb#21
  def call(_rd_config, bearer_name); end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def monitor; end
end

# Callback that connects to the librdkafka rebalance callback and converts those events into
# our internal events
#
# source://karafka//lib/karafka/instrumentation/callbacks/rebalance.rb#8
class Karafka::Instrumentation::Callbacks::Rebalance
  # @param subscription_group [Karafka::Routes::SubscriptionGroup] subscription group for
  #   which we want to manage rebalances
  # @return [Rebalance] a new instance of Rebalance
  #
  # source://karafka//lib/karafka/instrumentation/callbacks/rebalance.rb#15
  def initialize(subscription_group); end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def monitor; end

  # Publishes an event that partitions are going to be assigned
  #
  # @param tpl [Rdkafka::Consumer::TopicPartitionList]
  #
  # source://karafka//lib/karafka/instrumentation/callbacks/rebalance.rb#30
  def on_partitions_assign(tpl); end

  # Publishes an event that partitions were assigned.
  #
  # @param tpl [Rdkafka::Consumer::TopicPartitionList]
  #
  # source://karafka//lib/karafka/instrumentation/callbacks/rebalance.rb#45
  def on_partitions_assigned(tpl); end

  # Publishes an event that partitions are going to be revoked.
  # At this stage we can still commit offsets, etc.
  #
  # @param tpl [Rdkafka::Consumer::TopicPartitionList]
  #
  # source://karafka//lib/karafka/instrumentation/callbacks/rebalance.rb#23
  def on_partitions_revoke(tpl); end

  # Publishes an event that partitions were revoked. This is after we've lost them, so no
  # option to commit offsets.
  #
  # @param tpl [Rdkafka::Consumer::TopicPartitionList]
  #
  # source://karafka//lib/karafka/instrumentation/callbacks/rebalance.rb#38
  def on_partitions_revoked(tpl); end

  private

  # Publishes info that a rebalance event of a given type has happened
  #
  # @param name [String] name of the event
  # @param tpl [Rdkafka::Consumer::TopicPartitionList]
  #
  # source://karafka//lib/karafka/instrumentation/callbacks/rebalance.rb#55
  def instrument(name, tpl); end
end

# Statistics callback handler
#
# @see `WaterDrop::Instrumentation::Callbacks::Statistics` for details on why we decorate
#   those statistics
#
# source://karafka//lib/karafka/instrumentation/callbacks/statistics.rb#9
class Karafka::Instrumentation::Callbacks::Statistics
  # @param subscription_group_id [String] id of the current subscription group
  # @param consumer_group_id [String] id of the current consumer group
  # @param client_name [String] rdkafka client name
  # @return [Statistics] a new instance of Statistics
  #
  # source://karafka//lib/karafka/instrumentation/callbacks/statistics.rb#17
  def initialize(subscription_group_id, consumer_group_id, client_name); end

  # Emits decorated statistics to the monitor
  #
  # @param statistics [Hash] rdkafka statistics
  #
  # source://karafka//lib/karafka/instrumentation/callbacks/statistics.rb#26
  def call(statistics); end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def monitor; end
end

# Default logger for Event Delegator
#
# @note It uses ::Logger features - providing basic logging
#
# source://karafka//lib/karafka/instrumentation/logger.rb#7
class Karafka::Instrumentation::Logger < ::Logger
  # Creates a new instance of logger ensuring that it has a place to write to
  #
  # @param _args Any arguments that we don't care about but that are needed in order to
  #   make this logger compatible with the default Ruby one
  # @return [Logger] a new instance of Logger
  #
  # source://karafka//lib/karafka/instrumentation/logger.rb#22
  def initialize(*_args); end

  private

  # @note File is being opened in append mode ('a')
  # @return [File] file to which we want to write our logs
  #
  # source://karafka//lib/karafka/instrumentation/logger.rb#45
  def file; end

  # @return [Pathname] Path to a file to which we should log
  #
  # source://karafka//lib/karafka/instrumentation/logger.rb#39
  def log_path; end

  # We use this approach to log stuff to file and to the $stdout at the same time
  #
  # @return [Karafka::Helpers::MultiDelegator] multi delegator instance
  #   to which we will be writing logs
  #
  # source://karafka//lib/karafka/instrumentation/logger.rb#32
  def target; end
end

# Map containing information about log level for given environment
#
# source://karafka//lib/karafka/instrumentation/logger.rb#9
Karafka::Instrumentation::Logger::ENV_MAP = T.let(T.unsafe(nil), Hash)

# Default listener that hooks up to our instrumentation and uses its events for logging
# It can be removed/replaced or anything without any harm to the Karafka app flow.
#
# source://karafka//lib/karafka/instrumentation/logger_listener.rb#7
class Karafka::Instrumentation::LoggerListener
  # @param log_polling [Boolean] should we log the fact that messages are being polled. This is
  #   usually noisy and not useful in production but can be useful in dev. While users can
  #   do this themselves this has been requested and asked for often, thus similar to how
  #   extensive logging can be disabled in WaterDrop, we do it here as well.
  # @return [LoggerListener] a new instance of LoggerListener
  #
  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#23
  def initialize(log_polling: T.unsafe(nil)); end

  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#341
  def debug(*args); end

  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#341
  def error(*args); end

  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#341
  def fatal(*args); end

  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#341
  def info(*args); end

  # @param _event [Karafka::Core::Monitoring::Event] event details including payload
  #
  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#172
  def on_app_quiet(_event); end

  # @param _event [Karafka::Core::Monitoring::Event] event details including payload
  #
  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#167
  def on_app_quieting(_event); end

  # Logs info that we're running Karafka app.
  #
  # @param _event [Karafka::Core::Monitoring::Event] event details including payload
  #
  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#157
  def on_app_running(_event); end

  # Logs info that we stopped the Karafka server.
  #
  # @param _event [Karafka::Core::Monitoring::Event] event details including payload
  #
  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#186
  def on_app_stopped(_event); end

  # Logs info that we're going to stop the Karafka server.
  #
  # @param _event [Karafka::Core::Monitoring::Event] event details including payload
  #
  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#179
  def on_app_stopping(_event); end

  # Prints info about a consumer pause occurrence. Irrelevant if user or system initiated.
  #
  # @note There may be no offset provided in case user wants to pause on the consecutive offset
  #   position. This can be beneficial when not wanting to purge the buffers.
  # @param event [Karafka::Core::Monitoring::Event] event details including payload
  #
  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#88
  def on_client_pause(event); end

  # Prints information about resuming of processing of a given topic partition
  #
  # @param event [Karafka::Core::Monitoring::Event] event details including payload
  #
  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#104
  def on_client_resume(event); end

  # Logs each messages fetching attempt
  #
  # @param event [Karafka::Core::Monitoring::Event] event details including payload
  #
  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#30
  def on_connection_listener_fetch_loop(event); end

  # Logs about messages that we've received from Kafka
  #
  # @param event [Karafka::Core::Monitoring::Event] event details including payload
  #
  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#40
  def on_connection_listener_fetch_loop_received(event); end

  # Prints info about retry of processing after an error
  #
  # @param event [Karafka::Core::Monitoring::Event] event details including payload
  #
  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#117
  def on_consumer_consuming_retry(event); end

  # Logs info when we have dispatched a message the the DLQ
  #
  # @param event [Karafka::Core::Monitoring::Event] event details including payload
  #
  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#193
  def on_dead_letter_queue_dispatched(event); end

  # There are many types of errors that can occur in many places, but we provide a single
  # handler for all of them to simplify error instrumentation.
  #
  # @param event [Karafka::Core::Monitoring::Event] event details including payload
  #
  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#272
  def on_error_occurred(event); end

  # @param event [Karafka::Core::Monitoring::Event] event details including payload
  #
  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#227
  def on_filtering_seek(event); end

  # Logs info about throttling event
  #
  # @param event [Karafka::Core::Monitoring::Event] event details including payload
  #
  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#211
  def on_filtering_throttled(event); end

  # Logs info about system signals that Karafka received and prints backtrace for threads in
  # case of ttin
  #
  # @param event [Karafka::Core::Monitoring::Event] event details including payload
  #
  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#134
  def on_process_notice_signal(event); end

  # @param event [Karafka::Core::Monitoring::Event] event details including payload
  #
  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#254
  def on_swarm_manager_before_fork(event); end

  # @param event [Karafka::Core::Monitoring::Event] event details including payload
  #
  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#264
  def on_swarm_manager_control(event); end

  # @param event [Karafka::Core::Monitoring::Event] event details including payload
  #
  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#242
  def on_swarm_manager_stopping(event); end

  # @param event [Karafka::Core::Monitoring::Event] event details including payload
  #
  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#248
  def on_swarm_manager_terminating(event); end

  # @param _event [Karafka::Core::Monitoring::Event] event details including payload
  #
  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#259
  def on_swarm_node_after_fork(_event); end

  # Prints info about the fact that a given job has started
  #
  # @param event [Karafka::Core::Monitoring::Event] event details including payload
  #
  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#58
  def on_worker_process(event); end

  # Prints info about the fact that a given job has finished
  #
  # @param event [Karafka::Core::Monitoring::Event] event details including payload
  #
  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#70
  def on_worker_processed(event); end

  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#341
  def warn(*args); end

  private

  # @return [Boolean] should we log polling
  #
  # source://karafka//lib/karafka/instrumentation/logger_listener.rb#349
  def log_polling?; end
end

# Log levels that we use in this particular listener
#
# source://karafka//lib/karafka/instrumentation/logger_listener.rb#9
Karafka::Instrumentation::LoggerListener::USED_LOG_LEVELS = T.let(T.unsafe(nil), Array)

# Karafka instrumentation monitor that we use to publish events
# By default uses our internal notifications bus but can be used with
# `ActiveSupport::Notifications` as well
#
# source://karafka//lib/karafka/instrumentation/monitor.rb#8
class Karafka::Instrumentation::Monitor < ::Karafka::Core::Monitoring::Monitor
  # @param notifications_bus [Object] either our internal notifications bus or
  #   `ActiveSupport::Notifications`
  # @param namespace [String, nil] namespace for events or nil if no namespace
  # @return [Monitor] a new instance of Monitor
  #
  # source://karafka//lib/karafka/instrumentation/monitor.rb#14
  def initialize(notifications_bus = T.unsafe(nil), namespace = T.unsafe(nil)); end

  # Returns the value of attribute notifications_bus.
  #
  # source://karafka//lib/karafka/instrumentation/monitor.rb#9
  def notifications_bus; end
end

# Monitor is used to hookup external monitoring services to monitor how Karafka works
# It provides a standardized API for checking incoming messages/enqueueing etc
# Since it is a pub-sub based on dry-monitor, you can use as many subscribers/loggers at the
# same time, which means that you might have for example file logging and NewRelic at the same
# time
#
# @note This class acts as a singleton because we are only permitted to have single monitor
#   per running process (just as logger)
#
# source://karafka//lib/karafka/instrumentation/notifications.rb#13
class Karafka::Instrumentation::Notifications < ::Karafka::Core::Monitoring::Notifications
  # @return [Karafka::Instrumentation::Monitor] monitor instance for system instrumentation
  #
  # source://karafka//lib/karafka/instrumentation/notifications.rb#103
  def initialize; end
end

# List of events that we support in the system and to which a monitor client can hook up
#
# @note The non-error once support timestamp benchmarking
# @note Depending on Karafka extensions and additional engines, this might not be the
#   complete list of all the events. Please use the #available_events on fully loaded
#   Karafka system to determine all of the events you can use.
#
# source://karafka//lib/karafka/instrumentation/notifications.rb#19
Karafka::Instrumentation::Notifications::EVENTS = T.let(T.unsafe(nil), Array)

# Listener that sets a proc title with a nice descriptive value
#
# source://karafka//lib/karafka/instrumentation/proctitle_listener.rb#6
class Karafka::Instrumentation::ProctitleListener
  # source://karafka//lib/karafka/instrumentation/proctitle_listener.rb#10
  def on_app_initialized(_event); end

  # source://karafka//lib/karafka/instrumentation/proctitle_listener.rb#10
  def on_app_initializing(_event); end

  # source://karafka//lib/karafka/instrumentation/proctitle_listener.rb#10
  def on_app_quiet(_event); end

  # source://karafka//lib/karafka/instrumentation/proctitle_listener.rb#10
  def on_app_quieting(_event); end

  # source://karafka//lib/karafka/instrumentation/proctitle_listener.rb#10
  def on_app_running(_event); end

  # source://karafka//lib/karafka/instrumentation/proctitle_listener.rb#10
  def on_app_stopped(_event); end

  # source://karafka//lib/karafka/instrumentation/proctitle_listener.rb#10
  def on_app_stopping(_event); end

  # source://karafka//lib/karafka/instrumentation/proctitle_listener.rb#10
  def on_app_supervising(_event); end

  # source://karafka//lib/karafka/instrumentation/proctitle_listener.rb#10
  def on_app_terminated(_event); end

  private

  # Sets a proper proc title with our constant prefix
  #
  # @param status [String] any status we want to set
  #
  # source://karafka//lib/karafka/instrumentation/proctitle_listener.rb#20
  def setproctitle(status); end
end

# source://karafka//lib/karafka/licenser.rb#5
class Karafka::Licenser
  class << self
    # source://karafka//lib/karafka/licenser.rb#13
    def detect; end

    # source://karafka//lib/karafka/licenser.rb#27
    def prepare_and_verify(license_config); end

    private

    # @param license_config [Karafka::Core::Configurable::Node] config related to the licensing
    #
    # source://karafka//lib/karafka/licenser.rb#38
    def prepare(license_config); end

    # Raises an error with info, that used token is invalid
    #
    # @param license_config [Karafka::Core::Configurable::Node]
    # @raise [Errors::InvalidLicenseTokenError]
    #
    # source://karafka//lib/karafka/licenser.rb#64
    def raise_invalid_license_token(license_config); end

    # source://karafka//lib/karafka/licenser.rb#44
    def verify(license_config); end
  end
end

# Location in the gem where we store the public key
#
# source://karafka//lib/karafka/licenser.rb#7
Karafka::Licenser::PUBLIC_KEY_LOCATION = T.let(T.unsafe(nil), String)

# Messages namespace encapsulating all the logic that is directly related to messages handling
#
# source://karafka//lib/karafka.rb#0
module Karafka::Messages; end

# Simple batch metadata object that stores all non-message information received from Kafka
# cluster while fetching the data.
#
# @note This metadata object refers to per batch metadata, not `#message.metadata`
#
# source://karafka//lib/karafka/messages/batch_metadata.rb#9
class Karafka::Messages::BatchMetadata < ::Struct
  # This lag describes how long did it take for a message to be consumed from the moment it was
  # created
  #
  # @note In case of usage in workless flows, this value will be set to -1
  # @return [Integer] number of milliseconds
  #
  # source://karafka//lib/karafka/messages/batch_metadata.rb#27
  def consumption_lag; end

  # Returns the value of attribute created_at
  #
  # @return [Object] the current value of created_at
  def created_at; end

  # Sets the attribute created_at
  #
  # @param value [Object] the value to set the attribute created_at to.
  # @return [Object] the newly set value
  def created_at=(_); end

  # Returns the value of attribute deserializers
  #
  # @return [Object] the current value of deserializers
  def deserializers; end

  # Sets the attribute deserializers
  #
  # @param value [Object] the value to set the attribute deserializers to.
  # @return [Object] the newly set value
  def deserializers=(_); end

  # Returns the value of attribute first_offset
  #
  # @return [Object] the current value of first_offset
  def first_offset; end

  # Sets the attribute first_offset
  #
  # @param value [Object] the value to set the attribute first_offset to.
  # @return [Object] the newly set value
  def first_offset=(_); end

  # Returns the value of attribute last_offset
  #
  # @return [Object] the current value of last_offset
  def last_offset; end

  # Sets the attribute last_offset
  #
  # @param value [Object] the value to set the attribute last_offset to.
  # @return [Object] the newly set value
  def last_offset=(_); end

  # Returns the value of attribute partition
  #
  # @return [Object] the current value of partition
  def partition; end

  # Sets the attribute partition
  #
  # @param value [Object] the value to set the attribute partition to.
  # @return [Object] the newly set value
  def partition=(_); end

  # Returns the value of attribute processed_at
  #
  # @return [Object] the current value of processed_at
  def processed_at; end

  # Sets the attribute processed_at
  #
  # @param value [Object] the value to set the attribute processed_at to.
  # @return [Object] the newly set value
  def processed_at=(_); end

  # This lag describes how long did a batch have to wait before it was picked up by one of the
  # workers
  #
  # @note In case of usage in workless flows, this value will be set to -1
  # @return [Integer] number of milliseconds
  #
  # source://karafka//lib/karafka/messages/batch_metadata.rb#36
  def processing_lag; end

  # Returns the value of attribute scheduled_at
  #
  # @return [Object] the current value of scheduled_at
  def scheduled_at; end

  # Sets the attribute scheduled_at
  #
  # @param value [Object] the value to set the attribute scheduled_at to.
  # @return [Object] the newly set value
  def scheduled_at=(_); end

  # Returns the value of attribute size
  #
  # @return [Object] the current value of size
  def size; end

  # Sets the attribute size
  #
  # @param value [Object] the value to set the attribute size to.
  # @return [Object] the newly set value
  def size=(_); end

  # Returns the value of attribute topic
  #
  # @return [Object] the current value of topic
  def topic; end

  # Sets the attribute topic
  #
  # @param value [Object] the value to set the attribute topic to.
  # @return [Object] the newly set value
  def topic=(_); end

  private

  # Computes time distance in between two times in ms
  #
  # @param time1 [Time]
  # @param time2 [Time]
  # @return [Integer] distance in between two times in ms
  #
  # source://karafka//lib/karafka/messages/batch_metadata.rb#47
  def time_distance_in_ms(time1, time2); end

  class << self
    def [](*_arg0); end
    def inspect; end
    def keyword_init?; end
    def members; end
    def new(*_arg0); end
  end
end

# Builders encapsulate logic related to creating messages related objects.
#
# source://karafka//lib/karafka.rb#0
module Karafka::Messages::Builders; end

# Builder for creating batch metadata object based on the batch informations.
#
# source://karafka//lib/karafka/messages/builders/batch_metadata.rb#7
module Karafka::Messages::Builders::BatchMetadata
  class << self
    # Creates metadata based on the kafka batch data.
    #
    # @note We do not set `processed_at` as this needs to be assigned when the batch is
    #   picked up for processing.
    # @param messages [Array<Karafka::Messages::Message>] messages array
    # @param topic [Karafka::Routing::Topic] topic for which we've fetched the batch
    # @param partition [Integer] partition of this metadata
    # @param scheduled_at [Time] moment when the batch was scheduled for processing
    # @return [Karafka::Messages::BatchMetadata] batch metadata object
    #
    # source://karafka//lib/karafka/messages/builders/batch_metadata.rb#19
    def call(messages, topic, partition, scheduled_at); end

    private

    # Code that aligns the batch creation at into our local time. If time of current machine
    # and the Kafka cluster drift, this helps not to allow this to leak into the framework.
    #
    # @note Message can be from the future in case consumer machine and Kafka cluster drift
    #   apart and the machine is behind the cluster.
    # @param last_message [::Karafka::Messages::Message, nil] last message from the batch or
    #   nil if no message
    # @return [Time] batch creation time. Now if no messages (workless flow) or the last
    #   message time as long as the message is not from the future
    #
    # source://karafka//lib/karafka/messages/builders/batch_metadata.rb#48
    def local_created_at(last_message); end
  end
end

# Builder of a single message based on raw rdkafka message.
#
# source://karafka//lib/karafka/messages/builders/message.rb#8
module Karafka::Messages::Builders::Message
  class << self
    # @param kafka_message [Rdkafka::Consumer::Message] raw fetched message
    # @param topic [Karafka::Routing::Topic] topic for which this message was fetched
    # @param received_at [Time] moment when we've received the message
    # @return [Karafka::Messages::Message] message object with payload and metadata
    #
    # source://karafka//lib/karafka/messages/builders/message.rb#14
    def call(kafka_message, topic, received_at); end
  end
end

# Builder for creating message batch instances.
#
# source://karafka//lib/karafka/messages/builders/messages.rb#7
module Karafka::Messages::Builders::Messages
  class << self
    # Creates messages batch with messages inside based on the incoming messages and the
    # topic from which it comes.
    #
    # @param messages [Array<Karafka::Messages::Message>] karafka messages array
    # @param topic [Karafka::Routing::Topic] topic for which we're received messages
    # @param partition [Integer] partition of those messages
    # @param received_at [Time] moment in time when the messages were received
    # @return [Karafka::Messages::Messages] messages batch object
    #
    # source://karafka//lib/karafka/messages/builders/messages.rb#17
    def call(messages, topic, partition, received_at); end
  end
end

# It provides lazy loading not only until the first usage, but also allows us to skip
# using deserializer until we execute our logic. That way we can operate with
# heavy-deserialization data without slowing down the whole application.
#
# source://karafka//lib/karafka/messages/message.rb#9
class Karafka::Messages::Message
  extend ::Forwardable

  # @param raw_payload [Object] incoming payload before deserialization
  # @param metadata [Karafka::Messages::Metadata] message metadata object
  # @return [Message] a new instance of Message
  #
  # source://karafka//lib/karafka/messages/message.rb#31
  def initialize(raw_payload, metadata); end

  # @return [Boolean] did we deserialize payload already
  #
  # source://karafka//lib/karafka/messages/message.rb#50
  def deserialized?; end

  # source://forwardable/1.3.3/forwardable.rb#231
  def deserializers(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def headers(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def key(*args, **_arg1, &block); end

  # Returns the value of attribute metadata.
  #
  # source://karafka//lib/karafka/messages/message.rb#21
  def metadata; end

  # source://forwardable/1.3.3/forwardable.rb#231
  def offset(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def partition(*args, **_arg1, &block); end

  # @return [Object] lazy-deserialized data (deserialized upon first request)
  #
  # source://karafka//lib/karafka/messages/message.rb#39
  def payload; end

  # source://forwardable/1.3.3/forwardable.rb#231
  def raw_headers(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def raw_key(*args, **_arg1, &block); end

  # raw payload needs to be mutable as we want to have option to change it in the parser
  # prior to the final deserialization
  #
  # source://karafka//lib/karafka/messages/message.rb#24
  def raw_payload; end

  # raw payload needs to be mutable as we want to have option to change it in the parser
  # prior to the final deserialization
  #
  # source://karafka//lib/karafka/messages/message.rb#24
  def raw_payload=(_arg0); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def received_at(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def timestamp(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def topic(*args, **_arg1, &block); end

  private

  # @return [Object] deserialized data
  #
  # source://karafka//lib/karafka/messages/message.rb#57
  def deserialize; end

  class << self
    # @note We cache it here for performance reasons. It is 2.5x times faster than getting it
    #   via the config chain.
    # @return [Object] general parser
    #
    # source://karafka//lib/karafka/messages/message.rb#16
    def parser; end
  end
end

# Messages batch represents a set of messages received from Kafka of a single topic partition.
#
# source://karafka//lib/karafka/messages/messages.rb#6
class Karafka::Messages::Messages
  include ::Enumerable

  # @param messages_array [Array<Karafka::Messages::Message>] array with karafka messages
  # @param metadata [Karafka::Messages::BatchMetadata]
  # @return [Karafka::Messages::Messages] lazy evaluated messages batch object
  #
  # source://karafka//lib/karafka/messages/messages.rb#14
  def initialize(messages_array, metadata); end

  # @return [Integer] number of messages in the batch
  #
  # source://karafka//lib/karafka/messages/messages.rb#59
  def count; end

  # Runs deserialization of all the messages and returns them
  #
  # @return [Array<Karafka::Messages::Message>]
  #
  # source://karafka//lib/karafka/messages/messages.rb#27
  def deserialize!; end

  # @note Invocation of this method will not cause loading and deserializing of messages.
  # @param block [Proc] block we want to execute per each message
  #
  # source://karafka//lib/karafka/messages/messages.rb#21
  def each(&block); end

  # @return [Boolean] is the messages batch empty
  #
  # source://karafka//lib/karafka/messages/messages.rb#44
  def empty?; end

  # @return [Karafka::Messages::Message] first message
  #
  # source://karafka//lib/karafka/messages/messages.rb#49
  def first; end

  # @return [Karafka::Messages::Message] last message
  #
  # source://karafka//lib/karafka/messages/messages.rb#54
  def last; end

  # Returns the value of attribute metadata.
  #
  # source://karafka//lib/karafka/messages/messages.rb#9
  def metadata; end

  # @return [Array<Object>] array with deserialized payloads. This method can be useful when
  #   we don't care about metadata and just want to extract all the data payloads from the
  #   batch
  #
  # source://karafka//lib/karafka/messages/messages.rb#34
  def payloads; end

  # @return [Array<String>] array with raw, not deserialized payloads
  #
  # source://karafka//lib/karafka/messages/messages.rb#39
  def raw_payloads; end

  # @return [Integer] number of messages in the batch
  #
  # source://karafka//lib/karafka/messages/messages.rb#59
  def size; end

  # @return [Array<Karafka::Messages::Message>] copy of the pure array with messages
  #
  # source://karafka//lib/karafka/messages/messages.rb#64
  def to_a; end
end

# Single message metadata details that can be accessed without the need of deserialization.
#
# source://karafka//lib/karafka/messages/metadata.rb#6
class Karafka::Messages::Metadata < ::Struct
  # Returns the value of attribute deserializers
  #
  # @return [Object] the current value of deserializers
  def deserializers; end

  # Sets the attribute deserializers
  #
  # @param value [Object] the value to set the attribute deserializers to.
  # @return [Object] the newly set value
  def deserializers=(_); end

  # @return [Object] deserialized headers. By default its a hash with keys and payload being
  #   strings
  #
  # source://karafka//lib/karafka/messages/metadata.rb#27
  def headers; end

  # @return [Object] deserialized key. By default in the raw string format.
  #
  # source://karafka//lib/karafka/messages/metadata.rb#19
  def key; end

  # Returns the value of attribute message
  #
  # @return [Object] the current value of message
  def message; end

  # Sets the attribute message
  #
  # @param value [Object] the value to set the attribute message to.
  # @return [Object] the newly set value
  def message=(_); end

  # Returns the value of attribute offset
  #
  # @return [Object] the current value of offset
  def offset; end

  # Sets the attribute offset
  #
  # @param value [Object] the value to set the attribute offset to.
  # @return [Object] the newly set value
  def offset=(_); end

  # Returns the value of attribute partition
  #
  # @return [Object] the current value of partition
  def partition; end

  # Sets the attribute partition
  #
  # @param value [Object] the value to set the attribute partition to.
  # @return [Object] the newly set value
  def partition=(_); end

  # Returns the value of attribute raw_headers
  #
  # @return [Object] the current value of raw_headers
  def raw_headers; end

  # Sets the attribute raw_headers
  #
  # @param value [Object] the value to set the attribute raw_headers to.
  # @return [Object] the newly set value
  def raw_headers=(_); end

  # Returns the value of attribute raw_key
  #
  # @return [Object] the current value of raw_key
  def raw_key; end

  # Sets the attribute raw_key
  #
  # @param value [Object] the value to set the attribute raw_key to.
  # @return [Object] the newly set value
  def raw_key=(_); end

  # Returns the value of attribute received_at
  #
  # @return [Object] the current value of received_at
  def received_at; end

  # Sets the attribute received_at
  #
  # @param value [Object] the value to set the attribute received_at to.
  # @return [Object] the newly set value
  def received_at=(_); end

  # Returns the value of attribute timestamp
  #
  # @return [Object] the current value of timestamp
  def timestamp; end

  # Sets the attribute timestamp
  #
  # @param value [Object] the value to set the attribute timestamp to.
  # @return [Object] the newly set value
  def timestamp=(_); end

  # Returns the value of attribute topic
  #
  # @return [Object] the current value of topic
  def topic; end

  # Sets the attribute topic
  #
  # @param value [Object] the value to set the attribute topic to.
  # @return [Object] the newly set value
  def topic=(_); end

  class << self
    def [](*_arg0); end
    def inspect; end
    def keyword_init?; end
    def members; end
    def new(*_arg0); end
  end
end

# Default message parser. The only thing it does, is calling the deserializer
#
# source://karafka//lib/karafka/messages/parser.rb#6
class Karafka::Messages::Parser
  # @param message [::Karafka::Messages::Message]
  # @return [Object] deserialized payload
  #
  # source://karafka//lib/karafka/messages/parser.rb#9
  def call(message); end
end

# "Fake" message that we use as an abstraction layer when seeking back.
# This allows us to encapsulate a seek with a simple abstraction
#
# (first equal or greater)
#
# @note `#offset` can be either the offset value or the time of the offset
#
# source://karafka//lib/karafka/messages/seek.rb#10
class Karafka::Messages::Seek < ::Struct
  # Returns the value of attribute offset
  #
  # @return [Object] the current value of offset
  def offset; end

  # Sets the attribute offset
  #
  # @param value [Object] the value to set the attribute offset to.
  # @return [Object] the newly set value
  def offset=(_); end

  # Returns the value of attribute partition
  #
  # @return [Object] the current value of partition
  def partition; end

  # Sets the attribute partition
  #
  # @param value [Object] the value to set the attribute partition to.
  # @return [Object] the newly set value
  def partition=(_); end

  # Returns the value of attribute topic
  #
  # @return [Object] the current value of topic
  def topic; end

  # Sets the attribute topic
  #
  # @param value [Object] the value to set the attribute topic to.
  # @return [Object] the newly set value
  def topic=(_); end

  class << self
    def [](*_arg0); end
    def inspect; end
    def keyword_init?; end
    def members; end
    def new(*_arg0); end
  end
end

# Namespace for third-party libraries patches
#
# source://karafka//lib/karafka.rb#0
module Karafka::Patches; end

# Rdkafka patches specific to Karafka
#
# source://karafka//lib/karafka.rb#0
module Karafka::Patches::Rdkafka; end

# Binding patches that slightly change how rdkafka operates in certain places
#
# source://karafka//lib/karafka/patches/rdkafka/bindings.rb#9
module Karafka::Patches::Rdkafka::Bindings
  include ::Karafka::Core::Patches::Rdkafka::Bindings
  include ::Rdkafka::Bindings

  class << self
    # Handle assignments on cooperative rebalance
    #
    # @param client_ptr [FFI::Pointer]
    # @param code [Integer]
    # @param partitions_ptr [FFI::Pointer]
    # @param tpl [Rdkafka::Consumer::TopicPartitionList]
    # @param opaque [Rdkafka::Opaque]
    #
    # source://karafka//lib/karafka/patches/rdkafka/bindings.rb#23
    def on_cooperative_rebalance(client_ptr, code, partitions_ptr, tpl, opaque); end

    # Handle assignments on a eager rebalance
    #
    # @param client_ptr [FFI::Pointer]
    # @param code [Integer]
    # @param partitions_ptr [FFI::Pointer]
    # @param tpl [Rdkafka::Consumer::TopicPartitionList]
    # @param opaque [Rdkafka::Opaque]
    #
    # source://karafka//lib/karafka/patches/rdkafka/bindings.rb#48
    def on_eager_rebalance(client_ptr, code, partitions_ptr, tpl, opaque); end
  end
end

# Alias internally
#
# source://karafka//lib/karafka/patches/rdkafka/bindings.rb#13
Karafka::Patches::Rdkafka::Bindings::RB = Rdkafka::Bindings

# This patch changes few things:
# - it commits offsets (if any) upon partition revocation, so less jobs need to be
#   reprocessed if they are assigned to a different process
# - reports callback errors into the errors instrumentation instead of the logger
# - catches only StandardError instead of Exception as we fully control the directly
#   executed callbacks
#
# @see https://docs.confluent.io/2.0.0/clients/librdkafka/classRdKafka_1_1RebalanceCb.html
#
# source://karafka//lib/karafka/patches/rdkafka/bindings.rb#75
Karafka::Patches::Rdkafka::Bindings::RebalanceCallback = T.let(T.unsafe(nil), FFI::Function)

# Patches allowing us to run events on both pre and post rebalance events.
# Thanks to that, we can easily connect to the whole flow despite of the moment when things
# are happening
#
# source://karafka//lib/karafka/patches/rdkafka/opaque.rb#9
module Karafka::Patches::Rdkafka::Opaque
  # Handles pre-assign phase of rebalance
  #
  # @param tpl [Rdkafka::Consumer::TopicPartitionList]
  #
  # source://karafka//lib/karafka/patches/rdkafka/opaque.rb#13
  def call_on_partitions_assign(tpl); end

  # Handles pre-revoke phase of rebalance
  #
  # @param tpl [Rdkafka::Consumer::TopicPartitionList]
  #
  # source://karafka//lib/karafka/patches/rdkafka/opaque.rb#23
  def call_on_partitions_revoke(tpl); end
end

# source://karafka//lib/karafka/pro.rb#11
module Karafka::Pro; end

# Class used to catch signals from ruby Signal class in order to manage Karafka stop
#
# @note There might be only one process - this class is a singleton
#
# source://karafka//lib/karafka/process.rb#6
class Karafka::Process
  extend ::Karafka::Core::Taggable

  # Creates an instance of process and creates empty hash for callbacks
  #
  # @return [Process] a new instance of Process
  #
  # source://karafka//lib/karafka/process.rb#50
  def initialize; end

  # Clears all the defined callbacks. Useful for post-fork cleanup when parent already defined
  # some signals
  #
  # source://karafka//lib/karafka/process.rb#57
  def clear; end

  # Assigns a callback that will run on any supported signal that has at least one callback
  # registered already.
  #
  # @note This will only bind to signals that already have at least one callback defined
  # @param block [Proc] code we want to run
  #
  # source://karafka//lib/karafka/process.rb#41
  def on_any_active(&block); end

  # source://karafka//lib/karafka/process.rb#31
  def on_sigchld(&block); end

  # source://karafka//lib/karafka/process.rb#31
  def on_sigint(&block); end

  # source://karafka//lib/karafka/process.rb#31
  def on_sigquit(&block); end

  # source://karafka//lib/karafka/process.rb#31
  def on_sigterm(&block); end

  # source://karafka//lib/karafka/process.rb#31
  def on_sigtstp(&block); end

  # source://karafka//lib/karafka/process.rb#31
  def on_sigttin(&block); end

  # source://karafka//lib/karafka/process.rb#31
  def on_siguser1(&block); end

  # Method catches all HANDLED_SIGNALS and performs appropriate callbacks (if defined)
  #
  # @note If there are no callbacks, this method will just ignore a given signal that was sent
  #
  # source://karafka//lib/karafka/process.rb#63
  def supervise; end

  # Is the current process supervised and are trap signals installed
  #
  # @return [Boolean]
  #
  # source://karafka//lib/karafka/process.rb#75
  def supervised?; end

  private

  # Informs monitoring about trapped signal
  #
  # @param signal [Symbol] type that we received
  #
  # source://karafka//lib/karafka/process.rb#100
  def notice_signal(signal); end

  # Traps a single signal and performs callbacks (if any) or just ignores this signal
  # trap context s some things may not work there as expected, that is why we spawn a separate
  # thread to handle the signals process
  #
  # @note Since we do a lot of threading and queuing, we don't want to handle signals from the
  # @param signal [Symbol] type that we want to catch
  #
  # source://karafka//lib/karafka/process.rb#86
  def trap_signal(signal); end
end

# Signal types that we handle
#
# source://karafka//lib/karafka/process.rb#11
Karafka::Process::HANDLED_SIGNALS = T.let(T.unsafe(nil), Array)

# Namespace that encapsulates all the logic related to processing data.
#
# source://karafka//lib/karafka.rb#0
module Karafka::Processing; end

# Basic coordinator that allows us to provide coordination objects into consumers.
#
# This is a wrapping layer to simplify management of work to be handled around consumption.
#
# @note This coordinator needs to be thread safe. Some operations are performed only in the
#   listener thread, but we go with thread-safe by default for all not to worry about potential
#   future mistakes.
#
# source://karafka//lib/karafka/processing/coordinator.rb#12
class Karafka::Processing::Coordinator
  include ::Karafka::Core::Helpers::Time
  extend ::Forwardable

  # @param topic [Karafka::Routing::Topic]
  # @param partition [Integer]
  # @param pause_tracker [Karafka::TimeTrackers::Pause] pause tracker for given topic partition
  # @return [Coordinator] a new instance of Coordinator
  #
  # source://karafka//lib/karafka/processing/coordinator.rb#23
  def initialize(topic, partition, pause_tracker); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def attempt(*args, **_arg1, &block); end

  # @param consumer [Object] karafka consumer (normal or pro)
  # @return [Karafka::Processing::Result] result object which we can use to indicate
  #   consumption processing state.
  #
  # source://karafka//lib/karafka/processing/coordinator.rb#181
  def consumption(consumer); end

  # Decrements number of jobs we handle at the moment
  #
  # @param job_type [Symbol] type of job that we want to decrement
  #
  # source://karafka//lib/karafka/processing/coordinator.rb#85
  def decrement(job_type); end

  # Mark given consumption on consumer as failed
  #
  # @param consumer [Karafka::BaseConsumer] consumer that failed
  # @param error [StandardError] error that occurred
  #
  # source://karafka//lib/karafka/processing/coordinator.rb#119
  def failure!(consumer, error); end

  # @return [Boolean] true if any of work we were running failed
  #
  # source://karafka//lib/karafka/processing/coordinator.rb#127
  def failure?; end

  # Increases number of jobs that we handle with this coordinator
  #
  # @param job_type [Symbol] type of job that we want to increment
  #
  # source://karafka//lib/karafka/processing/coordinator.rb#76
  def increment(job_type); end

  # Store in the coordinator info, that this pause was done manually by the end user and not
  # by the system itself
  #
  # source://karafka//lib/karafka/processing/coordinator.rb#159
  def manual_pause; end

  # @return [Boolean] are we in a pause that was initiated by the user
  #
  # source://karafka//lib/karafka/processing/coordinator.rb#164
  def manual_pause?; end

  # Marks seek as manual for coordination purposes
  #
  # source://karafka//lib/karafka/processing/coordinator.rb#169
  def manual_seek; end

  # @return [Boolean] did a user invoke seek in the current operations scope
  #
  # source://karafka//lib/karafka/processing/coordinator.rb#174
  def manual_seek?; end

  # @return [Boolean] was the new seek offset assigned at least once. This is needed because
  #   by default we assign seek offset of a first message ever, however this is insufficient
  #   for DLQ in a scenario where the first message would be broken. We would never move
  #   out of it and would end up in an endless loop.
  #
  # source://karafka//lib/karafka/processing/coordinator.rb#153
  def marked?; end

  # Returns the value of attribute partition.
  #
  # source://karafka//lib/karafka/processing/coordinator.rb#16
  def partition; end

  # Returns the value of attribute pause_tracker.
  #
  # source://karafka//lib/karafka/processing/coordinator.rb#16
  def pause_tracker; end

  # source://forwardable/1.3.3/forwardable.rb#231
  def paused?(*args, **_arg1, &block); end

  # Marks given coordinator for processing group as revoked
  #
  # This is invoked in two places:
  #   - from the main listener loop when we detect revoked partitions
  #   - from the consumer in case checkpointing fails
  #
  # This means, we can end up having consumer being aware that it was revoked prior to the
  # listener loop dispatching the revocation job. It is ok, as effectively nothing will be
  # processed until revocation jobs are done.
  #
  # source://karafka//lib/karafka/processing/coordinator.rb#140
  def revoke; end

  # @return [Boolean] is the partition we are processing revoked or not
  #
  # source://karafka//lib/karafka/processing/coordinator.rb#145
  def revoked?; end

  # Returns the value of attribute seek_offset.
  #
  # source://karafka//lib/karafka/processing/coordinator.rb#16
  def seek_offset; end

  # @param offset [Integer] message offset
  #
  # source://karafka//lib/karafka/processing/coordinator.rb#67
  def seek_offset=(offset); end

  # Starts the coordinator for given consumption jobs
  #
  # @param messages [Array<Karafka::Messages::Message>] batch of message for which we are
  #   going to coordinate work. Not used with regular coordinator.
  #
  # source://karafka//lib/karafka/processing/coordinator.rb#41
  def start(messages); end

  # Mark given consumption on consumer as successful
  #
  # @param consumer [Karafka::BaseConsumer] consumer that finished successfully
  #
  # source://karafka//lib/karafka/processing/coordinator.rb#110
  def success!(consumer); end

  # Is all the consumption done and finished successfully for this coordinator
  # We do not say we're successful until all work is done, because running work may still
  # crash.
  #
  # @note This is only used for consume synchronization
  # @return [Boolean]
  #
  # source://karafka//lib/karafka/processing/coordinator.rb#102
  def success?; end

  # Allows to run synchronized (locked) code that can operate only from a given thread
  #
  # @note We check if mutex is not owned already by the current thread so we won't end up with
  #   a deadlock in case user runs coordinated code from inside of his own lock
  # @note This is internal and should **not** be used to synchronize user-facing code.
  #   Otherwise user indirectly could cause deadlocks or prolonged locks by running his logic.
  #   This can and should however be used for multi-thread strategy applications and other
  #   internal operations locks.
  # @param block [Proc] code we want to run in the synchronized mode
  #
  # source://karafka//lib/karafka/processing/coordinator.rb#196
  def synchronize(&block); end

  # Returns the value of attribute topic.
  #
  # source://karafka//lib/karafka/processing/coordinator.rb#16
  def topic; end
end

# Coordinators builder used to build coordinators per topic partition
#
# It provides direct pauses access for revocation
#
# @note This buffer operates only from the listener loop, thus we do not have to make it
#   thread-safe.
#
# source://karafka//lib/karafka/processing/coordinators_buffer.rb#11
class Karafka::Processing::CoordinatorsBuffer
  # @param topics [Karafka::Routing::Topics]
  # @return [CoordinatorsBuffer] a new instance of CoordinatorsBuffer
  #
  # source://karafka//lib/karafka/processing/coordinators_buffer.rb#17
  def initialize(topics); end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def coordinator_class; end

  # @param topic_name [String] topic name
  # @param partition [Integer] partition number
  #
  # source://karafka//lib/karafka/processing/coordinators_buffer.rb#25
  def find_or_create(topic_name, partition); end

  # Clears coordinators and re-created the pauses manager
  # This should be used only for critical errors recovery
  #
  # source://karafka//lib/karafka/processing/coordinators_buffer.rb#59
  def reset; end

  # Resumes processing of partitions for which pause time has ended.
  #
  # @param block we want to run for resumed topic partitions
  # @yieldparam topic [String] name
  # @yieldparam partition [Integer] number
  #
  # source://karafka//lib/karafka/processing/coordinators_buffer.rb#41
  def resume(&block); end

  # @param topic_name [String] topic name
  # @param partition [Integer] partition number
  #
  # source://karafka//lib/karafka/processing/coordinators_buffer.rb#47
  def revoke(topic_name, partition); end
end

# Executors:
# - run consumers code (for `#call`) or run given preparation / teardown operations when needed
#   from separate threads.
# - they re-create consumer instances in case of partitions that were revoked and assigned
#   back.
#
# @note Executors are not removed after partition is revoked. They are not that big and will
#   be re-used in case of a re-claim
# @note Since given consumer can run various operations, executor manages that and its
#   lifecycle. There are following types of operations with appropriate before/after, etc:
#
#   - consume - primary operation related to running user consumption code
#   - idle - cleanup job that runs on idle runs where no messages would be passed to the end
#   user. This is used for complex flows with filters, etc
#   - revoked - runs after the partition was revoked
#   - shutdown - runs when process is going to shutdown
#
# source://karafka//lib/karafka/processing/executor.rb#23
class Karafka::Processing::Executor
  extend ::Forwardable

  # @param group_id [String] id of the subscription group to which the executor belongs
  # @param client [Karafka::Connection::Client] kafka client
  # @param coordinator [Karafka::Processing::Coordinator]
  # @return [Executor] a new instance of Executor
  #
  # source://karafka//lib/karafka/processing/executor.rb#47
  def initialize(group_id, client, coordinator); end

  # Runs consumer after consumption code
  #
  # source://karafka//lib/karafka/processing/executor.rb#91
  def after_consume; end

  # Runs setup and warm-up code in the worker prior to running the consumption
  #
  # source://karafka//lib/karafka/processing/executor.rb#80
  def before_consume; end

  # Allows us to prepare the consumer in the listener thread prior to the job being send to
  # be scheduled. It also allows to run some code that is time sensitive and cannot wait in the
  # queue as it could cause starvation.
  #
  # @param messages [Array<Karafka::Messages::Message>]
  #
  # source://karafka//lib/karafka/processing/executor.rb#59
  def before_schedule_consume(messages); end

  # Runs the code needed before idle work is scheduled
  #
  # source://karafka//lib/karafka/processing/executor.rb#96
  def before_schedule_idle; end

  # Runs code needed before revoked job is scheduled
  #
  # source://karafka//lib/karafka/processing/executor.rb#108
  def before_schedule_revoked; end

  # Runs code needed before shutdown job is scheduled
  #
  # source://karafka//lib/karafka/processing/executor.rb#129
  def before_schedule_shutdown; end

  # Runs consumer data processing against given batch and handles failures and errors.
  #
  # source://karafka//lib/karafka/processing/executor.rb#85
  def consume; end

  # @return [Karafka::Processing::Coordinator] coordinator for this executor
  #
  # source://karafka//lib/karafka/processing/executor.rb#42
  def coordinator; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def expansions_selector; end

  # @return [String] subscription group id to which a given executor belongs
  #
  # source://karafka//lib/karafka/processing/executor.rb#36
  def group_id; end

  # @return [String] unique id that we use to ensure, that we use for state tracking
  #
  # source://karafka//lib/karafka/processing/executor.rb#33
  def id; end

  # Runs consumer idle operations
  # This may include house-keeping or other state management changes that can occur but that
  # not mean there are any new messages available for the end user to process
  #
  # source://karafka//lib/karafka/processing/executor.rb#103
  def idle; end

  # @return [Karafka::Messages::Messages] messages batch
  #
  # source://karafka//lib/karafka/processing/executor.rb#39
  def messages; end

  # source://forwardable/1.3.3/forwardable.rb#231
  def partition(*args, **_arg1, &block); end

  # Runs the controller `#revoked` method that should be triggered when a given consumer is
  # no longer needed due to partitions reassignment.
  #
  # @note Clearing the consumer will ensure, that if we get the partition back, it will be
  #   handled with a consumer with a clean state.
  # @note We run it only when consumer was present, because presence indicates, that at least
  #   a single message has been consumed.
  # @note We do not reset the consumer but we indicate need for recreation instead, because
  #   after the revocation, there still may be `#after_consume` running that needs a given
  #   consumer instance.
  #
  # source://karafka//lib/karafka/processing/executor.rb#124
  def revoked; end

  # Runs the controller `#shutdown` method that should be triggered when a given consumer is
  # no longer needed as we're closing the process.
  #
  # @note While we do not need to clear the consumer here, it's a good habit to clean after
  #   work is done.
  #
  # source://karafka//lib/karafka/processing/executor.rb#138
  def shutdown; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def strategy_selector; end

  # source://forwardable/1.3.3/forwardable.rb#231
  def topic(*args, **_arg1, &block); end

  private

  # @return [Object] cached consumer instance
  #
  # source://karafka//lib/karafka/processing/executor.rb#147
  def consumer; end

  # Initializes the messages set in case given operation would happen before any processing
  # This prevents us from having no messages object at all as the messages object and
  # its metadata may be used for statistics
  #
  # source://karafka//lib/karafka/processing/executor.rb#179
  def empty_messages; end
end

# Buffer for executors of a given subscription group. It wraps around the concept of building
# and caching them, so we can re-use them instead of creating new each time.
#
# source://karafka//lib/karafka/processing/executors_buffer.rb#7
class Karafka::Processing::ExecutorsBuffer
  # @param client [Connection::Client]
  # @param subscription_group [Routing::SubscriptionGroup]
  # @return [ExecutorsBuffer]
  #
  # source://karafka//lib/karafka/processing/executors_buffer.rb#15
  def initialize(client, subscription_group); end

  # Clears the executors buffer. Useful for critical errors recovery.
  #
  # source://karafka//lib/karafka/processing/executors_buffer.rb#85
  def clear; end

  # Iterates over all available executors and yields them together with topic and partition
  # info
  #
  # @yieldparam karafka [Routing::Topic] routing topic object
  # @yieldparam partition [Integer] number
  # @yieldparam given [Executor, Pro::Processing::Executor] executor
  #
  # source://karafka//lib/karafka/processing/executors_buffer.rb#74
  def each; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def executor_class; end

  # Finds all the executors available for a given topic partition
  #
  # @param topic [String] topic name
  # @param partition [Integer] partition number
  # @return [Array<Executor, Pro::Processing::Executor>] executors in use for this
  #   topic + partition
  #
  # source://karafka//lib/karafka/processing/executors_buffer.rb#65
  def find_all(topic, partition); end

  # Finds all existing executors for given topic partition or creates one for it
  #
  # @param topic [String] topic name
  # @param partition [Integer] partition number
  # @param coordinator [Karafka::Processing::Coordinator]
  # @return [Array<Executor, Pro::Processing::Executor>]
  #
  # source://karafka//lib/karafka/processing/executors_buffer.rb#42
  def find_all_or_create(topic, partition, coordinator); end

  # Finds or creates an executor based on the provided details
  #
  # @param topic [String] topic name
  # @param partition [Integer] partition number
  # @param parallel_key [String] parallel group key
  # @param coordinator [Karafka::Processing::Coordinator]
  # @return [Executor, Pro::Processing::Executor] consumer executor
  #
  # source://karafka//lib/karafka/processing/executors_buffer.rb#29
  def find_or_create(topic, partition, parallel_key, coordinator); end

  # Revokes executors of a given topic partition, so they won't be used anymore for incoming
  # messages
  #
  # @param topic [String] topic name
  # @param partition [Integer] partition number
  #
  # source://karafka//lib/karafka/processing/executors_buffer.rb#55
  def revoke(topic, partition); end
end

# Selector of appropriate topic setup based features enhancements.
#
# Those expansions to the consumer API are NOT about the flow of processing. For this we have
# strategies. Those are suppose to provide certain extra APIs that user can use to get some
# extra non-flow related functionalities.
#
# source://karafka//lib/karafka/processing/expansions_selector.rb#10
class Karafka::Processing::ExpansionsSelector
  # @param topic [Karafka::Routing::Topic] topic with settings based on which we find
  #   expansions
  # @return [Array<Module>] modules with proper expansions we're suppose to use to enhance the
  #   consumer
  #
  # source://karafka//lib/karafka/processing/expansions_selector.rb#15
  def find(topic); end
end

# Namespace of the Inline Insights feature "non routing" related components
#
# @note We use both `#insights` because it is the feature name but also `#statistics` to make
#   it consistent with the fact that we publish and operate on statistics. User can pick
#   whichever name they prefer.
#
# source://karafka//lib/karafka.rb#0
module Karafka::Processing::InlineInsights; end

# Module that adds extra methods to the consumer that allow us to fetch the insights
#
# source://karafka//lib/karafka/processing/inline_insights/consumer.rb#12
module Karafka::Processing::InlineInsights::Consumer
  # @note We cache insights on the consumer, as in some scenarios we may no longer have them
  #   inside the Tracker, for example under involuntary revocation, incoming statistics may
  #   no longer have lost partition insights. Since we want to be consistent during single
  #   batch operations, we want to ensure, that if we have insights they are available
  #   throughout the whole processing.
  # @return [Hash] empty hash or hash with given partition insights if already present
  #
  # source://karafka//lib/karafka/processing/inline_insights/consumer.rb#19
  def inline_insights; end

  # @return [Boolean] true if there are insights to work with, otherwise false
  #
  # source://karafka//lib/karafka/processing/inline_insights/consumer.rb#32
  def inline_insights?; end

  # @note We cache insights on the consumer, as in some scenarios we may no longer have them
  #   inside the Tracker, for example under involuntary revocation, incoming statistics may
  #   no longer have lost partition insights. Since we want to be consistent during single
  #   batch operations, we want to ensure, that if we have insights they are available
  #   throughout the whole processing.
  # @return [Hash] empty hash or hash with given partition insights if already present
  #
  # source://karafka//lib/karafka/processing/inline_insights/consumer.rb#19
  def insights; end

  # @return [Boolean] true if there are insights to work with, otherwise false
  #
  # source://karafka//lib/karafka/processing/inline_insights/consumer.rb#32
  def insights?; end

  # @note We cache insights on the consumer, as in some scenarios we may no longer have them
  #   inside the Tracker, for example under involuntary revocation, incoming statistics may
  #   no longer have lost partition insights. Since we want to be consistent during single
  #   batch operations, we want to ensure, that if we have insights they are available
  #   throughout the whole processing.
  # @return [Hash] empty hash or hash with given partition insights if already present
  #
  # source://karafka//lib/karafka/processing/inline_insights/consumer.rb#19
  def statistics; end

  # @return [Boolean] true if there are insights to work with, otherwise false
  #
  # source://karafka//lib/karafka/processing/inline_insights/consumer.rb#32
  def statistics?; end
end

# Listener that adds statistics to our inline tracker
#
# source://karafka//lib/karafka/processing/inline_insights/listener.rb#7
class Karafka::Processing::InlineInsights::Listener
  # Adds statistics to the tracker
  #
  # @param event [Karafka::Core::Monitoring::Event] event with statistics
  #
  # source://karafka//lib/karafka/processing/inline_insights/listener.rb#10
  def on_statistics_emitted(event); end
end

# Object used to track statistics coming from librdkafka in a way that can be accessible by
# the consumers
#
# We use a single tracker because we do not need state management here as our consumer groups
# clients identified by statistics name value are unique. On top of that, having a per
# process one that is a singleton allows us to use tracker easily also from other places like
# filtering API etc.
#
# @note We include cache of 5 minutes for revoked partitions to compensate for cases where
#   when using LRJ a lost partition data would not be present anymore, however we would still
#   be in the processing phase. Since those metrics are published with each `poll`, regular
#   processing is not a subject of this issue. For LRJ we keep the reference. The only case
#   where this could be switched midway is when LRJ is running for an extended period of time
#   after the involuntary revocation. Having a time based cache instead of tracking
#   simplifies the design as we do not have to deal with state tracking, especially since
#   we would have to track also operations running in a revoked state.
# @note This tracker keeps in memory data about all topics and partitions that it encounters
#   because in case of routing patterns, we may start getting statistics prior to registering
#   given topic via dynamic routing expansions. In such case we would not have insights
#   where they were actually available for us to use.
# @note Memory usage is negligible as long as we can evict expired data. Single metrics set
#   for a single partition contains around 4KB of data. This means, that in case of an
#   assignment of 1000 partitions, we use around 4MB of space for tracking those metrics.
#
# source://karafka//lib/karafka/processing/inline_insights/tracker.rb#31
class Karafka::Processing::InlineInsights::Tracker
  include ::Singleton
  include ::Karafka::Core::Helpers::Time
  extend ::Singleton::SingletonClassMethods

  # @return [Tracker] a new instance of Tracker
  #
  # source://karafka//lib/karafka/processing/inline_insights/tracker.rb#54
  def initialize; end

  # Adds each partition statistics into internal accumulator. Single statistics set may
  # contain data from multiple topics and their partitions because a single client can
  # operate on multiple topics and partitions.
  #
  # We iterate over those topics and partitions and store topics partitions data only.
  #
  # @param consumer_group_id [String] id of the consumer group for which statistics were
  #   emitted.
  # @param statistics [Hash] librdkafka enriched statistics
  #
  # source://karafka//lib/karafka/processing/inline_insights/tracker.rb#68
  def add(consumer_group_id, statistics); end

  # Clears the tracker
  #
  # source://karafka//lib/karafka/processing/inline_insights/tracker.rb#98
  def clear; end

  # Finds statistics about requested consumer group topic partition
  #
  # @note We do not enclose it with a mutex mainly because the only thing that could happen
  #   here that would be a race-condition is a miss that anyhow we need to support due to
  #   how librdkafka ships metrics and a potential removal of data on heavily revoked LRJ.
  # @param topic [Karafka::Routing::Topic]
  # @param partition [Integer]
  # @return [Hash] hash with given topic partition statistics or empty hash if not present
  #
  # source://karafka//lib/karafka/processing/inline_insights/tracker.rb#92
  def find(topic, partition); end

  private

  # Evicts expired data from the cache
  #
  # source://karafka//lib/karafka/processing/inline_insights/tracker.rb#105
  def evict; end

  # Should we track given partition
  #
  # We do not track stopped partitions and the once we do not work with actively
  #
  # @param partition_id [String] partition id as a string
  # @param p_details [Hash] partition statistics details
  # @return [Boolean] true if we should track given partition
  #
  # source://karafka//lib/karafka/processing/inline_insights/tracker.rb#115
  def track?(partition_id, p_details); end

  class << self
    # source://forwardable/1.3.3/forwardable.rb#231
    def add(*args, **_arg1, &block); end

    # source://forwardable/1.3.3/forwardable.rb#231
    def clear(*args, **_arg1, &block); end

    # source://forwardable/1.3.3/forwardable.rb#231
    def exists?(*args, **_arg1, &block); end

    # source://forwardable/1.3.3/forwardable.rb#231
    def find(*args, **_arg1, &block); end

    private

    def allocate; end
    def new(*_arg0); end
  end
end

# Empty array to save on memory allocations.
#
# source://karafka//lib/karafka/processing/inline_insights/tracker.rb#40
Karafka::Processing::InlineInsights::Tracker::EMPTY_ARRAY = T.let(T.unsafe(nil), Array)

# Empty hash we want to return in any case where we could not locate appropriate topic
# partition statistics.
#
# source://karafka//lib/karafka/processing/inline_insights/tracker.rb#37
Karafka::Processing::InlineInsights::Tracker::EMPTY_HASH = T.let(T.unsafe(nil), Hash)

# 5 minutes of cache. We cache last result per consumer group topic partition so we are
# not affected by involuntary rebalances during LRJ execution.
#
# source://karafka//lib/karafka/processing/inline_insights/tracker.rb#44
Karafka::Processing::InlineInsights::Tracker::TTL = T.let(T.unsafe(nil), Integer)

# Namespace for all the jobs that are supposed to run in workers.
#
# source://karafka//lib/karafka.rb#0
module Karafka::Processing::Jobs; end

# Base class for all the jobs types that are suppose to run in workers threads.
# Each job can have 3 main entry-points: `#before_call`, `#call` and `#after_call`
# Only `#call` is required.
#
# source://karafka//lib/karafka/processing/jobs/base.rb#10
class Karafka::Processing::Jobs::Base
  extend ::Forwardable

  # Creates a new job instance
  #
  # @return [Base] a new instance of Base
  #
  # source://karafka//lib/karafka/processing/jobs/base.rb#19
  def initialize; end

  # When redefined can run any code that should run after executing the proper code
  #
  # source://karafka//lib/karafka/processing/jobs/base.rb#41
  def after_call; end

  # When redefined can run any code that should run before executing the proper code
  #
  # source://karafka//lib/karafka/processing/jobs/base.rb#33
  def before_call; end

  # When redefined can run any code prior to the job being scheduled
  #
  # @note This will run in the listener thread and not in the worker
  # @raise [NotImplementedError]
  #
  # source://karafka//lib/karafka/processing/jobs/base.rb#28
  def before_schedule; end

  # The main entry-point of a job
  #
  # @raise [NotImplementedError]
  #
  # source://karafka//lib/karafka/processing/jobs/base.rb#36
  def call; end

  # Returns the value of attribute executor.
  #
  # source://karafka//lib/karafka/processing/jobs/base.rb#16
  def executor; end

  # Marks the job as finished. Used by the worker to indicate, that this job is done.
  #
  # @note Since the scheduler knows exactly when it schedules jobs and when it keeps them
  #   pending, we do not need advanced state tracking and the only information from the
  #   "outside" is whether it was finished or not after it was scheduled for execution.
  #
  # source://karafka//lib/karafka/processing/jobs/base.rb#66
  def finish!; end

  # @return [Boolean] was this job finished.
  #
  # source://karafka//lib/karafka/processing/jobs/base.rb#57
  def finished?; end

  # source://forwardable/1.3.3/forwardable.rb#231
  def group_id(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def id(*args, **_arg1, &block); end

  # @note Blocking job is a job, that will cause the job queue to wait until it is finished
  #   before removing the lock on new jobs being added
  # @note All the jobs are blocking by default
  # @note Job **needs** to mark itself as non-blocking only **after** it is done with all
  #   the blocking things (pausing partition, etc).
  # @return [Boolean] is this a non-blocking job
  #
  # source://karafka//lib/karafka/processing/jobs/base.rb#52
  def non_blocking?; end
end

# The main job type. It runs the executor that triggers given topic partition messages
# processing in an underlying consumer instance.
#
# source://karafka//lib/karafka/processing/jobs/consume.rb#8
class Karafka::Processing::Jobs::Consume < ::Karafka::Processing::Jobs::Base
  # @param executor [Karafka::Processing::Executor] executor that is suppose to run a given
  #   job
  # @param messages [Karafka::Messages::Messages] karafka messages batch
  # @return [Consume]
  #
  # source://karafka//lib/karafka/processing/jobs/consume.rb#16
  def initialize(executor, messages); end

  # Runs any error handling and other post-consumption stuff on the executor
  #
  # source://karafka//lib/karafka/processing/jobs/consume.rb#39
  def after_call; end

  # Runs the before consumption preparations on the executor
  #
  # source://karafka//lib/karafka/processing/jobs/consume.rb#29
  def before_call; end

  # Runs all the preparation code on the executor that needs to happen before the job is
  # scheduled.
  #
  # source://karafka//lib/karafka/processing/jobs/consume.rb#24
  def before_schedule; end

  # Runs the given executor
  #
  # source://karafka//lib/karafka/processing/jobs/consume.rb#34
  def call; end

  # @return [Array<Rdkafka::Consumer::Message>] array with messages
  #
  # source://karafka//lib/karafka/processing/jobs/consume.rb#10
  def messages; end
end

# Type of job that we may use to run some extra handling that happens without the user
# related lifecycle event like consumption, revocation, etc.
#
# source://karafka//lib/karafka/processing/jobs/idle.rb#8
class Karafka::Processing::Jobs::Idle < ::Karafka::Processing::Jobs::Base
  # @param executor [Karafka::Processing::Executor] executor that is suppose to run a given
  #   job on an active consumer
  # @return [Shutdown]
  #
  # source://karafka//lib/karafka/processing/jobs/idle.rb#12
  def initialize(executor); end

  # Runs code prior to scheduling this idle job
  #
  # source://karafka//lib/karafka/processing/jobs/idle.rb#18
  def before_schedule; end

  # Run the idle work via the executor
  #
  # source://karafka//lib/karafka/processing/jobs/idle.rb#23
  def call; end
end

# Job that runs the revoked operation when we loose a partition on a consumer that lost it.
#
# source://karafka//lib/karafka/processing/jobs/revoked.rb#7
class Karafka::Processing::Jobs::Revoked < ::Karafka::Processing::Jobs::Base
  # @param executor [Karafka::Processing::Executor] executor that is suppose to run the job
  # @return [Revoked]
  #
  # source://karafka//lib/karafka/processing/jobs/revoked.rb#10
  def initialize(executor); end

  # Runs code prior to scheduling this revoked job
  #
  # source://karafka//lib/karafka/processing/jobs/revoked.rb#16
  def before_schedule; end

  # Runs the revoking job via an executor.
  #
  # source://karafka//lib/karafka/processing/jobs/revoked.rb#21
  def call; end
end

# Job that runs on each active consumer upon process shutdown (one job per consumer).
#
# source://karafka//lib/karafka/processing/jobs/shutdown.rb#7
class Karafka::Processing::Jobs::Shutdown < ::Karafka::Processing::Jobs::Base
  # @param executor [Karafka::Processing::Executor] executor that is suppose to run a given
  #   job on an active consumer
  # @return [Shutdown]
  #
  # source://karafka//lib/karafka/processing/jobs/shutdown.rb#11
  def initialize(executor); end

  # Runs code prior to scheduling this shutdown job
  #
  # source://karafka//lib/karafka/processing/jobs/shutdown.rb#17
  def before_schedule; end

  # Runs the shutdown job via an executor.
  #
  # source://karafka//lib/karafka/processing/jobs/shutdown.rb#22
  def call; end
end

# Class responsible for deciding what type of job should we build to run a given command and
# for building a proper job for it.
#
# source://karafka//lib/karafka/processing/jobs_builder.rb#7
class Karafka::Processing::JobsBuilder
  # @param executor [Karafka::Processing::Executor]
  # @param messages [Karafka::Messages::Messages] messages batch to be consumed
  # @return [Karafka::Processing::Jobs::Consume] consumption job
  #
  # source://karafka//lib/karafka/processing/jobs_builder.rb#11
  def consume(executor, messages); end

  # @param executor [Karafka::Processing::Executor]
  # @return [Karafka::Processing::Jobs::Revoked] revocation job
  #
  # source://karafka//lib/karafka/processing/jobs_builder.rb#17
  def revoked(executor); end

  # @param executor [Karafka::Processing::Executor]
  # @return [Karafka::Processing::Jobs::Shutdown] shutdown job
  #
  # source://karafka//lib/karafka/processing/jobs_builder.rb#23
  def shutdown(executor); end
end

# This is the key work component for Karafka jobs distribution. It provides API for running
# jobs in parallel while operating within more than one subscription group.
#
# We need to take into consideration fact, that more than one subscription group can operate
# on this queue, that's why internally we keep track of processing per group.
#
# We work with the assumption, that partitions data is evenly distributed.
#
# @note This job queue also keeps track / understands number of busy workers. This is because
#   we use a single workers poll that can have granular scheduling.
#
# source://karafka//lib/karafka/processing/jobs_queue.rb#15
class Karafka::Processing::JobsQueue
  # @return [Karafka::Processing::JobsQueue]
  #
  # source://karafka//lib/karafka/processing/jobs_queue.rb#22
  def initialize; end

  # Adds the job to the internal main queue, scheduling it for execution in a worker and marks
  # this job as in processing pipeline.
  #
  # @param job [Jobs::Base] job that we want to run
  #
  # source://karafka//lib/karafka/processing/jobs_queue.rb#58
  def <<(job); end

  # Clears the processing states for a provided group. Useful when a recovery happens and we
  # need to clean up state but only for a given subscription group.
  #
  # @param group_id [String]
  #
  # source://karafka//lib/karafka/processing/jobs_queue.rb#123
  def clear(group_id); end

  # Stops the whole processing queue.
  #
  # source://karafka//lib/karafka/processing/jobs_queue.rb#132
  def close; end

  # Marks a given job from a given group as completed. When there are no more jobs from a given
  # group to be executed, we won't wait.
  #
  # @param job [Jobs::Base] that was completed
  #
  # source://karafka//lib/karafka/processing/jobs_queue.rb#103
  def complete(job); end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def concurrency; end

  # a given group.
  #
  # @param group_id [String]
  # @return [Boolean] tell us if we have anything in the processing (or for processing) from
  #
  # source://karafka//lib/karafka/processing/jobs_queue.rb#145
  def empty?(group_id); end

  # @note This command is blocking and will wait until any job is available on the main queue
  # @return [Jobs::Base, nil] waits for a job from the main queue and returns it once available
  #   or returns nil if the queue has been stopped and there won't be anything more to process
  #   ever.
  #
  # source://karafka//lib/karafka/processing/jobs_queue.rb#88
  def pop; end

  # Registers given subscription group id in the queue. It is needed so we do not dynamically
  # create semaphore, hence avoiding potential race conditions
  #
  # @param group_id [String]
  #
  # source://karafka//lib/karafka/processing/jobs_queue.rb#42
  def register(group_id); end

  # - `busy` - number of jobs that are currently being processed (active work)
  # - `enqueued` - number of jobs in the queue that are waiting to be picked up by a worker
  #
  # @return [Hash] hash with basic usage statistics of this queue.
  #
  # source://karafka//lib/karafka/processing/jobs_queue.rb#176
  def statistics; end

  # Causes the wait lock to re-check the lock conditions and potential unlock.
  #
  # @note This does not release the wait lock. It just causes a conditions recheck
  # @param group_id [String] id of the group we want to unlock for one tick
  #
  # source://karafka//lib/karafka/processing/jobs_queue.rb#95
  def tick(group_id); end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def tick_interval; end

  # Blocks when there are things in the queue in a given group and waits until all the blocking
  #   jobs from a given group are completed
  #
  # @note This method is blocking.
  # @param group_id [String] id of the group in which jobs we're interested.
  # @yieldparam block [Block] we want to run before each pop (in case of Ruby pre 3.2) or
  #   before each pop and on every tick interval.
  #   This allows us to run extra code that needs to be executed even when we are waiting on
  #   the work to be finished.
  #
  # source://karafka//lib/karafka/processing/jobs_queue.rb#160
  def wait(group_id); end

  private

  # @note We do not wait for non-blocking jobs. Their flow should allow for `poll` running
  #   as they may exceed `max.poll.interval`
  # @param group_id [String] id of the group in which jobs we're interested.
  # @return [Boolean] should we keep waiting or not
  #
  # source://karafka//lib/karafka/processing/jobs_queue.rb#189
  def wait?(group_id); end
end

# Basic partitioner for work division
# It does not divide any work.
#
# source://karafka//lib/karafka/processing/partitioner.rb#7
class Karafka::Processing::Partitioner
  # @param subscription_group [Karafka::Routing::SubscriptionGroup] subscription group
  # @return [Partitioner] a new instance of Partitioner
  #
  # source://karafka//lib/karafka/processing/partitioner.rb#9
  def initialize(subscription_group); end

  # @param _topic [String] topic name
  # @param messages [Array<Karafka::Messages::Message>] karafka messages
  # @param _coordinator [Karafka::Processing::Coordinator] processing coordinator that will
  #   be used with those messages
  # @yieldparam group [Integer] id
  # @yieldparam karafka [Array<Karafka::Messages::Message>] messages
  #
  # source://karafka//lib/karafka/processing/partitioner.rb#19
  def call(_topic, messages, _coordinator); end
end

# A simple object that allows us to keep track of processing state.
# It allows to indicate if given thing moved from success to a failure or the other way around
# Useful for tracking consumption state
#
# source://karafka//lib/karafka/processing/result.rb#8
class Karafka::Processing::Result
  # @return [Result] a new instance of Result
  #
  # source://karafka//lib/karafka/processing/result.rb#11
  def initialize; end

  # Returns the value of attribute cause.
  #
  # source://karafka//lib/karafka/processing/result.rb#9
  def cause; end

  # Marks state as failure
  #
  # @param cause [StandardError] error that occurred and caused failure
  #
  # source://karafka//lib/karafka/processing/result.rb#31
  def failure!(cause); end

  # @return [Boolean] true if processing failed
  #
  # source://karafka//lib/karafka/processing/result.rb#37
  def failure?; end

  # Marks state as successful
  #
  # source://karafka//lib/karafka/processing/result.rb#22
  def success!; end

  # @return [Boolean]
  #
  # source://karafka//lib/karafka/processing/result.rb#17
  def success?; end
end

# Namespace for Karafka OSS schedulers
#
# source://karafka//lib/karafka.rb#0
module Karafka::Processing::Schedulers; end

# FIFO scheduler for messages coming from various topics and partitions
#
# source://karafka//lib/karafka/processing/schedulers/default.rb#8
class Karafka::Processing::Schedulers::Default
  # @param queue [Karafka::Processing::JobsQueue] queue where we want to put the jobs
  # @return [Default] a new instance of Default
  #
  # source://karafka//lib/karafka/processing/schedulers/default.rb#10
  def initialize(queue); end

  # This scheduler does not need to be cleared because it is stateless
  #
  # @param _group_id [String] Subscription group id
  #
  # source://karafka//lib/karafka/processing/schedulers/default.rb#36
  def on_clear(_group_id); end

  # This scheduler does not have anything to manage as it is a pass through and has no state
  #
  # source://karafka//lib/karafka/processing/schedulers/default.rb#29
  def on_manage; end

  # Schedules jobs in the fifo order
  #
  # @param jobs_array [Array<Karafka::Processing::Jobs::Consume>] jobs we want to schedule
  #
  # source://karafka//lib/karafka/processing/schedulers/default.rb#17
  def on_schedule_consumption(jobs_array); end

  # Schedules jobs in the fifo order
  #
  # @param jobs_array [Array<Karafka::Processing::Jobs::Consume>] jobs we want to schedule
  #
  # source://karafka//lib/karafka/processing/schedulers/default.rb#17
  def on_schedule_idle(jobs_array); end

  # Schedules jobs in the fifo order
  # Revocation, shutdown and idle jobs can also run in fifo by default
  #
  # @param jobs_array [Array<Karafka::Processing::Jobs::Consume>] jobs we want to schedule
  #
  # source://karafka//lib/karafka/processing/schedulers/default.rb#17
  def on_schedule_revocation(jobs_array); end

  # Schedules jobs in the fifo order
  #
  # @param jobs_array [Array<Karafka::Processing::Jobs::Consume>] jobs we want to schedule
  #
  # source://karafka//lib/karafka/processing/schedulers/default.rb#17
  def on_schedule_shutdown(jobs_array); end
end

# Our processing patterns differ depending on various features configurations
# In this namespace we collect strategies for particular feature combinations to simplify the
# design. Based on features combinations we can then select handling strategy for a given case.
#
# @note The lack of common code here is intentional. It would get complex if there would be
#   any type of composition, so each strategy is expected to be self-sufficient
#
# source://karafka//lib/karafka.rb#0
module Karafka::Processing::Strategies; end

# ActiveJob strategy to cooperate with the DLQ.
#
# While AJ is uses MOM by default because it delegates the offset management to the AJ
# consumer. With DLQ however there is an extra case for skipping broken jobs with offset
# marking due to ordered processing.
#
# source://karafka//lib/karafka/processing/strategies/aj_dlq_mom.rb#11
module Karafka::Processing::Strategies::AjDlqMom
  include ::Karafka::Processing::Strategies::Base
  include ::Karafka::Processing::Strategies::Default
  include ::Karafka::Processing::Strategies::Dlq
  include ::Karafka::Processing::Strategies::DlqMom

  # How should we post-finalize consumption.
  #
  # source://karafka//lib/karafka/processing/strategies/aj_dlq_mom.rb#22
  def handle_after_consume; end
end

# Apply strategy when only when using AJ with MOM and DLQ
#
# source://karafka//lib/karafka/processing/strategies/aj_dlq_mom.rb#15
Karafka::Processing::Strategies::AjDlqMom::FEATURES = T.let(T.unsafe(nil), Array)

# ActiveJob enabled
# Manual offset management enabled
#
# This is the default AJ strategy since AJ cannot be used without MOM
#
# source://karafka//lib/karafka/processing/strategies/aj_mom.rb#10
module Karafka::Processing::Strategies::AjMom
  include ::Karafka::Processing::Strategies::Base
  include ::Karafka::Processing::Strategies::Default
  include ::Karafka::Processing::Strategies::Mom
end

# Apply strategy when only when using AJ with MOM
#
# source://karafka//lib/karafka/processing/strategies/aj_mom.rb#14
Karafka::Processing::Strategies::AjMom::FEATURES = T.let(T.unsafe(nil), Array)

# Base strategy that should be included in each strategy, just to ensure the API
#
# source://karafka//lib/karafka/processing/strategies/base.rb#13
module Karafka::Processing::Strategies::Base
  # Post-consumption handling
  #
  # @raise [NotImplementedError]
  #
  # source://karafka//lib/karafka/processing/strategies/base.rb#40
  def handle_after_consume; end

  # What should happen before we kick in the processing
  #
  # @raise [NotImplementedError]
  #
  # source://karafka//lib/karafka/processing/strategies/base.rb#30
  def handle_before_consume; end

  # source://karafka//lib/karafka/processing/strategies/base.rb#22
  def handle_before_schedule_consume; end

  # source://karafka//lib/karafka/processing/strategies/base.rb#22
  def handle_before_schedule_idle; end

  # source://karafka//lib/karafka/processing/strategies/base.rb#22
  def handle_before_schedule_revoked; end

  # source://karafka//lib/karafka/processing/strategies/base.rb#22
  def handle_before_schedule_shutdown; end

  # What should happen in the processing
  #
  # @raise [NotImplementedError]
  #
  # source://karafka//lib/karafka/processing/strategies/base.rb#35
  def handle_consume; end

  # Idle run handling
  #
  # @raise [NotImplementedError]
  #
  # source://karafka//lib/karafka/processing/strategies/base.rb#45
  def handle_idle; end

  # Revocation handling
  #
  # @raise [NotImplementedError]
  #
  # source://karafka//lib/karafka/processing/strategies/base.rb#50
  def handle_revoked; end

  # Shutdown handling
  #
  # @raise [NotImplementedError]
  #
  # source://karafka//lib/karafka/processing/strategies/base.rb#55
  def handle_shutdown; end
end

# No features enabled.
# No manual offset management
# No long running jobs
# Nothing. Just standard, automatic flow
#
# source://karafka//lib/karafka/processing/strategies/default.rb#10
module Karafka::Processing::Strategies::Default
  include ::Karafka::Processing::Strategies::Base

  # Triggers an async offset commit
  #
  # @note Due to its async nature, this may not fully represent the offset state in some
  #   edge cases (like for example going beyond max.poll.interval)
  # @param async [Boolean] should we use async (default) or sync commit
  # @return [Boolean] true if we still own the partition.
  #
  # source://karafka//lib/karafka/processing/strategies/default.rb#83
  def commit_offsets(async: T.unsafe(nil)); end

  # Triggers a synchronous offsets commit to Kafka
  #
  # @note This is fully synchronous, hence the result of this can be used in DB transactions
  #   etc as a way of making sure, that we still own the partition.
  # @return [Boolean] true if we still own the partition, false otherwise.
  #
  # source://karafka//lib/karafka/processing/strategies/default.rb#98
  def commit_offsets!; end

  # Standard flow marks work as consumed and moves on if everything went ok.
  # If there was a processing error, we will pause and continue from the next message
  # (next that is +1 from the last one that was successfully marked as consumed)
  #
  # source://karafka//lib/karafka/processing/strategies/default.rb#129
  def handle_after_consume; end

  # Increment number of attempts
  #
  # source://karafka//lib/karafka/processing/strategies/default.rb#103
  def handle_before_consume; end

  # source://karafka//lib/karafka/processing/strategies/default.rb#25
  def handle_before_schedule_consume; end

  # source://karafka//lib/karafka/processing/strategies/default.rb#25
  def handle_before_schedule_idle; end

  # source://karafka//lib/karafka/processing/strategies/default.rb#25
  def handle_before_schedule_revoked; end

  # source://karafka//lib/karafka/processing/strategies/default.rb#25
  def handle_before_schedule_shutdown; end

  # Run the user consumption code
  #
  # source://karafka//lib/karafka/processing/strategies/default.rb#108
  def handle_consume; end

  # Code that should run on idle runs without messages available
  #
  # source://karafka//lib/karafka/processing/strategies/default.rb#148
  def handle_idle; end

  # We need to always un-pause the processing in case we have lost a given partition.
  # Otherwise the underlying librdkafka would not know we may want to continue processing and
  # the pause could in theory last forever
  #
  # source://karafka//lib/karafka/processing/strategies/default.rb#157
  def handle_revoked; end

  # Runs the shutdown code
  #
  # source://karafka//lib/karafka/processing/strategies/default.rb#171
  def handle_shutdown; end

  # Marks message as consumed in an async way.
  #
  # @note We keep track of this offset in case we would mark as consumed and got error when
  #   processing another message. In case like this we do not pause on the message we've
  #   already processed but rather at the next one. This applies to both sync and async
  #   versions of this method.
  # @param message [Messages::Message] last successfully processed message.
  # @return [Boolean] true if we were able to mark the offset, false otherwise.
  #   False indicates that we were not able and that we have lost the partition.
  #
  # source://karafka//lib/karafka/processing/strategies/default.rb#43
  def mark_as_consumed(message); end

  # Marks message as consumed in a sync way.
  #
  # @param message [Messages::Message] last successfully processed message.
  # @return [Boolean] true if we were able to mark the offset, false otherwise.
  #   False indicates that we were not able and that we have lost the partition.
  #
  # source://karafka//lib/karafka/processing/strategies/default.rb#62
  def mark_as_consumed!(message); end
end

# Apply strategy for a non-feature based flow
#
# source://karafka//lib/karafka/processing/strategies/default.rb#14
Karafka::Processing::Strategies::Default::FEATURES = T.let(T.unsafe(nil), Array)

# When using dead letter queue, processing won't stop after defined number of retries
# upon encountering non-critical errors but the messages that error will be moved to a
# separate topic with their payload and metadata, so they can be handled differently.
#
# source://karafka//lib/karafka/processing/strategies/dlq.rb#9
module Karafka::Processing::Strategies::Dlq
  include ::Karafka::Processing::Strategies::Base
  include ::Karafka::Processing::Strategies::Default

  # Moves the broken message into a separate queue defined via the settings
  #
  # @param skippable_message [Karafka::Messages::Message] message we are skipping that also
  #   should go to the dlq topic
  # @private
  #
  # source://karafka//lib/karafka/processing/strategies/dlq.rb#108
  def dispatch_to_dlq(skippable_message); end

  # Finds the message may want to skip (all, starting from first)
  #
  # @private
  # @return [Array<Karafka::Messages::Message, Boolean>] message we may want to skip and
  #   information if this message was from marked offset or figured out via mom flow
  #
  # source://karafka//lib/karafka/processing/strategies/dlq.rb#92
  def find_skippable_message; end

  # When manual offset management is on, we do not mark anything as consumed automatically
  # and we rely on the user to figure things out
  #
  # source://karafka//lib/karafka/processing/strategies/dlq.rb#55
  def handle_after_consume; end

  # Override of the standard `#mark_as_consumed` in order to handle the pause tracker
  # reset in case DLQ is marked as fully independent. When DLQ is marked independent,
  # any offset marking causes the pause count tracker to reset. This is useful when
  # the error is not due to the collective batch operations state but due to intermediate
  # "crawling" errors that move with it
  #
  # @param message [Messages::Message]
  # @see `Strategies::Default#mark_as_consumed` for more details
  #
  # source://karafka//lib/karafka/processing/strategies/dlq.rb#25
  def mark_as_consumed(message); end

  # Override of the standard `#mark_as_consumed!`. Resets the pause tracker count in case
  # DLQ was configured with the `independent` flag.
  #
  # @param message [Messages::Message]
  # @see `Strategies::Default#mark_as_consumed!` for more details
  #
  # source://karafka//lib/karafka/processing/strategies/dlq.rb#43
  def mark_as_consumed!(message); end

  # Marks message that went to DLQ (if applicable) based on the requested method
  #
  # @param skippable_message [Karafka::Messages::Message]
  #
  # source://karafka//lib/karafka/processing/strategies/dlq.rb#125
  def mark_dispatched_to_dlq(skippable_message); end
end

# Apply strategy when only dead letter queue is turned on
#
# source://karafka//lib/karafka/processing/strategies/dlq.rb#13
Karafka::Processing::Strategies::Dlq::FEATURES = T.let(T.unsafe(nil), Array)

# Same as pure dead letter queue but we do not marked failed message as consumed
#
# source://karafka//lib/karafka/processing/strategies/dlq_mom.rb#7
module Karafka::Processing::Strategies::DlqMom
  include ::Karafka::Processing::Strategies::Base
  include ::Karafka::Processing::Strategies::Default
  include ::Karafka::Processing::Strategies::Dlq

  # When manual offset management is on, we do not mark anything as consumed automatically
  # and we rely on the user to figure things out
  #
  # source://karafka//lib/karafka/processing/strategies/dlq_mom.rb#18
  def handle_after_consume; end
end

# Apply strategy when dlq is on with manual offset management
#
# source://karafka//lib/karafka/processing/strategies/dlq_mom.rb#11
Karafka::Processing::Strategies::DlqMom::FEATURES = T.let(T.unsafe(nil), Array)

# When using manual offset management, we do not mark as consumed after successful processing
#
# source://karafka//lib/karafka/processing/strategies/mom.rb#7
module Karafka::Processing::Strategies::Mom
  include ::Karafka::Processing::Strategies::Base
  include ::Karafka::Processing::Strategies::Default

  # When manual offset management is on, we do not mark anything as consumed automatically
  # and we rely on the user to figure things out
  #
  # source://karafka//lib/karafka/processing/strategies/mom.rb#17
  def handle_after_consume; end
end

# Apply strategy when only manual offset management is turned on
#
# source://karafka//lib/karafka/processing/strategies/mom.rb#11
Karafka::Processing::Strategies::Mom::FEATURES = T.let(T.unsafe(nil), Array)

# Selector of appropriate processing strategy matching topic combinations
#
# source://karafka//lib/karafka/processing/strategy_selector.rb#6
class Karafka::Processing::StrategySelector
  # @return [StrategySelector] a new instance of StrategySelector
  #
  # source://karafka//lib/karafka/processing/strategy_selector.rb#16
  def initialize; end

  # @param topic [Karafka::Routing::Topic] topic with settings based on which we find strategy
  # @return [Module] module with proper strategy
  #
  # source://karafka//lib/karafka/processing/strategy_selector.rb#23
  def find(topic); end

  # Returns the value of attribute strategies.
  #
  # source://karafka//lib/karafka/processing/strategy_selector.rb#7
  def strategies; end

  private

  # @return [Array<Module>] available strategies
  #
  # source://karafka//lib/karafka/processing/strategy_selector.rb#38
  def find_all; end
end

# Features we support in the OSS offering.
#
# source://karafka//lib/karafka/processing/strategy_selector.rb#10
Karafka::Processing::StrategySelector::SUPPORTED_FEATURES = T.let(T.unsafe(nil), Array)

# Minimal queue with timeout for Ruby 3.1 and lower.
#
# It is needed because only since 3.2, Ruby has a timeout on `#pop`
#
# source://karafka//lib/karafka/processing/timed_queue.rb#8
class Karafka::Processing::TimedQueue
  include ::Karafka::Core::Helpers::Time

  # @return [TimedQueue] a new instance of TimedQueue
  #
  # source://karafka//lib/karafka/processing/timed_queue.rb#11
  def initialize; end

  # Adds element to the queue
  #
  # @param obj [Object] pushes an element onto the queue
  #
  # source://karafka//lib/karafka/processing/timed_queue.rb#20
  def <<(obj); end

  # Closes the internal queue and releases the lock
  #
  # source://karafka//lib/karafka/processing/timed_queue.rb#54
  def close; end

  # No timeout means waiting up to 31 years
  #
  # @note We use timeout in seconds because this is how Ruby 3.2+ works and we want to have
  #   the same API for newer and older Ruby versions
  # @param timeout [Integer] max number of seconds to wait on the pop
  # @return [Object] element inserted on the array or `nil` on timeout
  #
  # source://karafka//lib/karafka/processing/timed_queue.rb#36
  def pop(timeout: T.unsafe(nil)); end

  # Adds element to the queue
  #
  # @param obj [Object] pushes an element onto the queue
  #
  # source://karafka//lib/karafka/processing/timed_queue.rb#20
  def push(obj); end
end

# Workers are used to run jobs in separate threads.
# Workers are the main processing units of the Karafka framework.
#
# Each job runs in three stages:
#   - prepare - here we can run any code that we would need to run blocking before we allow
#               the job to run fully async (non blocking). This will always run in a blocking
#               way and can be used to make sure all the resources and external dependencies
#               are satisfied before going async.
#
#   - call - actual processing logic that can run sync or async
#
#   - teardown - it should include any code that we want to run after we executed the user
#                code. This can be used to unlock certain resources or do other things that are
#                not user code but need to run after user code base is executed.
#
# source://karafka//lib/karafka/processing/worker.rb#19
class Karafka::Processing::Worker
  include ::Karafka::Helpers::Async
  extend ::Forwardable

  # @param jobs_queue [JobsQueue]
  # @return [Worker]
  #
  # source://karafka//lib/karafka/processing/worker.rb#27
  def initialize(jobs_queue); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def alive?(*args, **_arg1, &block); end

  # @return [String] id of this worker
  #
  # source://karafka//lib/karafka/processing/worker.rb#23
  def id; end

  # source://forwardable/1.3.3/forwardable.rb#231
  def join(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def name(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def terminate(*args, **_arg1, &block); end

  private

  # Runs processing of jobs in a loop
  # Stops when queue is closed.
  #
  # source://karafka//lib/karafka/processing/worker.rb#36
  def call; end

  # Fetches a single job, processes it and marks as completed.
  #
  # @note We do not have error handling here, as no errors should propagate this far. If they
  #   do, it is a critical error and should bubble up.
  # @note Upon closing the jobs queue, worker will close it's thread
  #
  # source://karafka//lib/karafka/processing/worker.rb#46
  def process; end
end

# Abstraction layer around workers batch.
#
# source://karafka//lib/karafka/processing/workers_batch.rb#6
class Karafka::Processing::WorkersBatch
  include ::Enumerable

  # @param jobs_queue [JobsQueue]
  # @return [WorkersBatch]
  #
  # source://karafka//lib/karafka/processing/workers_batch.rb#14
  def initialize(jobs_queue); end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def concurrency; end

  # Iterates over available workers and yields each worker
  #
  # @param block [Proc] block we want to run
  #
  # source://karafka//lib/karafka/processing/workers_batch.rb#20
  def each(&block); end

  # @return [Integer] number of workers in the batch
  #
  # source://karafka//lib/karafka/processing/workers_batch.rb#25
  def size; end
end

# Railtie for setting up Rails integration
#
# source://karafka//lib/karafka/railtie.rb#22
class Karafka::Railtie < ::Rails::Railtie; end

# Namespace for all elements related to requests routing
#
# source://karafka//lib/karafka.rb#0
module Karafka::Routing; end

# Allows us to get track of which consumer groups, subscription groups and topics are enabled
# or disabled via CLI
#
# source://karafka//lib/karafka/routing/activity_manager.rb#7
class Karafka::Routing::ActivityManager
  # @return [ActivityManager] a new instance of ActivityManager
  #
  # source://karafka//lib/karafka/routing/activity_manager.rb#15
  def initialize; end

  # @param type [Symbol] type for inclusion
  # @param name [String] name of the element
  # @return [Boolean] is the given resource active or not
  #
  # source://karafka//lib/karafka/routing/activity_manager.rb#41
  def active?(type, name); end

  # Clears the manager
  #
  # source://karafka//lib/karafka/routing/activity_manager.rb#67
  def clear; end

  # Adds resource to excluded
  #
  # @param type [Symbol] type for inclusion
  # @param name [String] name of the element
  #
  # source://karafka//lib/karafka/routing/activity_manager.rb#32
  def exclude(type, name); end

  # Adds resource to included/active
  #
  # @param type [Symbol] type for inclusion
  # @param name [String] name of the element
  #
  # source://karafka//lib/karafka/routing/activity_manager.rb#23
  def include(type, name); end

  # @return [Hash] accumulated data in a hash for validations
  #
  # source://karafka//lib/karafka/routing/activity_manager.rb#59
  def to_h; end

  private

  # Checks if the type we want to register is supported
  #
  # @param type [Symbol] type for inclusion
  # @raise [::Karafka::Errors::UnsupportedCaseError]
  #
  # source://karafka//lib/karafka/routing/activity_manager.rb#77
  def validate!(type); end
end

# Supported types of inclusions and exclusions
#
# source://karafka//lib/karafka/routing/activity_manager.rb#9
Karafka::Routing::ActivityManager::SUPPORTED_TYPES = T.let(T.unsafe(nil), Array)

# Builder used as a DSL layer for building consumers and telling them which topics to consume
#
# @example Build a simple (most common) route
#   consumers do
#   topic :new_videos do
#   consumer NewVideosConsumer
#   end
#   end
# @note We lock the access just in case this is used in patterns. The locks here do not have
#   any impact on routing usage unless being expanded, so no race conditions risks.
#
# source://karafka//lib/karafka/routing/builder.rb#16
class Karafka::Routing::Builder < ::Array
  include ::Karafka::Routing::Features::ActiveJob::Builder

  # @return [Builder] a new instance of Builder
  #
  # source://karafka//lib/karafka/routing/builder.rb#26
  def initialize; end

  # @return [Array<Karafka::Routing::ConsumerGroup>] only active consumer groups that
  #   we want to use. Since Karafka supports multi-process setup, we need to be able
  #   to pick only those consumer groups that should be active in our given process context
  #
  # source://karafka//lib/karafka/routing/builder.rb#69
  def active; end

  # Clears the builder and the draws memory
  #
  # source://karafka//lib/karafka/routing/builder.rb#74
  def clear; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def default_group_id; end

  # @param block [Proc] block with per-topic evaluated defaults
  # @return [Proc] defaults that should be evaluated per topic
  #
  # source://karafka//lib/karafka/routing/builder.rb#84
  def defaults(&block); end

  # Used to draw routes for Karafka
  #
  # @example
  #   draw do
  #   topic :xyz do
  #   end
  #   end
  # @note After it is done drawing it will store and validate all the routes to make sure that
  #   they are correct and that there are no topic/group duplications (this is forbidden)
  # @param block [Proc] block we will evaluate within the builder context
  # @raise [Karafka::Errors::InvalidConfigurationError] raised when configuration
  #   doesn't match with the config contract
  # @yield Evaluates provided block in a builder context so we can describe routes
  #
  # source://karafka//lib/karafka/routing/features/base/expander.rb#36
  def draw(&block); end

  private

  # Builds and saves given consumer group
  #
  # @param group_id [String, Symbol] name for consumer group
  # @param block [Proc] proc that should be executed in the proxy context
  #
  # source://karafka//lib/karafka/routing/builder.rb#101
  def consumer_group(group_id, &block); end

  # Handles the simple routing case where we create one consumer group and allow for further
  # subscription group customization
  #
  # @param subscription_group_name [String, Symbol] subscription group id. When not provided,
  #   a random uuid will be used
  # @param args [Array] any extra arguments accepted by the subscription group builder
  # @param block [Proc] further topics definitions
  #
  # source://karafka//lib/karafka/routing/builder.rb#118
  def subscription_group(subscription_group_name = T.unsafe(nil), **args, &block); end

  # In case we use simple style of routing, all topics will be assigned to the same consumer
  # group that will be based on the client_id
  #
  # @param topic_name [String, Symbol] name of a topic from which we want to consumer
  # @param block [Proc] proc we want to evaluate in the topic context
  #
  # source://karafka//lib/karafka/routing/builder.rb#138
  def topic(topic_name, &block); end
end

# Empty default per-topic config
#
# source://karafka//lib/karafka/routing/builder.rb#22
Karafka::Routing::Builder::EMPTY_DEFAULTS = T.let(T.unsafe(nil), Proc)

# Object used to describe a single consumer group that is going to subscribe to
# given topics
# It is a part of Karafka's DSL
#
# @note A single consumer group represents Kafka consumer group, but it may not match 1:1 with
#   subscription groups. There can be more subscription groups than consumer groups
#
# source://karafka//lib/karafka/routing/consumer_group.rb#10
class Karafka::Routing::ConsumerGroup
  # @param name [String, Symbol] raw name of this consumer group. Raw means, that it does not
  #   yet have an application client_id namespace, this will be added here by default.
  #   We add it to make a multi-system development easier for people that don't use
  #   kafka and don't understand the concept of consumer groups.
  # @return [ConsumerGroup] a new instance of ConsumerGroup
  #
  # source://karafka//lib/karafka/routing/consumer_group.rb#23
  def initialize(name); end

  # @return [Boolean] true if this consumer group should be active in our current process
  #
  # source://karafka//lib/karafka/routing/consumer_group.rb#34
  def active?; end

  # This is a "virtual" attribute that is not building subscription groups.
  # It allows us to store the "current" subscription group defined in the routing
  # This subscription group id is then injected into topics, so we can compute the subscription
  # groups
  #
  # source://karafka//lib/karafka/routing/consumer_group.rb#17
  def current_subscription_group_details; end

  # This is a "virtual" attribute that is not building subscription groups.
  # It allows us to store the "current" subscription group defined in the routing
  # This subscription group id is then injected into topics, so we can compute the subscription
  # groups
  #
  # source://karafka//lib/karafka/routing/consumer_group.rb#17
  def current_subscription_group_details=(_arg0); end

  # Returns the value of attribute id.
  #
  # source://karafka//lib/karafka/routing/consumer_group.rb#11
  def id; end

  # Returns the value of attribute name.
  #
  # source://karafka//lib/karafka/routing/consumer_group.rb#11
  def name; end

  # Assigns the current subscription group id based on the defined one and allows for further
  # topic definition
  #
  # @param name [String, Symbol] name of the current subscription group
  # @param block [Proc] block that may include topics definitions
  #
  # source://karafka//lib/karafka/routing/consumer_group.rb#60
  def subscription_group=(name = T.unsafe(nil), &block); end

  # @return [Array<Routing::SubscriptionGroup>] all the subscription groups build based on
  #   the consumer group topics
  #
  # source://karafka//lib/karafka/routing/consumer_group.rb#74
  def subscription_groups; end

  # Hashed version of consumer group that can be used for validation purposes
  # topics inside of it.
  #
  # @return [Hash] hash with consumer group attributes including serialized to hash
  #
  # source://karafka//lib/karafka/routing/consumer_group.rb#85
  def to_h; end

  # Builds a topic representation inside of a current consumer group route
  #
  # @param name [String, Symbol] name of topic to which we want to subscribe
  # @param block [Proc] block that we want to evaluate in the topic context
  # @return [Karafka::Routing::Topic] newly built topic instance
  #
  # source://karafka//lib/karafka/routing/consumer_group.rb#42
  def topic=(name, &block); end

  # Returns the value of attribute topics.
  #
  # source://karafka//lib/karafka/routing/consumer_group.rb#11
  def topics; end

  private

  # @return [Karafka::Core::Configurable::Node] root node config
  #
  # source://karafka//lib/karafka/routing/consumer_group.rb#95
  def config; end
end

# Namespace for all the topic related features we support
#
# @note Not all the Karafka features need to be defined here as only those that have routing
#   or other extensions need to be here. That is why we keep (for now) features under the
#   routing namespace.
#
# source://karafka//lib/karafka.rb#0
module Karafka::Routing::Features; end

# Active-Job related components
#
# @note We can load it always, despite someone not using ActiveJob as it just adds a method
#   to the routing, without actually breaking anything.
#
# source://karafka//lib/karafka/routing/features/active_job.rb#9
class Karafka::Routing::Features::ActiveJob < ::Karafka::Routing::Features::Base; end

# Routing extensions for ActiveJob
#
# source://karafka//lib/karafka/routing/features/active_job/builder.rb#8
module Karafka::Routing::Features::ActiveJob::Builder
  # This method simplifies routes definition for ActiveJob topics / queues by
  # auto-injecting the consumer class
  #
  # @param name [String, Symbol] name of the topic where ActiveJobs jobs should go
  # @param block [Proc] block that we can use for some extra configuration
  #
  # source://karafka//lib/karafka/routing/features/active_job/builder.rb#14
  def active_job_topic(name, &block); end
end

# Config for ActiveJob usage
#
# source://karafka//lib/karafka/routing/features/active_job/config.rb#8
class Karafka::Routing::Features::ActiveJob::Config < ::Struct
  # Returns the value of attribute active
  #
  # @return [Object] the current value of active
  def active; end

  # Sets the attribute active
  #
  # @param value [Object] the value to set the attribute active to.
  # @return [Object] the newly set value
  def active=(_); end

  # Returns the value of attribute active
  #
  # @return [Object] the current value of active
  def active?; end

  class << self
    def [](*_arg0); end
    def inspect; end
    def keyword_init?; end
    def members; end
    def new(*_arg0); end
  end
end

# This feature validation contracts
#
# source://karafka//lib/karafka.rb#0
module Karafka::Routing::Features::ActiveJob::Contracts; end

# Rules around using ActiveJob routing - basically you need to have ActiveJob available
# in order to be able to use active job routing
#
# source://karafka//lib/karafka/routing/features/active_job/contracts/topic.rb#11
class Karafka::Routing::Features::ActiveJob::Contracts::Topic < ::Karafka::Contracts::Base; end

# Routing proxy extensions for ActiveJob
#
# source://karafka//lib/karafka/routing/features/active_job/proxy.rb#8
module Karafka::Routing::Features::ActiveJob::Proxy
  include ::Karafka::Routing::Features::ActiveJob::Builder
end

# Topic extensions to be able to check if given topic is ActiveJob topic
#
# source://karafka//lib/karafka/routing/features/active_job/topic.rb#8
module Karafka::Routing::Features::ActiveJob::Topic
  # where the boolean would be an argument
  #
  # @note Since this feature supports only one setting (active), we can use the old API
  # @param active [Boolean] should this topic be considered one working with ActiveJob
  #
  # source://karafka//lib/karafka/routing/features/active_job/topic.rb#13
  def active_job(active = T.unsafe(nil)); end

  # @return [Boolean] is this an ActiveJob topic
  #
  # source://karafka//lib/karafka/routing/features/active_job/topic.rb#18
  def active_job?; end

  # @return [Hash] topic with all its native configuration options plus active job
  #   namespace settings
  #
  # source://karafka//lib/karafka/routing/features/active_job/topic.rb#24
  def to_h; end
end

# Base for all the features
#
# source://karafka//lib/karafka/routing/features/base.rb#12
class Karafka::Routing::Features::Base
  class << self
    # Extends topic and builder with given feature API
    #
    # source://karafka//lib/karafka/routing/features/base.rb#15
    def activate; end

    # Loads all the features and activates them once
    #
    # source://karafka//lib/karafka/routing/features/base.rb#50
    def load_all; end

    # Runs post setup routing features configuration operations
    #
    # @param config [Karafka::Core::Configurable::Node]
    #
    # source://karafka//lib/karafka/routing/features/base.rb#67
    def post_setup_all(config); end

    # @param config [Karafka::Core::Configurable::Node] app config that we can alter with
    #   particular routing feature specific stuff if needed
    #
    # source://karafka//lib/karafka/routing/features/base.rb#60
    def pre_setup_all(config); end

    protected

    # Runs post-setup configuration of a particular routing feature
    #
    # @param _config [Karafka::Core::Configurable::Node] app config node
    #
    # source://karafka//lib/karafka/routing/features/base.rb#100
    def post_setup(_config); end

    # Runs pre-setup configuration of a particular routing feature
    #
    # @param _config [Karafka::Core::Configurable::Node] app config node
    #
    # source://karafka//lib/karafka/routing/features/base.rb#93
    def pre_setup(_config); end

    private

    # @return [Array<Class>] all available routing features that are direct descendants of
    #   the features base.Approach with using `#superclass` prevents us from accidentally
    #   loading Pro components
    #
    # source://karafka//lib/karafka/routing/features/base.rb#76
    def features; end
  end
end

# Routing builder expander that injects feature related drawing operations into it
#
# source://karafka//lib/karafka/routing/features/base/expander.rb#8
class Karafka::Routing::Features::Base::Expander < ::Module
  # @param scope [Module] feature scope in which contract and other things should be
  # @return [Expander] builder expander instance
  #
  # source://karafka//lib/karafka/routing/features/base/expander.rb#11
  def initialize(scope); end

  # Builds anonymous module that alters how `#draw` behaves allowing the feature contracts
  # to run.
  #
  # @param mod [::Karafka::Routing::Builder] builder we will prepend to
  #
  # source://karafka//lib/karafka/routing/features/base/expander.rb#19
  def prepended(mod); end

  private

  # @return [Module] builds an anonymous module with `#draw` that will alter the builder
  #   `#draw` allowing to run feature context aware code.
  #
  # source://karafka//lib/karafka/routing/features/base/expander.rb#29
  def prepended_module; end
end

# This feature allows to continue processing when encountering errors.
# After certain number of retries, given messages will be moved to alternative topic,
# unclogging processing.
#
# @note This feature has an expanded version in the Pro mode. We do not use a new feature
#   injection in Pro (topic settings)
#
# source://karafka//lib/karafka/routing/features/dead_letter_queue.rb#12
class Karafka::Routing::Features::DeadLetterQueue < ::Karafka::Routing::Features::Base; end

# Config for dead letter queue feature
#
# source://karafka//lib/karafka/routing/features/dead_letter_queue/config.rb#8
class Karafka::Routing::Features::DeadLetterQueue::Config < ::Struct
  # Returns the value of attribute active
  #
  # @return [Object] the current value of active
  def active; end

  # Sets the attribute active
  #
  # @param value [Object] the value to set the attribute active to.
  # @return [Object] the newly set value
  def active=(_); end

  # Returns the value of attribute active
  #
  # @return [Object] the current value of active
  def active?; end

  # Returns the value of attribute dispatch_method
  #
  # @return [Object] the current value of dispatch_method
  def dispatch_method; end

  # Sets the attribute dispatch_method
  #
  # @param value [Object] the value to set the attribute dispatch_method to.
  # @return [Object] the newly set value
  def dispatch_method=(_); end

  # Returns the value of attribute independent
  #
  # @return [Object] the current value of independent
  def independent; end

  # Sets the attribute independent
  #
  # @param value [Object] the value to set the attribute independent to.
  # @return [Object] the newly set value
  def independent=(_); end

  # Returns the value of attribute independent
  #
  # @return [Object] the current value of independent
  def independent?; end

  # Returns the value of attribute marking_method
  #
  # @return [Object] the current value of marking_method
  def marking_method; end

  # Sets the attribute marking_method
  #
  # @param value [Object] the value to set the attribute marking_method to.
  # @return [Object] the newly set value
  def marking_method=(_); end

  # Returns the value of attribute max_retries
  #
  # @return [Object] the current value of max_retries
  def max_retries; end

  # Sets the attribute max_retries
  #
  # @param value [Object] the value to set the attribute max_retries to.
  # @return [Object] the newly set value
  def max_retries=(_); end

  # Returns the value of attribute strategy
  #
  # @return [Object] the current value of strategy
  def strategy; end

  # Sets the attribute strategy
  #
  # @param value [Object] the value to set the attribute strategy to.
  # @return [Object] the newly set value
  def strategy=(_); end

  # Returns the value of attribute topic
  #
  # @return [Object] the current value of topic
  def topic; end

  # Sets the attribute topic
  #
  # @param value [Object] the value to set the attribute topic to.
  # @return [Object] the newly set value
  def topic=(_); end

  # Returns the value of attribute transactional
  #
  # @return [Object] the current value of transactional
  def transactional; end

  # Sets the attribute transactional
  #
  # @param value [Object] the value to set the attribute transactional to.
  # @return [Object] the newly set value
  def transactional=(_); end

  # Returns the value of attribute transactional
  #
  # @return [Object] the current value of transactional
  def transactional?; end

  class << self
    def [](*_arg0); end
    def inspect; end
    def keyword_init?; end
    def members; end
    def new(*_arg0); end
  end
end

# This feature validation contracts
#
# source://karafka//lib/karafka.rb#0
module Karafka::Routing::Features::DeadLetterQueue::Contracts; end

# Rules around dead letter queue settings
#
# source://karafka//lib/karafka/routing/features/dead_letter_queue/contracts/topic.rb#10
class Karafka::Routing::Features::DeadLetterQueue::Contracts::Topic < ::Karafka::Contracts::Base; end

# DLQ topic extensions
#
# source://karafka//lib/karafka/routing/features/dead_letter_queue/topic.rb#8
module Karafka::Routing::Features::DeadLetterQueue::Topic
  # @param max_retries [Integer] after how many retries should we move data to dlq
  # @param topic [String, false] where the messages should be moved if failing or false
  #   if we do not want to move it anywhere and just skip
  # @param independent [Boolean] needs to be true in order for each marking as consumed
  #   in a retry flow to reset the errors counter
  # @param transactional [Boolean] if applicable, should transaction be used to move
  #   given message to the dead-letter topic and mark it as consumed.
  # @param dispatch_method [Symbol] `:produce_async` or `:produce_sync`. Describes
  #   whether dispatch on dlq should be sync or async (async by default)
  # @param marking_method [Symbol] `:mark_as_consumed` or `:mark_as_consumed!`. Describes
  #   whether marking on DLQ should be async or sync (async by default)
  # @return [Config] defined config
  #
  # source://karafka//lib/karafka/routing/features/dead_letter_queue/topic.rb#26
  def dead_letter_queue(max_retries: T.unsafe(nil), topic: T.unsafe(nil), independent: T.unsafe(nil), transactional: T.unsafe(nil), dispatch_method: T.unsafe(nil), marking_method: T.unsafe(nil)); end

  # @return [Boolean] is the dlq active or not
  #
  # source://karafka//lib/karafka/routing/features/dead_letter_queue/topic.rb#46
  def dead_letter_queue?; end

  # @return [Hash] topic with all its native configuration options plus dlq settings
  #
  # source://karafka//lib/karafka/routing/features/dead_letter_queue/topic.rb#51
  def to_h; end
end

# After how many retries should be move data to DLQ
#
# source://karafka//lib/karafka/routing/features/dead_letter_queue/topic.rb#10
Karafka::Routing::Features::DeadLetterQueue::Topic::DEFAULT_MAX_RETRIES = T.let(T.unsafe(nil), Integer)

# This feature allows to store per topic structure that can be later on used to bootstrap
# topics structure for test/development, etc. This allows to share the same set of settings
# for topics despite the environment. Pretty much similar to how the `structure.sql` or
# `schema.rb` operate for SQL dbs.
#
# source://karafka//lib/karafka/routing/features/declaratives.rb#10
class Karafka::Routing::Features::Declaratives < ::Karafka::Routing::Features::Base; end

# Config for declarative topics feature
#
# source://karafka//lib/karafka/routing/features/declaratives/config.rb#8
class Karafka::Routing::Features::Declaratives::Config < ::Struct
  # Returns the value of attribute active
  #
  # @return [Object] the current value of active
  def active; end

  # Sets the attribute active
  #
  # @param value [Object] the value to set the attribute active to.
  # @return [Object] the newly set value
  def active=(_); end

  # Returns the value of attribute active
  #
  # @return [Object] the current value of active
  def active?; end

  # Returns the value of attribute details
  #
  # @return [Object] the current value of details
  def details; end

  # Sets the attribute details
  #
  # @param value [Object] the value to set the attribute details to.
  # @return [Object] the newly set value
  def details=(_); end

  # Returns the value of attribute partitions
  #
  # @return [Object] the current value of partitions
  def partitions; end

  # Sets the attribute partitions
  #
  # @param value [Object] the value to set the attribute partitions to.
  # @return [Object] the newly set value
  def partitions=(_); end

  # Returns the value of attribute replication_factor
  #
  # @return [Object] the current value of replication_factor
  def replication_factor; end

  # Sets the attribute replication_factor
  #
  # @param value [Object] the value to set the attribute replication_factor to.
  # @return [Object] the newly set value
  def replication_factor=(_); end

  class << self
    def [](*_arg0); end
    def inspect; end
    def keyword_init?; end
    def members; end
    def new(*_arg0); end
  end
end

# This feature validation contracts
#
# source://karafka//lib/karafka.rb#0
module Karafka::Routing::Features::Declaratives::Contracts; end

# Basic validation of the Kafka expected config details
#
# source://karafka//lib/karafka/routing/features/declaratives/contracts/topic.rb#10
class Karafka::Routing::Features::Declaratives::Contracts::Topic < ::Karafka::Contracts::Base; end

# Extension for managing Kafka topic configuration
#
# source://karafka//lib/karafka/routing/features/declaratives/topic.rb#8
module Karafka::Routing::Features::Declaratives::Topic
  # @param active [Boolean] is the topic structure management feature active
  # @param partitions [Integer]
  # @param replication_factor [Integer]
  # @param details [Hash] extra configuration for the topic
  # @return [Config] defined structure
  #
  # source://karafka//lib/karafka/routing/features/declaratives/topic.rb#14
  def config(active: T.unsafe(nil), partitions: T.unsafe(nil), replication_factor: T.unsafe(nil), **details); end

  # @return [Config] config details
  #
  # source://karafka//lib/karafka/routing/features/declaratives/topic.rb#24
  def declaratives; end

  # @return [true] declaratives is always active
  #
  # source://karafka//lib/karafka/routing/features/declaratives/topic.rb#29
  def declaratives?; end

  # @return [Hash] topic with all its native configuration options plus declaratives
  #   settings
  #
  # source://karafka//lib/karafka/routing/features/declaratives/topic.rb#35
  def to_h; end
end

# Namespace for feature allowing to configure deserializers for payload, key and headers
#
# source://karafka//lib/karafka/routing/features/deserializers.rb#7
class Karafka::Routing::Features::Deserializers < ::Karafka::Routing::Features::Base; end

# Config of this feature
#
# source://karafka//lib/karafka/routing/features/deserializers/config.rb#8
class Karafka::Routing::Features::Deserializers::Config < ::Struct
  # Returns the value of attribute active
  #
  # @return [Object] the current value of active
  def active; end

  # Sets the attribute active
  #
  # @param value [Object] the value to set the attribute active to.
  # @return [Object] the newly set value
  def active=(_); end

  # Returns the value of attribute active
  #
  # @return [Object] the current value of active
  def active?; end

  # Returns the value of attribute headers
  #
  # @return [Object] the current value of headers
  def headers; end

  # Sets the attribute headers
  #
  # @param value [Object] the value to set the attribute headers to.
  # @return [Object] the newly set value
  def headers=(_); end

  # Returns the value of attribute key
  #
  # @return [Object] the current value of key
  def key; end

  # Sets the attribute key
  #
  # @param value [Object] the value to set the attribute key to.
  # @return [Object] the newly set value
  def key=(_); end

  # Returns the value of attribute payload
  #
  # @return [Object] the current value of payload
  def payload; end

  # Sets the attribute payload
  #
  # @param value [Object] the value to set the attribute payload to.
  # @return [Object] the newly set value
  def payload=(_); end

  class << self
    def [](*_arg0); end
    def inspect; end
    def keyword_init?; end
    def members; end
    def new(*_arg0); end
  end
end

# This feature validation contracts
#
# source://karafka//lib/karafka.rb#0
module Karafka::Routing::Features::Deserializers::Contracts; end

# Basic validation of the Kafka expected config details
#
# source://karafka//lib/karafka/routing/features/deserializers/contracts/topic.rb#10
class Karafka::Routing::Features::Deserializers::Contracts::Topic < ::Karafka::Contracts::Base; end

# Routing topic deserializers API. It allows to configure deserializers for various
# components of each message.
#
# source://karafka//lib/karafka/routing/features/deserializers/topic.rb#10
module Karafka::Routing::Features::Deserializers::Topic
  # Supports pre 2.4 format where only payload deserializer could be defined. We do not
  # retire this format because it is not bad when users do not do anything advanced with
  # key or headers
  #
  # @param payload [Object] payload deserializer
  #
  # source://karafka//lib/karafka/routing/features/deserializers/topic.rb#32
  def deserializer(payload); end

  # Allows for setting all the deserializers with standard defaults
  #
  # @param payload [Object] Deserializer for the message payload
  # @param key [Object] deserializer for the message key
  # @param headers [Object] deserializer for the message headers
  #
  # source://karafka//lib/karafka/routing/features/deserializers/topic.rb#15
  def deserializers(payload: T.unsafe(nil), key: T.unsafe(nil), headers: T.unsafe(nil)); end

  # @return [Boolean] Deserializers are always active
  #
  # source://karafka//lib/karafka/routing/features/deserializers/topic.rb#37
  def deserializers?; end

  # @return [Hash] topic setup hash
  #
  # source://karafka//lib/karafka/routing/features/deserializers/topic.rb#42
  def to_h; end
end

# Feature allowing us to get visibility during the consumption into metrics of particular
# partition we operate on. It can be useful when making context-aware consumers that change
# their behaviours based on the lag and other parameters.
#
# source://karafka//lib/karafka/routing/features/inline_insights.rb#9
class Karafka::Routing::Features::InlineInsights < ::Karafka::Routing::Features::Base
  class << self
    # If needed installs the needed listener and initializes tracker
    #
    # @param _config [Karafka::Core::Configurable::Node] app config
    #
    # source://karafka//lib/karafka/routing/features/inline_insights.rb#14
    def post_setup(_config); end
  end
end

# Config of this feature
#
# source://karafka//lib/karafka/routing/features/inline_insights/config.rb#8
class Karafka::Routing::Features::InlineInsights::Config < ::Struct
  # Returns the value of attribute active
  #
  # @return [Object] the current value of active
  def active; end

  # Sets the attribute active
  #
  # @param value [Object] the value to set the attribute active to.
  # @return [Object] the newly set value
  def active=(_); end

  # Returns the value of attribute active
  #
  # @return [Object] the current value of active
  def active?; end

  class << self
    def [](*_arg0); end
    def inspect; end
    def keyword_init?; end
    def members; end
    def new(*_arg0); end
  end
end

# Inline Insights related contracts namespace
#
# source://karafka//lib/karafka.rb#0
module Karafka::Routing::Features::InlineInsights::Contracts; end

# Contract for inline insights topic setup
#
# source://karafka//lib/karafka/routing/features/inline_insights/contracts/topic.rb#10
class Karafka::Routing::Features::InlineInsights::Contracts::Topic < ::Karafka::Contracts::Base; end

# Routing topic inline insights API
#
# source://karafka//lib/karafka/routing/features/inline_insights/topic.rb#8
module Karafka::Routing::Features::InlineInsights::Topic
  # @param active [Boolean] should inline insights be activated
  #
  # source://karafka//lib/karafka/routing/features/inline_insights/topic.rb#10
  def inline_insights(active = T.unsafe(nil)); end

  # @return [Boolean] Are inline insights active
  #
  # source://karafka//lib/karafka/routing/features/inline_insights/topic.rb#17
  def inline_insights?; end

  # @return [Hash] topic setup hash
  #
  # source://karafka//lib/karafka/routing/features/inline_insights/topic.rb#22
  def to_h; end
end

# All the things needed to be able to manage manual offset management from the routing
# perspective.
#
# Manual offset management allows users to completely disable automatic management of the
# offset. This can be used for implementing long-living window operations and other things
# where we do not want to commit the offset with each batch.
#
# Not all the Karafka and Karafka Pro features may be compatible with this feature being on.
#
# source://karafka//lib/karafka/routing/features/manual_offset_management.rb#14
class Karafka::Routing::Features::ManualOffsetManagement < ::Karafka::Routing::Features::Base; end

# Config for manual offset management feature
#
# source://karafka//lib/karafka/routing/features/manual_offset_management/config.rb#8
class Karafka::Routing::Features::ManualOffsetManagement::Config < ::Struct
  # Returns the value of attribute active
  #
  # @return [Object] the current value of active
  def active; end

  # Sets the attribute active
  #
  # @param value [Object] the value to set the attribute active to.
  # @return [Object] the newly set value
  def active=(_); end

  # Returns the value of attribute active
  #
  # @return [Object] the current value of active
  def active?; end

  class << self
    def [](*_arg0); end
    def inspect; end
    def keyword_init?; end
    def members; end
    def new(*_arg0); end
  end
end

# This feature validation contracts
#
# source://karafka//lib/karafka.rb#0
module Karafka::Routing::Features::ManualOffsetManagement::Contracts; end

# Rules around manual offset management settings
#
# source://karafka//lib/karafka/routing/features/manual_offset_management/contracts/topic.rb#10
class Karafka::Routing::Features::ManualOffsetManagement::Contracts::Topic < ::Karafka::Contracts::Base; end

# Topic extensions to be able to manage manual offset management settings
#
# source://karafka//lib/karafka/routing/features/manual_offset_management/topic.rb#8
module Karafka::Routing::Features::ManualOffsetManagement::Topic
  # where the boolean would be an argument
  #
  # @note Since this feature supports only one setting (active), we can use the old API
  # @param active [Boolean] should we stop managing the offset in Karafka and make the user
  #   responsible for marking messages as consumed.
  # @return [Config] defined config
  #
  # source://karafka//lib/karafka/routing/features/manual_offset_management/topic.rb#15
  def manual_offset_management(active = T.unsafe(nil)); end

  # @return [Boolean] is manual offset management enabled for a given topic
  #
  # source://karafka//lib/karafka/routing/features/manual_offset_management/topic.rb#20
  def manual_offset_management?; end

  # @return [Hash] topic with all its native configuration options plus manual offset
  #   management namespace settings
  #
  # source://karafka//lib/karafka/routing/features/manual_offset_management/topic.rb#26
  def to_h; end
end

# Proxy is used as a translation layer in between the DSL and raw topic and consumer group
# objects.
#
# source://karafka//lib/karafka/routing/proxy.rb#7
class Karafka::Routing::Proxy
  include ::Karafka::Routing::Features::ActiveJob::Builder
  include ::Karafka::Routing::Features::ActiveJob::Proxy

  # @param target [Object] target object to which we proxy any DSL call
  # @param defaults [Proc] defaults for target that should be applicable after the proper
  #   proxy context (if needed)
  # @param block [Proc, nil] block that we want to evaluate in the proxy context or nil if no
  #   proxy block context for example because whole context is taken from defaults
  # @return [Proxy] a new instance of Proxy
  #
  # source://karafka//lib/karafka/routing/proxy.rb#15
  def initialize(target, defaults = T.unsafe(nil), &block); end

  # Translates the no "=" DSL of routing into elements assignments on target
  #
  # @param method_name [Symbol] name of the missing method
  #
  # source://karafka//lib/karafka/routing/proxy.rb#23
  def method_missing(method_name, *_arg1, **_arg2, &_arg3); end

  # Returns the value of attribute target.
  #
  # source://karafka//lib/karafka/routing/proxy.rb#8
  def target; end

  private

  # Tells whether or not a given element exists on the target
  #
  # @param method_name [Symbol] name of the missing method
  # @param include_private [Boolean] should we include private in the check as well
  # @return [Boolean]
  #
  # source://karafka//lib/karafka/routing/proxy.rb#36
  def respond_to_missing?(method_name, include_private = T.unsafe(nil)); end
end

# Karafka framework Router for routing incoming messages to proper consumers
#
# @note Since Kafka does not provide namespaces or modules for topics, they all have "flat"
#   structure so all the routes are being stored in a single level array
#
# source://karafka//lib/karafka/routing/router.rb#9
module Karafka::Routing::Router
  private

  # Finds first reference of a given topic based on provided lookup attribute
  #
  # @param lookup [Hash<Symbol, String>] hash with attribute - value key pairs
  # @return [Karafka::Routing::Topic, nil] proper route details or nil if not found
  #
  # source://karafka//lib/karafka/routing/router.rb#13
  def find_by(lookup); end

  # Finds the topic by name (in any consumer group) and if not present, will built a new
  # representation of the topic with the defaults and default deserializers.
  #
  # This is used in places where we may operate on topics that are not part of the routing
  # but we want to do something on them (display data, iterate over, etc)
  #
  # @note Please note, that in case of a new topic, it will have a newly built consumer group
  #   as well, that is not part of the routing.
  # @param name [String] name of the topic we are looking for
  # @return [Karafka::Routing::Topic]
  #
  # source://karafka//lib/karafka/routing/router.rb#35
  def find_or_initialize_by_name(name); end

  class << self
    # Finds first reference of a given topic based on provided lookup attribute
    #
    # @param lookup [Hash<Symbol, String>] hash with attribute - value key pairs
    # @return [Karafka::Routing::Topic, nil] proper route details or nil if not found
    #
    # source://karafka//lib/karafka/routing/router.rb#13
    def find_by(lookup); end

    # Finds the topic by name (in any consumer group) and if not present, will built a new
    # representation of the topic with the defaults and default deserializers.
    #
    # This is used in places where we may operate on topics that are not part of the routing
    # but we want to do something on them (display data, iterate over, etc)
    #
    # @note Please note, that in case of a new topic, it will have a newly built consumer group
    #   as well, that is not part of the routing.
    # @param name [String] name of the topic we are looking for
    # @return [Karafka::Routing::Topic]
    #
    # source://karafka//lib/karafka/routing/router.rb#35
    def find_or_initialize_by_name(name); end
  end
end

# Object representing a set of single consumer group topics that can be subscribed together
# with one connection.
#
# @note One subscription group will always belong to one consumer group, but one consumer
#   group can have multiple subscription groups.
#
# source://karafka//lib/karafka/routing/subscription_group.rb#10
class Karafka::Routing::SubscriptionGroup
  # @param position [Integer] position of this subscription group in all the subscriptions
  #   groups array. We need to have this value for sake of static group memberships, where
  #   we need a "in-between" restarts unique identifier
  # @param topics [Karafka::Routing::Topics] all the topics that share the same key settings
  # @return [SubscriptionGroup] built subscription group
  #
  # source://karafka//lib/karafka/routing/subscription_group.rb#45
  def initialize(position, topics); end

  # @return [Boolean] is this subscription group one of active once
  #
  # source://karafka//lib/karafka/routing/subscription_group.rb#75
  def active?; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def activity_manager; end

  # @param _consumer [Karafka::Connection::Proxy]
  # @return [false, Rdkafka::Consumer::TopicPartitionList] List of tpls for direct assignments
  #   or false for the normal mode
  #
  # source://karafka//lib/karafka/routing/subscription_group.rb#92
  def assignments(_consumer); end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def client_id; end

  # Returns the value of attribute consumer_group.
  #
  # source://karafka//lib/karafka/routing/subscription_group.rb#17
  def consumer_group; end

  # @return [String] consumer group id
  #
  # source://karafka//lib/karafka/routing/subscription_group.rb#60
  def consumer_group_id; end

  # Returns the value of attribute id.
  #
  # source://karafka//lib/karafka/routing/subscription_group.rb#17
  def id; end

  # Returns the value of attribute kafka.
  #
  # source://karafka//lib/karafka/routing/subscription_group.rb#17
  def kafka; end

  # @return [Integer] max messages fetched in a single go
  #
  # source://karafka//lib/karafka/routing/subscription_group.rb#65
  def max_messages; end

  # @return [Integer] max milliseconds we can wait for incoming messages
  #
  # source://karafka//lib/karafka/routing/subscription_group.rb#70
  def max_wait_time; end

  # Returns the value of attribute name.
  #
  # source://karafka//lib/karafka/routing/subscription_group.rb#17
  def name; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def node; end

  # Refreshes the configuration of this subscription group if needed based on the execution
  # context.
  #
  # Since the initial routing setup happens in the supervisor, it is inherited by the children.
  # This causes incomplete assignment of `group.instance.id` which is not expanded with proper
  # node identifier. This refreshes this if needed when in swarm.
  #
  # source://karafka//lib/karafka/routing/subscription_group.rb#108
  def refresh; end

  # @note Most of the time it should not include inactive topics but in case of pattern
  #   matching the matcher topics become inactive down the road, hence we filter out so
  #   they are later removed.
  # @return [false, Array<String>] names of topics to which we should subscribe or false when
  #   operating only on direct assignments
  #
  # source://karafka//lib/karafka/routing/subscription_group.rb#85
  def subscriptions; end

  # @note This is an alias for displaying in places where we print the stringified version.
  # @return [String] id of the subscription group
  #
  # source://karafka//lib/karafka/routing/subscription_group.rb#98
  def to_s; end

  # Returns the value of attribute topics.
  #
  # source://karafka//lib/karafka/routing/subscription_group.rb#17
  def topics; end

  private

  # @return [Hash] kafka settings are a bit special. They are exactly the same for all of the
  #   topics but they lack the group.id (unless explicitly) provided. To make it compatible
  #   with our routing engine, we inject it before it will go to the consumer
  #
  # source://karafka//lib/karafka/routing/subscription_group.rb#120
  def build_kafka; end

  # If we use static group memberships, there can be a case, where same instance id would
  # be set on many subscription groups as the group instance id from Karafka perspective is
  # set per config. Each instance even if they are subscribed to different topics needs to
  # have it fully unique. To make sure of that, we just add extra postfix at the end that
  # increments.
  #
  # We also handle a swarm case, where the same setup would run from many forked nodes, hence
  # affecting the instance id and causing conflicts
  #
  # @param kafka [Hash] kafka level config
  #
  # source://karafka//lib/karafka/routing/subscription_group.rb#144
  def inject_group_instance_id(kafka); end

  class << self
    # Generates new subscription group id that will be used in case of anonymous subscription
    #   groups
    #
    # @return [String] hex(6) compatible reproducible id
    #
    # source://karafka//lib/karafka/routing/subscription_group.rb#28
    def id; end
  end
end

# Lock for generating new ids safely
#
# source://karafka//lib/karafka/routing/subscription_group.rb#20
Karafka::Routing::SubscriptionGroup::ID_MUTEX = T.let(T.unsafe(nil), Thread::Mutex)

# rdkafka allows us to group topics subscriptions when they have same settings.
# This builder groups topics from a single consumer group into subscription groups that can be
# subscribed with one rdkafka connection.
# This way we save resources as having several rdkafka consumers under the hood is not the
# cheapest thing in a bigger system.
#
# In general, if we can, we try to subscribe to as many topics with one rdkafka connection as
# possible, but if not possible, we divide.
#
# source://karafka//lib/karafka/routing/subscription_groups_builder.rb#13
class Karafka::Routing::SubscriptionGroupsBuilder
  # @return [SubscriptionGroupsBuilder] a new instance of SubscriptionGroupsBuilder
  #
  # source://karafka//lib/karafka/routing/subscription_groups_builder.rb#27
  def initialize; end

  # @param topics [Karafka::Routing::Topics] all the topics based on which we want to build
  #   subscription groups
  # @return [Array<SubscriptionGroup>] all subscription groups we need in separate threads
  #
  # source://karafka//lib/karafka/routing/subscription_groups_builder.rb#34
  def call(topics); end

  private

  # @param topic [Karafka::Routing::Topic] topic for which we compute the grouping checksum
  # @return [Integer] checksum that we can use to check if topics have the same set of
  #   settings based on which we group
  #
  # source://karafka//lib/karafka/routing/subscription_groups_builder.rb#56
  def checksum(topic); end

  # Hook for optional expansion of groups based on subscription group features
  #
  # @param topics_array [Array<Routing::Topic>] group of topics that have the same settings
  #   and can use the same connection
  # @return [Array<Array<Routing::Topics>>] expanded groups
  #
  # source://karafka//lib/karafka/routing/subscription_groups_builder.rb#69
  def expand(topics_array); end
end

# Keys used to build up a hash for subscription groups distribution.
# In order to be able to use the same rdkafka connection for several topics, those keys need
# to have same values.
#
# source://karafka//lib/karafka/routing/subscription_groups_builder.rb#17
Karafka::Routing::SubscriptionGroupsBuilder::DISTRIBUTION_KEYS = T.let(T.unsafe(nil), Array)

# Topic stores all the details on how we should interact with Kafka given topic.
# It belongs to a consumer group as from 0.6 all the topics can work in the same consumer group
# It is a part of Karafka's DSL.
#
# source://karafka//lib/karafka/routing/topic.rb#8
class Karafka::Routing::Topic
  include ::Karafka::Routing::Features::ActiveJob::Topic
  include ::Karafka::Routing::Features::DeadLetterQueue::Topic
  include ::Karafka::Routing::Features::Declaratives::Topic
  include ::Karafka::Routing::Features::Deserializers::Topic
  include ::Karafka::Routing::Features::InlineInsights::Topic
  include ::Karafka::Routing::Features::ManualOffsetManagement::Topic

  # @param name [String, Symbol] of a topic on which we want to listen
  # @param consumer_group [Karafka::Routing::ConsumerGroup] owning consumer group of this topic
  # @return [Topic] a new instance of Topic
  #
  # source://karafka//lib/karafka/routing/topic.rb#34
  def initialize(name, consumer_group); end

  # Allows to disable topic by invoking this method and setting it to `false`.
  #
  # @param active [Boolean] should this topic be consumed or not
  #
  # source://karafka//lib/karafka/routing/topic.rb#83
  def active(active); end

  # @return [Boolean] should this topic be in use
  #
  # source://karafka//lib/karafka/routing/topic.rb#106
  def active?; end

  # @return [Class] consumer class that we should use
  #
  # source://karafka//lib/karafka/routing/topic.rb#63
  def consumer; end

  # Sets the attribute consumer
  #
  # @param value the value to set the attribute consumer to.
  #
  # source://karafka//lib/karafka/routing/topic.rb#10
  def consumer=(_arg0); end

  # @note This is just an alias to the `#consumer` method. We however want to use it internally
  #   instead of referencing the `#consumer`. We use this to indicate that this method returns
  #   class and not an instance. In the routing we want to keep the `#consumer Consumer`
  #   routing syntax, but for references outside, we should use this one.
  # @return [Class] consumer class that we should use
  #
  # source://karafka//lib/karafka/routing/topic.rb#101
  def consumer_class; end

  # Returns the value of attribute consumer_group.
  #
  # source://karafka//lib/karafka/routing/topic.rb#9
  def consumer_group; end

  # source://karafka//lib/karafka/routing/topic.rb#49
  def consumer_persistence; end

  # source://karafka//lib/karafka/routing/topic.rb#46
  def consumer_persistence=(_arg0); end

  # Returns the value of attribute id.
  #
  # source://karafka//lib/karafka/routing/topic.rb#9
  def id; end

  # source://karafka//lib/karafka/routing/topic.rb#49
  def initial_offset; end

  # source://karafka//lib/karafka/routing/topic.rb#46
  def initial_offset=(_arg0); end

  # source://karafka//lib/karafka/routing/topic.rb#49
  def kafka; end

  # source://karafka//lib/karafka/routing/topic.rb#46
  def kafka=(_arg0); end

  # source://karafka//lib/karafka/routing/topic.rb#49
  def max_messages; end

  # source://karafka//lib/karafka/routing/topic.rb#46
  def max_messages=(_arg0); end

  # source://karafka//lib/karafka/routing/topic.rb#49
  def max_wait_time; end

  # source://karafka//lib/karafka/routing/topic.rb#46
  def max_wait_time=(_arg0); end

  # Returns the value of attribute name.
  #
  # source://karafka//lib/karafka/routing/topic.rb#9
  def name; end

  # source://karafka//lib/karafka/routing/topic.rb#49
  def pause_max_timeout; end

  # source://karafka//lib/karafka/routing/topic.rb#46
  def pause_max_timeout=(_arg0); end

  # source://karafka//lib/karafka/routing/topic.rb#49
  def pause_timeout; end

  # source://karafka//lib/karafka/routing/topic.rb#46
  def pause_timeout=(_arg0); end

  # source://karafka//lib/karafka/routing/topic.rb#49
  def pause_with_exponential_backoff; end

  # source://karafka//lib/karafka/routing/topic.rb#46
  def pause_with_exponential_backoff=(_arg0); end

  # Full subscription group reference can be built only when we have knowledge about the
  # whole routing tree, this is why it is going to be set later on
  #
  # source://karafka//lib/karafka/routing/topic.rb#16
  def subscription_group; end

  # Full subscription group reference can be built only when we have knowledge about the
  # whole routing tree, this is why it is going to be set later on
  #
  # source://karafka//lib/karafka/routing/topic.rb#16
  def subscription_group=(_arg0); end

  # Returns the value of attribute subscription_group_details.
  #
  # source://karafka//lib/karafka/routing/topic.rb#12
  def subscription_group_details; end

  # Sets the attribute subscription_group_details
  #
  # @param value the value to set the attribute subscription_group_details to.
  #
  # source://karafka//lib/karafka/routing/topic.rb#12
  def subscription_group_details=(_arg0); end

  # @return [String] name of subscription that will go to librdkafka
  #
  # source://karafka//lib/karafka/routing/topic.rb#58
  def subscription_name; end

  # @note This is being used when we validate the consumer_group and its topics
  # @return [Hash] hash with all the topic attributes
  #
  # source://karafka//lib/karafka/routing/features/manual_offset_management/topic.rb#26
  def to_h; end
end

# Attributes we can inherit from the root unless they were defined on this level
#
# source://karafka//lib/karafka/routing/topic.rb#19
Karafka::Routing::Topic::INHERITABLE_ATTRIBUTES = T.let(T.unsafe(nil), Array)

# Abstraction layer on top of groups of topics
#
# source://karafka//lib/karafka/routing/topics.rb#8
class Karafka::Routing::Topics
  include ::Enumerable
  extend ::Forwardable

  # @param topics_array [Array<Karafka::Routing::Topic>] array with topics
  # @return [Topics] a new instance of Topics
  #
  # source://karafka//lib/karafka/routing/topics.rb#15
  def initialize(topics_array); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def <<(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def [](*args, **_arg1, &block); end

  # Allows us to remove elements from the topics
  #
  # Block to decide what to delete
  #
  # @param block [Proc]
  #
  # source://karafka//lib/karafka/routing/topics.rb#30
  def delete_if(&block); end

  # Yields each topic
  #
  # @param block [Proc] we want to yield with on each topic
  #
  # source://karafka//lib/karafka/routing/topics.rb#22
  def each(&block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def empty?(*args, **_arg1, &block); end

  # Finds topic by its name
  #
  # @param topic_name [String] topic name
  # @raise [Karafka::Errors::TopicNotFoundError] this should never happen. If you see it,
  #   please create an issue.
  # @return [Karafka::Routing::Topic]
  #
  # source://karafka//lib/karafka/routing/topics.rb#40
  def find(topic_name); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def last(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def map!(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def reverse!(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def size(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def sort_by!(*args, **_arg1, &block); end
end

# Class used to run the Karafka listeners in separate threads
#
# source://karafka//lib/karafka/runner.rb#5
class Karafka::Runner
  # @return [Runner] a new instance of Runner
  #
  # source://karafka//lib/karafka/runner.rb#6
  def initialize; end

  # Starts listening on all the listeners asynchronously and handles the jobs queue closing
  # after listeners are done with their work.
  #
  # source://karafka//lib/karafka/runner.rb#13
  def call; end
end

# Karafka consuming server class
#
# source://karafka//lib/karafka/server.rb#5
class Karafka::Server
  class << self
    # Jobs queue
    #
    # source://karafka//lib/karafka/server.rb#14
    def jobs_queue; end

    # Jobs queue
    #
    # source://karafka//lib/karafka/server.rb#14
    def jobs_queue=(_arg0); end

    # Set of consuming threads. Each consumer thread contains a single consumer
    #
    # source://karafka//lib/karafka/server.rb#8
    def listeners; end

    # Set of consuming threads. Each consumer thread contains a single consumer
    #
    # source://karafka//lib/karafka/server.rb#8
    def listeners=(_arg0); end

    # Quiets the Karafka server.
    #
    # Karafka will stop processing but won't quit the consumer group, so no rebalance will be
    # triggered until final shutdown.
    #
    # source://karafka//lib/karafka/server.rb#136
    def quiet; end

    # Method which runs app
    #
    # source://karafka//lib/karafka/server.rb#17
    def run; end

    # Starts Karafka with a supervision
    # (and it won't happen until we explicitly want to stop)
    #
    # @note We don't need to sleep because Karafka::Runner is locking and waiting to finish loop
    #
    # source://karafka//lib/karafka/server.rb#64
    def start; end

    # Stops Karafka with a supervision (as long as there is a shutdown timeout)
    # If consumers or workers won't stop in a given time frame, it will force them to exit
    #
    # @note This method is not async. It should not be executed from the workers as it will
    #   lock them forever. If you need to run Karafka shutdown from within workers threads,
    #   please start a separate thread to do so.
    #
    # source://karafka//lib/karafka/server.rb#74
    def stop; end

    # Set of workers
    #
    # source://karafka//lib/karafka/server.rb#11
    def workers; end

    # Set of workers
    #
    # source://karafka//lib/karafka/server.rb#11
    def workers=(_arg0); end

    private

    # @return [Karafka::Core::Configurable::Node] root config node
    #
    # source://karafka//lib/karafka/server.rb#145
    def config; end

    # @return [Karafka::Process] process wrapper instance used to catch system signal calls
    #
    # source://karafka//lib/karafka/server.rb#150
    def process; end
  end
end

# Module containing all Karafka setup related elements like configuration settings,
# config validations and configurators for external gems integration
#
# source://karafka//lib/karafka/app.rb#0
module Karafka::Setup; end

# To simplify the overall design, in Karafka we define all the rdkafka settings in one scope
# under `kafka`. rdkafka though does not like when producer options are passed to the
# consumer configuration and issues warnings. This target map is used as a filtering layer, so
# only appropriate settings go to both producer and consumer
#
# It is built based on https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md
#
# source://karafka//lib/karafka/setup/attributes_map.rb#11
module Karafka::Setup::AttributesMap
  class << self
    # Filter the provided settings leaving only the once applicable to the consumer
    #
    # @param kafka_settings [Hash] all kafka settings
    # @return [Hash] settings applicable to the consumer
    #
    # source://karafka//lib/karafka/setup/attributes_map.rb#304
    def consumer(kafka_settings); end

    # @note This method should not be used directly. It is only used to generate appropriate
    #   options list in case it would change
    # @private
    # @return [Hash<Symbol, Array<Symbol>>] hash with consumer and producer attributes list
    #   that is sorted.
    #
    # source://karafka//lib/karafka/setup/attributes_map.rb#320
    def generate; end

    # Filter the provided settings leaving only the once applicable to the producer
    #
    # @param kafka_settings [Hash] all kafka settings
    # @return [Hash] settings applicable to the producer
    #
    # source://karafka//lib/karafka/setup/attributes_map.rb#311
    def producer(kafka_settings); end
  end
end

# List of rdkafka consumer accepted attributes
#
# source://karafka//lib/karafka/setup/attributes_map.rb#13
Karafka::Setup::AttributesMap::CONSUMER = T.let(T.unsafe(nil), Array)

# List of rdkafka producer accepted attributes
#
# source://karafka//lib/karafka/setup/attributes_map.rb#152
Karafka::Setup::AttributesMap::PRODUCER = T.let(T.unsafe(nil), Array)

# Location of the file with rdkafka settings list
#
# source://karafka//lib/karafka/setup/attributes_map.rb#291
Karafka::Setup::AttributesMap::SOURCE = T.let(T.unsafe(nil), String)

# Configurator for setting up all the framework details that are required to make it work
#
# @note If you want to do some configurations after all of this is done, please add to
#   karafka/config a proper file (needs to inherit from Karafka::Setup::Configurators::Base
#   and implement setup method) after that everything will happen automatically
# @note This config object allows to create a 1 level nesting (nodes) only. This should be
#   enough and will still keep the code simple
# @see Karafka::Setup::Configurators::Base for more details about configurators api
#
# source://karafka//lib/karafka/setup/config.rb#14
class Karafka::Setup::Config
  extend ::Karafka::Core::Configurable
  extend ::Karafka::Core::Configurable::ClassMethods

  class << self
    # Configuring method
    #
    # @param block [Proc] block we want to execute with the config instance
    #
    # source://karafka//lib/karafka/setup/config.rb#340
    def setup(&block); end

    private

    # Sets up all the components that are based on the user configuration
    #
    # @note At the moment it is only WaterDrop
    #
    # source://karafka//lib/karafka/setup/config.rb#402
    def configure_components; end

    # Propagates the kafka setting defaults unless they are already present
    # This makes it easier to set some values that users usually don't change but still allows
    # them to overwrite the whole hash if they want to
    #
    # @param config [Karafka::Core::Configurable::Node] config of this producer
    #
    # source://karafka//lib/karafka/setup/config.rb#381
    def merge_kafka_defaults!(config); end
  end
end

# Defaults for kafka settings, that will be overwritten only if not present already
#
# source://karafka//lib/karafka/setup/config.rb#18
Karafka::Setup::Config::KAFKA_DEFAULTS = T.let(T.unsafe(nil), Hash)

# Contains settings that should not be used in production but make life easier in dev
#
# source://karafka//lib/karafka/setup/config.rb#31
Karafka::Setup::Config::KAFKA_DEV_DEFAULTS = T.let(T.unsafe(nil), Hash)

# Dsl for allowing to work with the configuration from the Karafka::App
# from the Karafka::Setup::Config
#
# @note Despite providing methods, everything is still persisted and fetched
#
# source://karafka//lib/karafka/setup/dsl.rb#8
module Karafka::Setup::Dsl
  # @return [Karafka::Config] config instance
  #
  # source://karafka//lib/karafka/setup/dsl.rb#16
  def config; end

  # Sets up the whole configuration
  #
  # @param block [Block] configuration block
  #
  # source://karafka//lib/karafka/setup/dsl.rb#11
  def setup(&block); end
end

# App status monitor
#
# source://karafka//lib/karafka/status.rb#5
class Karafka::Status
  # By default we are in the initializing state
  #
  # @return [Status] a new instance of Status
  #
  # source://karafka//lib/karafka/status.rb#35
  def initialize; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def conductor; end

  # @return [Boolean] true if we are in any of the status that would indicate we should no longer
  #   process incoming data. It is a meta status built from others and not a separate state in
  #   the sense of a state machine
  #
  # source://karafka//lib/karafka/status.rb#78
  def done?; end

  # source://karafka//lib/karafka/status.rb#56
  def initialize!; end

  # source://karafka//lib/karafka/status.rb#56
  def initialized!; end

  # source://karafka//lib/karafka/status.rb#52
  def initialized?; end

  # source://karafka//lib/karafka/status.rb#52
  def initializing?; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def monitor; end

  # source://karafka//lib/karafka/status.rb#56
  def quiet!; end

  # source://karafka//lib/karafka/status.rb#52
  def quiet?; end

  # source://karafka//lib/karafka/status.rb#56
  def quieted!; end

  # source://karafka//lib/karafka/status.rb#52
  def quieting?; end

  # Resets the status state
  # This is used mostly in the integration suite
  #
  # source://karafka//lib/karafka/status.rb#46
  def reset!; end

  # source://karafka//lib/karafka/status.rb#56
  def run!; end

  # source://karafka//lib/karafka/status.rb#52
  def running?; end

  # source://karafka//lib/karafka/status.rb#56
  def stop!; end

  # source://karafka//lib/karafka/status.rb#56
  def stopped!; end

  # source://karafka//lib/karafka/status.rb#52
  def stopped?; end

  # source://karafka//lib/karafka/status.rb#52
  def stopping?; end

  # source://karafka//lib/karafka/status.rb#56
  def supervise!; end

  # source://karafka//lib/karafka/status.rb#52
  def supervising?; end

  # source://karafka//lib/karafka/status.rb#56
  def terminate!; end

  # source://karafka//lib/karafka/status.rb#52
  def terminated?; end

  # @return [String] stringified current app status
  #
  # source://karafka//lib/karafka/status.rb#40
  def to_s; end
end

# Mutex to ensure that state transitions are thread-safe
#
# source://karafka//lib/karafka/status.rb#30
Karafka::Status::MUTEX = T.let(T.unsafe(nil), Thread::Mutex)

# Available states and their transitions.
#
# source://karafka//lib/karafka/status.rb#12
Karafka::Status::STATES = T.let(T.unsafe(nil), Hash)

# Namespace for the Swarm capabilities.
#
# Karafka in the swarm mode will fork additional processes and use the parent process as a
# supervisor. This capability allows to run multiple processes alongside but saves some memory
# due to CoW.
#
# source://karafka//lib/karafka/swarm.rb#9
module Karafka::Swarm
  class << self
    # Raises an error if swarm is not supported on a given platform
    #
    # @raise [Errors::UnsupportedOptionError]
    #
    # source://karafka//lib/karafka/swarm.rb#12
    def ensure_supported!; end

    # @return [Boolean] true if fork API and pidfd OS API are available, otherwise false
    #
    # source://karafka//lib/karafka/swarm.rb#22
    def supported?; end
  end
end

# Simple listener for swarm nodes that:
#   - reports once in a while to make sure that supervisor is aware we do not hang
#   - makes sure we did not become an orphan and if so, exits
#
# source://karafka//lib/karafka/swarm/liveness_listener.rb#8
class Karafka::Swarm::LivenessListener
  include ::Karafka::Core::Helpers::Time

  # @return [LivenessListener] a new instance of LivenessListener
  #
  # source://karafka//lib/karafka/swarm/liveness_listener.rb#16
  def initialize; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def liveness_interval; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def node; end

  # Since there may be many statistics emitted from multiple listeners, we do not want to write
  # statuses that often. Instead we do it only once in a while which should be enough
  #
  # While this may provide a small lag in the orphaned detection, it does not really matter
  # as it will be picked up fast enough.
  #
  # @param _event [Karafka::Core::Monitoring::Event]
  #
  # source://karafka//lib/karafka/swarm/liveness_listener.rb#27
  def on_statistics_emitted(_event); end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def orphaned_exit_code; end

  private

  # Runs requested code once in a while
  #
  # source://karafka//lib/karafka/swarm/liveness_listener.rb#44
  def periodically; end

  # Wraps the logic with a mutex
  #
  # @param block [Proc] code we want to run in mutex
  #
  # source://karafka//lib/karafka/swarm/liveness_listener.rb#39
  def synchronize(&block); end
end

# Manager similar to the one for threads but managing processing nodes
# It starts nodes and keeps an eye on them.
#
# In any of the nodes is misbehaving (based on liveness listener) it will be restarted.
# Initially gracefully but if won't stop itself, it will be forced to.
#
# @note This is intended to run in the supervisor under mutexes (when needed)
#
# source://karafka//lib/karafka/swarm/manager.rb#12
class Karafka::Swarm::Manager
  include ::Karafka::Core::Helpers::Time

  # @return [Manager] a new instance of Manager
  #
  # source://karafka//lib/karafka/swarm/manager.rb#32
  def initialize; end

  # Collects all processes statuses
  #
  # source://karafka//lib/karafka/swarm/manager.rb#62
  def cleanup; end

  # Checks on nodes if they are ok one after another
  #
  # source://karafka//lib/karafka/swarm/manager.rb#78
  def control; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def monitor; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def node_report_timeout; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def node_restart_timeout; end

  # @return [Array<Node>] All nodes that manager manages
  #
  # source://karafka//lib/karafka/swarm/manager.rb#30
  def nodes; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def nodes_count; end

  # Attempts to quiet all the nodes
  #
  # source://karafka//lib/karafka/swarm/manager.rb#47
  def quiet; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def shutdown_timeout; end

  # Sends given signal to all nodes
  #
  # @param signal [String] signal name
  #
  # source://karafka//lib/karafka/swarm/manager.rb#68
  def signal(signal); end

  # Starts all the expected nodes for the first time
  #
  # source://karafka//lib/karafka/swarm/manager.rb#38
  def start; end

  # Attempts to stop all the nodes
  #
  # source://karafka//lib/karafka/swarm/manager.rb#52
  def stop; end

  # @return [Boolean] true if none of the nodes is running
  #
  # source://karafka//lib/karafka/swarm/manager.rb#73
  def stopped?; end

  # Terminates all the nodes
  #
  # source://karafka//lib/karafka/swarm/manager.rb#57
  def terminate; end

  private

  # Cleans up a dead process and remembers time of death for restart after a period.
  #
  # @param statuses [Hash] hash with statuses transitions with times
  # @param node [Swarm::Node] we're checking
  # @return [Boolean] should it be the last action taken on this node in this run
  #
  # source://karafka//lib/karafka/swarm/manager.rb#176
  def cleanup_one(statuses, node); end

  # Are we over certain time from an event happening
  #
  # @param event_time [Float] when something happened
  # @param delay [Float] how long should we wait
  # @return [Boolean] true if we're past the delay
  #
  # source://karafka//lib/karafka/swarm/manager.rb#224
  def over?(event_time, delay); end

  # Restarts the node if there was enough of a backoff.
  #
  # We always wait a bit to make sure, we do not overload the system in case forks would be
  # killed for some external reason.
  #
  # @param statuses [Hash] hash with statuses transitions with times
  # @param node [Swarm::Node] we're checking
  # @return [Boolean] should it be the last action taken on this node in this run
  #
  # source://karafka//lib/karafka/swarm/manager.rb#193
  def restart_after_timeout(statuses, node); end

  # Starts a new node (or restarts dead)
  #
  # @param node [Swarm::Node] we're starting
  #
  # source://karafka//lib/karafka/swarm/manager.rb#204
  def start_one(node); end

  # Checks if there is any new liveness report from given node and if yes, issues stop if it
  # reported it is not healthy.
  #
  # @param statuses [Hash] hash with statuses transitions with times
  # @param node [Swarm::Node] we're checking
  # @return [Boolean] should it be the last action taken on this node in this run
  #
  # source://karafka//lib/karafka/swarm/manager.rb#123
  def stop_if_not_healthy(statuses, node); end

  # If node stopped responding, starts the stopping procedure.
  #
  # @param statuses [Hash] hash with statuses transitions with times
  # @param node [Swarm::Node] we're checking
  # @return [Boolean] should it be the last action taken on this node in this run
  #
  # source://karafka//lib/karafka/swarm/manager.rb#151
  def stop_if_not_responding(statuses, node); end

  # If we've issued a stop to this process and it does not want to stop in the period, kills it
  #
  # @param statuses [Hash] hash with statuses transitions with times
  # @param node [Swarm::Node] we're checking
  # @return [Boolean] should it be the last action taken on this node in this run
  #
  # source://karafka//lib/karafka/swarm/manager.rb#102
  def terminate_if_hanging(statuses, node); end
end

# Status we issue when we decide to shutdown unresponsive node
# We use -1 because nodes are expected to report 0+ statuses and we can use negative numbers
# for non-node based statuses
#
# source://karafka//lib/karafka/swarm/manager.rb#25
Karafka::Swarm::Manager::NOT_RESPONDING_SHUTDOWN_STATUS = T.let(T.unsafe(nil), Integer)

# Represents a single forked process node in a swarm
# Provides simple API to control forks and check their status
#
# @note Some of this APIs are for parent process only
# @note Keep in mind this can be used in both forks and supervisor and has a slightly different
#   role in each. In case of the supervisor it is used to get information about the child and
#   make certain requests to it. In case of child, it is used to provide zombie-fencing and
#   report liveness
#
# source://karafka//lib/karafka/swarm/node.rb#14
class Karafka::Swarm::Node
  # @param id [Integer] number of the fork. Used for uniqueness setup for group client ids and
  #   other stuff where we need to know a unique reference of the fork in regards to the rest
  #   of them.
  # @param parent_pid [Integer] parent pid for zombie fencing
  # @return [Node] a new instance of Node
  #
  # source://karafka//lib/karafka/swarm/node.rb#34
  def initialize(id, parent_pid); end

  # @note Parent API
  # @note Keep in mind that the fact that process is alive does not mean it is healthy
  # @return [Boolean] true if node is alive or false if died
  #
  # source://karafka//lib/karafka/swarm/node.rb#116
  def alive?; end

  # Removes the dead process from the processes table
  #
  # source://karafka//lib/karafka/swarm/node.rb#151
  def cleanup; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def config; end

  # Indicates that this node is doing well
  #
  # @note Child API
  #
  # source://karafka//lib/karafka/swarm/node.rb#83
  def healthy; end

  # @return [Integer] id of the node. Useful for client.group.id assignment
  #
  # source://karafka//lib/karafka/swarm/node.rb#25
  def id; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def kafka; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def liveness_listener; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def monitor; end

  # @note Child API
  # @return [Boolean] true if node is orphaned or false otherwise. Used for orphans detection.
  #
  # source://karafka//lib/karafka/swarm/node.rb#122
  def orphaned?; end

  # @return [Integer] pid of the node
  #
  # source://karafka//lib/karafka/swarm/node.rb#28
  def pid; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def process; end

  # Sends sigtstp to the node
  #
  # @note Parent API
  #
  # source://karafka//lib/karafka/swarm/node.rb#134
  def quiet; end

  # Sends provided signal to the node
  #
  # @param signal [String]
  #
  # source://karafka//lib/karafka/swarm/node.rb#146
  def signal(signal); end

  # Starts a new fork and:
  #   - stores pid and parent reference
  #   - makes sure reader pipe is closed
  #   - sets up liveness listener
  #   - recreates producer and web producer
  #
  # @note Parent API
  #
  # source://karafka//lib/karafka/swarm/node.rb#45
  def start; end

  # @note Parent API
  # @note If there were few issues reported, it will pick the one with highest number
  # @return [Integer] This returns following status code depending on the data:
  #   - -1 if node did not report anything new
  #   - 0 if all good,
  #   - positive number if there was a problem (indicates error code)
  #
  # source://karafka//lib/karafka/swarm/node.rb#104
  def status; end

  # Sends sigterm to the node
  #
  # @note Parent API
  #
  # source://karafka//lib/karafka/swarm/node.rb#128
  def stop; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def swarm; end

  # Terminates node
  #
  # @note Parent API
  #
  # source://karafka//lib/karafka/swarm/node.rb#140
  def terminate; end

  # Indicates, that this node has failed
  #
  # @note Child API
  # @note We convert this to string to normalize the API
  # @param reason_code [Integer, String] numeric code we want to use to indicate that we are
  #   not healthy. Anything bigger than 0 will be considered not healthy. Useful it we want to
  #   have complex health-checking with reporting.
  #
  # source://karafka//lib/karafka/swarm/node.rb#93
  def unhealthy(reason_code = T.unsafe(nil)); end

  private

  # Reads in a non-blocking way provided content
  #
  # @note Parent API
  # @return [String, false] Content from the pipe or false if nothing or something went wrong
  #
  # source://karafka//lib/karafka/swarm/node.rb#160
  def read; end

  # Writes in a non-blocking way provided content into the pipe
  #
  # @note Child API
  # @param content [Integer, String] anything we want to write to the parent
  # @return [Boolean] true if ok, otherwise false
  #
  # source://karafka//lib/karafka/swarm/node.rb#170
  def write(content); end
end

# Pidfd Linux representation wrapped with Ruby for communication within Swarm
# It is more stable than using `#pid` and `#ppid` + signals and cheaper
#
# source://karafka//lib/karafka/swarm/pidfd.rb#7
class Karafka::Swarm::Pidfd
  extend ::FFI::Library

  # @param pid [Integer] pid of the node we want to work with
  # @return [Pidfd] a new instance of Pidfd
  #
  # source://karafka//lib/karafka/swarm/pidfd.rb#63
  def initialize(pid); end

  # @return [Boolean] true if given process is alive, false if no longer
  #
  # source://karafka//lib/karafka/swarm/pidfd.rb#72
  def alive?; end

  # Cleans the zombie process
  #
  # @note This should run **only** on processes that exited, otherwise will wait
  #
  # source://karafka//lib/karafka/swarm/pidfd.rb#90
  def cleanup; end

  def fdpid_open(*_arg0); end
  def fdpid_signal(*_arg0); end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def pidfd_open_syscall; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def pidfd_signal_syscall; end

  # Sends given signal to the process using its pidfd
  #
  # @note It will not send signals to dead processes
  # @param sig_name [String] signal name
  # @return [Boolean] true if signal was sent, otherwise false or error raised. `false`
  #   returned when we attempt to send a signal to a dead process
  #
  # source://karafka//lib/karafka/swarm/pidfd.rb#109
  def signal(sig_name); end

  def waitid(*_arg0); end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def waitid_syscall; end

  private

  # Opens a pidfd for the provided pid
  #
  # @param pid [Integer]
  # @raise [Errors::PidfdOpenFailedError]
  # @return [Integer] pidfd
  #
  # source://karafka//lib/karafka/swarm/pidfd.rb#134
  def open(pid); end

  class << self
    def fdpid_open(*_arg0); end
    def fdpid_signal(*_arg0); end

    # @return [Boolean] true if syscall is supported via FFI
    #
    # source://karafka//lib/karafka/swarm/pidfd.rb#43
    def supported?; end

    def waitid(*_arg0); end
  end
end

# source://karafka//lib/karafka/swarm/pidfd.rb#25
Karafka::Swarm::Pidfd::API_SUPPORTED = T.let(T.unsafe(nil), TrueClass)

# https://github.com/torvalds/linux/blob/7e90b5c295/include/uapi/linux/wait.h#L20
#
# source://karafka//lib/karafka/swarm/pidfd.rb#34
Karafka::Swarm::Pidfd::P_PIDFD = T.let(T.unsafe(nil), Integer)

# Wait for child processes that have exited
#
# source://karafka//lib/karafka/swarm/pidfd.rb#37
Karafka::Swarm::Pidfd::WEXITED = T.let(T.unsafe(nil), Integer)

# Supervisor that starts forks and uses monitor to monitor them. Also handles shutdown of
# all the processes including itself.
#
# In case any node dies, it will be restarted.
#
# @note Technically speaking supervisor is never in the running state because we do not want
#   to have any sockets or anything else on it that could break under forking.
#   It has its own "supervising" state from which it can go to the final shutdown.
#
# source://karafka//lib/karafka/swarm/supervisor.rb#13
class Karafka::Swarm::Supervisor
  include ::Karafka::Core::Helpers::Time

  # @return [Supervisor] a new instance of Supervisor
  #
  # source://karafka//lib/karafka/swarm/supervisor.rb#35
  def initialize; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def forceful_exit_code; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def manager; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def monitor; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def process; end

  # Creates needed number of forks, installs signals and starts supervision
  #
  # source://karafka//lib/karafka/swarm/supervisor.rb#41
  def run; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def shutdown_timeout; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def supervision_interval; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def supervision_sleep; end

  # source://karafka//lib/karafka/helpers/config_importer.rb#22
  def swarm; end

  private

  # Checks on the children nodes and takes appropriate actions.
  # - If node is dead, will cleanup
  # - If node is no longer reporting as healthy will start a graceful shutdown
  # - If node does not want to close itself gracefully, will kill it
  # - If node was dead, new node will be started as a recovery means
  #
  # source://karafka//lib/karafka/swarm/supervisor.rb#179
  def control; end

  # Keeps the lock on the queue so we control nodes only when it is needed
  #
  # @note We convert to seconds since the queue timeout requires seconds
  #
  # source://karafka//lib/karafka/swarm/supervisor.rb#92
  def lock; end

  # Moves all the nodes and itself to the quiet state
  #
  # source://karafka//lib/karafka/swarm/supervisor.rb#162
  def quiet; end

  # Sends desired signal to each node
  #
  # @param signal [String]
  #
  # source://karafka//lib/karafka/swarm/supervisor.rb#194
  def signal(signal); end

  # Stops all the nodes and supervisor once all nodes are dead.
  # It will forcefully stop all nodes if they exit the shutdown timeout. While in theory each
  # of the nodes anyhow has its own supervisor, this is a last resort to stop everything.
  #
  # source://karafka//lib/karafka/swarm/supervisor.rb#104
  def stop; end

  # Frees the lock on events that could require nodes control
  #
  # source://karafka//lib/karafka/swarm/supervisor.rb#97
  def unlock; end
end

# How long extra should we wait on shutdown before forceful termination
# We add this time because we send signals and it always can take a bit of time for them
# to reach out nodes and be processed to start the shutdown flow. Because of that and
# because we always want to give all nodes all the time of `shutdown_timeout` they are
# expected to have, we add this just to compensate.
#
# source://karafka//lib/karafka/swarm/supervisor.rb#31
Karafka::Swarm::Supervisor::SHUTDOWN_GRACE_PERIOD = T.let(T.unsafe(nil), Integer)

# Time trackers module.
#
# Time trackers are used to track time in context of having a time poll (amount of time
# available for processing) or a pausing engine (pause for a time period).
#
# source://karafka//lib/karafka.rb#0
module Karafka::TimeTrackers; end

# Base class for all the time-trackers.
#
# source://karafka//lib/karafka/time_trackers/base.rb#10
class Karafka::TimeTrackers::Base
  include ::Karafka::Core::Helpers::Time
end

# Handles Kafka topic partition pausing and resuming with exponential back-offs.
# Since expiring and pausing can happen from both consumer and listener, this needs to be
# thread-safe.
#
# @note We do not have to worry about performance implications of a mutex wrapping most of the
#   code here, as this is not a frequently used tracker. It is active only once per batch in
#   case of long-running-jobs and upon errors.
#
# source://karafka//lib/karafka/time_trackers/pause.rb#12
class Karafka::TimeTrackers::Pause < ::Karafka::TimeTrackers::Base
  # @example
  #   options = { timeout: 1000, max_timeout: 1000, exponential_backoff: false }
  #   pause = Karafka::TimeTrackers::Pause.new(**options)
  #   pause.expired? #=> true
  #   pause.paused? #=> false
  #   pause.pause
  #   pause.increment
  #   sleep(1.1)
  #   pause.paused? #=> true
  #   pause.expired? #=> true
  #   pause.attempt #=> 1
  #   pause.pause
  #   pause.increment
  #   pause.attempt #=> 2
  #   pause.paused? #=> true
  #   pause.expired? #=> false
  #   pause.resume
  #   pause.attempt #=> 2
  #   pause.paused? #=> false
  #   pause.reset
  #   pause.attempt #=> 0
  # @param timeout [Integer] how long should we wait when anything went wrong (in ms)
  # @param max_timeout [Integer, nil] if exponential is on, what is the max value we can reach
  #   exponentially on which we will stay
  # @param exponential_backoff [Boolean] should we wait exponentially or with the same
  #   timeout value
  # @return [Karafka::TimeTrackers::Pause]
  #
  # source://karafka//lib/karafka/time_trackers/pause.rb#42
  def initialize(timeout:, max_timeout:, exponential_backoff:); end

  # Returns the value of attribute attempt.
  #
  # source://karafka//lib/karafka/time_trackers/pause.rb#13
  def attempt; end

  # Returns the value of attribute current_timeout.
  #
  # source://karafka//lib/karafka/time_trackers/pause.rb#13
  def current_timeout; end

  # Expires the pause, so it can be considered expired
  #
  # source://karafka//lib/karafka/time_trackers/pause.rb#82
  def expire; end

  # @return [Boolean] did the pause expire
  #
  # source://karafka//lib/karafka/time_trackers/pause.rb#96
  def expired?; end

  # Increments the number of attempt by 1
  #
  # source://karafka//lib/karafka/time_trackers/pause.rb#67
  def increment; end

  # Pauses the processing from now till the end of the interval (backoff or non-backoff)
  # and records the attempt.
  #
  # @note Providing this value can be useful when we explicitly want to pause for a certain
  #   period of time, outside of any regular pausing logic
  # @param timeout [Integer] timeout value in milliseconds that overwrites the default timeout
  #
  # source://karafka//lib/karafka/time_trackers/pause.rb#58
  def pause(timeout = T.unsafe(nil)); end

  # @return [Boolean] are we paused from processing
  #
  # source://karafka//lib/karafka/time_trackers/pause.rb#89
  def paused?; end

  # Resets the pause attempt count.
  #
  # source://karafka//lib/karafka/time_trackers/pause.rb#103
  def reset; end

  # Marks the pause as resumed.
  #
  # source://karafka//lib/karafka/time_trackers/pause.rb#74
  def resume; end

  private

  # Computers the exponential backoff
  #
  # @return [Integer] backoff in milliseconds
  #
  # source://karafka//lib/karafka/time_trackers/pause.rb#113
  def backoff_interval; end
end

# Object used to keep track of time we've used running certain operations. Polling is
# running in a single thread, thus we do not have to worry about this being thread-safe.
#
# @example Keep track of sleeping and stop after 3 seconds of 0.1 sleep intervals
#   time_poll = Poll.new(3000)
#   time_poll.start
#
#   until time_poll.exceeded?
#   time_poll.start
#   puts "I have #{time_poll.remaining.to_i}ms remaining to sleep..."
#   sleep(0.1)
#   time_poll.checkpoint
#   end
#
# source://karafka//lib/karafka/time_trackers/poll.rb#18
class Karafka::TimeTrackers::Poll < ::Karafka::TimeTrackers::Base
  # @param total_time [Integer] amount of milliseconds before we exceed the given time limit
  # @return [TimeTracker] time poll instance
  #
  # source://karafka//lib/karafka/time_trackers/poll.rb#23
  def initialize(total_time); end

  # Returns the value of attribute attempts.
  #
  # source://karafka//lib/karafka/time_trackers/poll.rb#19
  def attempts; end

  # Sleeps for amount of time matching attempt, so we sleep more with each attempt in case of
  #   a retry.
  #
  # source://karafka//lib/karafka/time_trackers/poll.rb#53
  def backoff; end

  # Stops time tracking of a given piece of code and updates the remaining time.
  #
  # source://karafka//lib/karafka/time_trackers/poll.rb#41
  def checkpoint; end

  # @return [Boolean] did we exceed the time limit
  #
  # source://karafka//lib/karafka/time_trackers/poll.rb#30
  def exceeded?; end

  # Returns the value of attribute remaining.
  #
  # source://karafka//lib/karafka/time_trackers/poll.rb#19
  def remaining; end

  # @return [Boolean] If anything went wrong, can we retry after a backoff period or not
  #   (do we have enough time)
  #
  # source://karafka//lib/karafka/time_trackers/poll.rb#47
  def retryable?; end

  # Starts time tracking.
  #
  # source://karafka//lib/karafka/time_trackers/poll.rb#35
  def start; end

  private

  # @return [Integer] milliseconds of the backoff time
  #
  # source://karafka//lib/karafka/time_trackers/poll.rb#64
  def backoff_interval; end
end

# Current Karafka version
#
# source://karafka//lib/karafka/version.rb#6
Karafka::VERSION = T.let(T.unsafe(nil), String)

class Rdkafka::Opaque
  include ::Karafka::Patches::Rdkafka::Opaque
end
