# typed: true

# DO NOT EDIT MANUALLY
# This is an autogenerated file for types exported from the `langchainrb` gem.
# Please instead update this file by running `bin/tapioca gem langchainrb`.


# Langchain.rb a is library for building LLM-backed Ruby applications. It is an abstraction layer that sits on top of the emerging AI-related tools that makes it easy for developers to consume and string those services together.
#
# = Installation
# Install the gem and add to the application's Gemfile by executing:
#
#     $ bundle add langchainrb
#
# If bundler is not being used to manage dependencies, install the gem by executing:
#
#     $ gem install langchainrb
#
# Require the gem to start using it:
#
#     require "langchain"
#
# = Concepts
#
# == Processors
# Processors load and parse/process various data types such as CSVs, PDFs, Word documents, HTML pages, and others.
#
# == Chunkers
# Chunkers split data based on various available options such as delimeters, chunk sizes or custom-defined functions. Chunkers are used when data needs to be split up before being imported in vector databases.
#
# == Prompts
# Prompts are structured inputs to the LLMs. Prompts provide instructions, context and other user input that LLMs use to generate responses.
#
# == Large Language Models (LLMs)
# LLM is a language model consisting of a neural network with many parameters (typically billions of weights or more), trained on large quantities of unlabeled text using self-supervised learning or semi-supervised learning.
#
# == Vectorsearch Databases
# Vector database is a type of database that stores data as high-dimensional vectors, which are mathematical representations of features or attributes. Each vector has a certain number of dimensions, which can range from tens to thousands, depending on the complexity and granularity of the data.
#
# == Embedding
# Word embedding or word vector is an approach with which we represent documents and words. It is defined as a numeric vector input that allows words with similar meanings to have the same representation. It can approximate meaning and represent a word in a lower dimensional space.
#
#
# = Logging
#
# Langchain.rb uses standard logging mechanisms and defaults to :debug level. Most messages are at info level, but we will add debug or warn statements as needed. To show all log messages:
#
# Langchain.logger.level = :info
#
# source://langchainrb//lib/langchain.rb#89
module Langchain
  class << self
    # @return [ContextualLogger]
    #
    # source://langchainrb//lib/langchain.rb#92
    def logger; end

    # @param logger [Logger]
    # @return [ContextualLogger]
    #
    # source://langchainrb//lib/langchain.rb#96
    def logger=(logger); end

    # @return [Pathname]
    #
    # source://langchainrb//lib/langchain.rb#101
    def root; end
  end
end

# Assistants are Agent-like objects that leverage helpful instructions, LLMs, tools and knowledge to respond to user queries.
# Assistants can be configured with an LLM of your choice, any vector search database and easily extended with additional tools.
#
# Usage:
#     llm = Langchain::LLM::GoogleGemini.new(api_key: ENV["GOOGLE_GEMINI_API_KEY"])
#     assistant = Langchain::Assistant.new(
#       llm: llm,
#       instructions: "You're a News Reporter AI",
#       tools: [Langchain::Tool::NewsRetriever.new(api_key: ENV["NEWS_API_KEY"])]
#     )
#
# source://langchainrb//lib/langchain/assistants/assistant.rb#14
class Langchain::Assistant
  extend ::Forwardable

  # Create a new assistant
  #
  # @param llm [Langchain::LLM::Base] LLM instance that the assistant will use
  # @param thread [Langchain::Thread] The thread that'll keep track of the conversation
  # @param tools [Array<Langchain::Tool::Base>] Tools that the assistant has access to
  # @param instructions [String] The system instructions to include in the thread
  # @raise [ArgumentError]
  # @return [Assistant] a new instance of Assistant
  #
  # source://langchainrb//lib/langchain/assistants/assistant.rb#34
  def initialize(llm:, thread: T.unsafe(nil), tools: T.unsafe(nil), instructions: T.unsafe(nil)); end

  # Add a user message to the thread
  #
  # @param content [String] The content of the message
  # @param role [String] The role attribute of the message. Default: "user"
  # @param tool_calls [Array<Hash>] The tool calls to include in the message
  # @param tool_call_id [String] The ID of the tool call to include in the message
  # @return [Array<Langchain::Message>] The messages in the thread
  #
  # source://langchainrb//lib/langchain/assistants/assistant.rb#67
  def add_message(content: T.unsafe(nil), role: T.unsafe(nil), tool_calls: T.unsafe(nil), tool_call_id: T.unsafe(nil)); end

  # Add a user message to the thread and run the assistant
  #
  # @param content [String] The content of the message
  # @param auto_tool_execution [Boolean] Whether or not to automatically run tools
  # @return [Array<Langchain::Message>] The messages in the thread
  #
  # source://langchainrb//lib/langchain/assistants/assistant.rb#138
  def add_message_and_run(content:, auto_tool_execution: T.unsafe(nil)); end

  # Delete all messages in the thread
  #
  # @return [Array] Empty messages array
  #
  # source://langchainrb//lib/langchain/assistants/assistant.rb#164
  def clear_thread!; end

  # Returns the value of attribute instructions.
  #
  # source://langchainrb//lib/langchain/assistants/assistant.rb#18
  def instructions; end

  # Set new instructions
  #
  # @param New [String] instructions that will be set as a system message
  # @return [Array<Langchain::Message>] The messages in the thread
  #
  # source://langchainrb//lib/langchain/assistants/assistant.rb#173
  def instructions=(new_instructions); end

  # Returns the value of attribute llm.
  #
  # source://langchainrb//lib/langchain/assistants/assistant.rb#18
  def llm; end

  # source://forwardable/1.3.3/forwardable.rb#231
  def messages(*args, **_arg1, &block); end

  # source://forwardable/1.3.3/forwardable.rb#231
  def messages=(*args, **_arg1, &block); end

  # Run the assistant
  #
  # @param auto_tool_execution [Boolean] Whether or not to automatically run tools
  # @return [Array<Langchain::Message>] The messages in the thread
  #
  # source://langchainrb//lib/langchain/assistants/assistant.rb#76
  def run(auto_tool_execution: T.unsafe(nil)); end

  # Submit tool output to the thread
  #
  # @param tool_call_id [String] The ID of the tool call to submit output for
  # @param output [String] The output of the tool
  # @return [Array<Langchain::Message>] The messages in the thread
  #
  # source://langchainrb//lib/langchain/assistants/assistant.rb#148
  def submit_tool_output(tool_call_id:, output:); end

  # Returns the value of attribute thread.
  #
  # source://langchainrb//lib/langchain/assistants/assistant.rb#18
  def thread; end

  # Returns the value of attribute tools.
  #
  # source://langchainrb//lib/langchain/assistants/assistant.rb#19
  def tools; end

  # Sets the attribute tools
  #
  # @param value the value to set the attribute tools to.
  #
  # source://langchainrb//lib/langchain/assistants/assistant.rb#19
  def tools=(_arg0); end

  private

  # Build a message
  #
  # @param role [String] The role of the message
  # @param content [String] The content of the message
  # @param tool_calls [Array<Hash>] The tool calls to include in the message
  # @param tool_call_id [String] The ID of the tool call to include in the message
  # @return [Langchain::Message] The Message object
  #
  # source://langchainrb//lib/langchain/assistants/assistant.rb#294
  def build_message(role:, content: T.unsafe(nil), tool_calls: T.unsafe(nil), tool_call_id: T.unsafe(nil)); end

  # Call to the LLM#chat() method
  #
  # @return [Langchain::LLM::BaseResponse] The LLM response object
  #
  # source://langchainrb//lib/langchain/assistants/assistant.rb#189
  def chat_with_llm; end

  # Extract the tool call information from the Anthropic tool call hash
  #
  # @param tool_call [Hash] The tool call hash, format: {"type"=>"tool_use", "id"=>"toolu_01TjusbFApEbwKPRWTRwzadR", "name"=>"news_retriever__get_top_headlines", "input"=>{"country"=>"us", "page_size"=>10}}], "stop_reason"=>"tool_use"}
  # @return [Array] The tool call information
  #
  # source://langchainrb//lib/langchain/assistants/assistant.rb#263
  def extract_anthropic_tool_call(tool_call:); end

  # Extract the tool call information from the Google Gemini tool call hash
  #
  # @param tool_call [Hash] The tool call hash, format: {"functionCall"=>{"name"=>"weather__execute", "args"=>{"input"=>"NYC"}}}
  # @return [Array] The tool call information
  #
  # source://langchainrb//lib/langchain/assistants/assistant.rb#277
  def extract_google_gemini_tool_call(tool_call:); end

  # Extract the tool call information from the OpenAI tool call hash
  #
  # @param tool_call [Hash] The tool call hash
  # @return [Array] The tool call information
  #
  # source://langchainrb//lib/langchain/assistants/assistant.rb#249
  def extract_openai_tool_call(tool_call:); end

  # Run the tools automatically
  #
  # @param tool_calls [Array<Hash>] The tool calls to run
  #
  # source://langchainrb//lib/langchain/assistants/assistant.rb#216
  def run_tools(tool_calls); end
end

# source://langchainrb//lib/langchain/assistants/assistant.rb#21
Langchain::Assistant::SUPPORTED_LLMS = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/chunk.rb#4
class Langchain::Chunk
  # Initialize a new chunk
  #
  # @param text [String]
  # @return [Langchain::Chunk]
  #
  # source://langchainrb//lib/langchain/chunk.rb#12
  def initialize(text:); end

  # The chunking process is the process of splitting a document into smaller chunks and creating instances of Langchain::Chunk
  #
  # source://langchainrb//lib/langchain/chunk.rb#7
  def text; end
end

# source://langchainrb//lib/langchain/chunker/base.rb#4
module Langchain::Chunker; end

# = Chunkers
# Chunkers are used to split documents into smaller chunks before indexing into vector search databases.
# Otherwise large documents, when retrieved and passed to LLMs, may hit the context window limits.
#
# == Available chunkers
#
# - {Langchain::Chunker::RecursiveText}
# - {Langchain::Chunker::Text}
# - {Langchain::Chunker::Semantic}
# - {Langchain::Chunker::Sentence}
#
# source://langchainrb//lib/langchain/chunker/base.rb#15
class Langchain::Chunker::Base
  # @raise [NotImplementedError]
  # @return [Array<Langchain::Chunk>]
  #
  # source://langchainrb//lib/langchain/chunker/base.rb#17
  def chunks; end
end

# Simple text chunker
#
# Usage:
#     Langchain::Chunker::Markdown.new(text).chunks
#
# source://langchainrb//lib/langchain/chunker/markdown.rb#11
class Langchain::Chunker::Markdown < ::Langchain::Chunker::Base
  # @param text [String]
  # @param chunk_size [Integer]
  # @param chunk_overlap [Integer]
  # @param separator [String]
  # @return [Markdown] a new instance of Markdown
  #
  # source://langchainrb//lib/langchain/chunker/markdown.rb#18
  def initialize(text, chunk_size: T.unsafe(nil), chunk_overlap: T.unsafe(nil)); end

  # Returns the value of attribute chunk_overlap.
  #
  # source://langchainrb//lib/langchain/chunker/markdown.rb#12
  def chunk_overlap; end

  # Returns the value of attribute chunk_size.
  #
  # source://langchainrb//lib/langchain/chunker/markdown.rb#12
  def chunk_size; end

  # @return [Array<Langchain::Chunk>]
  #
  # source://langchainrb//lib/langchain/chunker/markdown.rb#25
  def chunks; end

  # Returns the value of attribute text.
  #
  # source://langchainrb//lib/langchain/chunker/markdown.rb#12
  def text; end
end

# Recursive text chunker. Preferentially splits on separators.
#
# Usage:
#     Langchain::Chunker::RecursiveText.new(text).chunks
#
# source://langchainrb//lib/langchain/chunker/recursive_text.rb#11
class Langchain::Chunker::RecursiveText < ::Langchain::Chunker::Base
  # @param text [String]
  # @param chunk_size [Integer]
  # @param chunk_overlap [Integer]
  # @param separators [Array<String>]
  # @return [RecursiveText] a new instance of RecursiveText
  #
  # source://langchainrb//lib/langchain/chunker/recursive_text.rb#18
  def initialize(text, chunk_size: T.unsafe(nil), chunk_overlap: T.unsafe(nil), separators: T.unsafe(nil)); end

  # Returns the value of attribute chunk_overlap.
  #
  # source://langchainrb//lib/langchain/chunker/recursive_text.rb#12
  def chunk_overlap; end

  # Returns the value of attribute chunk_size.
  #
  # source://langchainrb//lib/langchain/chunker/recursive_text.rb#12
  def chunk_size; end

  # @return [Array<Langchain::Chunk>]
  #
  # source://langchainrb//lib/langchain/chunker/recursive_text.rb#26
  def chunks; end

  # Returns the value of attribute separators.
  #
  # source://langchainrb//lib/langchain/chunker/recursive_text.rb#12
  def separators; end

  # Returns the value of attribute text.
  #
  # source://langchainrb//lib/langchain/chunker/recursive_text.rb#12
  def text; end
end

# LLM-powered semantic chunker.
# Semantic chunking is a technique of splitting texts by their semantic meaning, e.g.: themes, topics, and ideas.
# We use an LLM to accomplish this. The Anthropic LLM is highly recommended for this task as it has the longest context window (100k tokens).
#
# Usage:
#     Langchain::Chunker::Semantic.new(
#       text,
#       llm: Langchain::LLM::Anthropic.new(api_key: ENV["ANTHROPIC_API_KEY"])
#     ).chunks
#
# source://langchainrb//lib/langchain/chunker/semantic.rb#14
class Langchain::Chunker::Semantic < ::Langchain::Chunker::Base
  # @param Langchain::LLM::* [Langchain::LLM::Base] instance
  # @param Optional [Langchain::Prompt::PromptTemplate] custom prompt template
  # @return [Semantic] a new instance of Semantic
  #
  # source://langchainrb//lib/langchain/chunker/semantic.rb#18
  def initialize(text, llm:, prompt_template: T.unsafe(nil)); end

  # @return [Array<Langchain::Chunk>]
  #
  # source://langchainrb//lib/langchain/chunker/semantic.rb#25
  def chunks; end

  # Returns the value of attribute llm.
  #
  # source://langchainrb//lib/langchain/chunker/semantic.rb#15
  def llm; end

  # Returns the value of attribute prompt_template.
  #
  # source://langchainrb//lib/langchain/chunker/semantic.rb#15
  def prompt_template; end

  # Returns the value of attribute text.
  #
  # source://langchainrb//lib/langchain/chunker/semantic.rb#15
  def text; end

  private

  # @return [Langchain::Prompt::PromptTemplate] Default prompt template for semantic chunking
  #
  # source://langchainrb//lib/langchain/chunker/semantic.rb#43
  def default_prompt_template; end
end

# This chunker splits text by sentences.
#
# Usage:
#     Langchain::Chunker::Sentence.new(text).chunks
#
# source://langchainrb//lib/langchain/chunker/sentence.rb#11
class Langchain::Chunker::Sentence < ::Langchain::Chunker::Base
  # @param text [String]
  # @return [Langchain::Chunker::Sentence]
  #
  # source://langchainrb//lib/langchain/chunker/sentence.rb#16
  def initialize(text); end

  # @return [Array<Langchain::Chunk>]
  #
  # source://langchainrb//lib/langchain/chunker/sentence.rb#21
  def chunks; end

  # Returns the value of attribute text.
  #
  # source://langchainrb//lib/langchain/chunker/sentence.rb#12
  def text; end
end

# Simple text chunker
#
# Usage:
#     Langchain::Chunker::Text.new(text).chunks
#
# source://langchainrb//lib/langchain/chunker/text.rb#11
class Langchain::Chunker::Text < ::Langchain::Chunker::Base
  # @param text [String]
  # @param chunk_size [Integer]
  # @param chunk_overlap [Integer]
  # @param separator [String]
  # @return [Text] a new instance of Text
  #
  # source://langchainrb//lib/langchain/chunker/text.rb#18
  def initialize(text, chunk_size: T.unsafe(nil), chunk_overlap: T.unsafe(nil), separator: T.unsafe(nil)); end

  # Returns the value of attribute chunk_overlap.
  #
  # source://langchainrb//lib/langchain/chunker/text.rb#12
  def chunk_overlap; end

  # Returns the value of attribute chunk_size.
  #
  # source://langchainrb//lib/langchain/chunker/text.rb#12
  def chunk_size; end

  # @return [Array<Langchain::Chunk>]
  #
  # source://langchainrb//lib/langchain/chunker/text.rb#26
  def chunks; end

  # Returns the value of attribute separator.
  #
  # source://langchainrb//lib/langchain/chunker/text.rb#12
  def separator; end

  # Returns the value of attribute text.
  #
  # source://langchainrb//lib/langchain/chunker/text.rb#12
  def text; end
end

# source://langchainrb//lib/langchain/contextual_logger.rb#4
class Langchain::ContextualLogger
  # @return [ContextualLogger] a new instance of ContextualLogger
  #
  # source://langchainrb//lib/langchain/contextual_logger.rb#29
  def initialize(logger); end

  # source://langchainrb//lib/langchain/contextual_logger.rb#38
  def method_missing(method, *args, **kwargs, &block); end

  private

  # @return [Boolean]
  #
  # source://langchainrb//lib/langchain/contextual_logger.rb#34
  def respond_to_missing?(method, include_private = T.unsafe(nil)); end
end

# source://langchainrb//lib/langchain/contextual_logger.rb#5
Langchain::ContextualLogger::MESSAGE_COLOR_OPTIONS = T.let(T.unsafe(nil), Hash)

# Abstraction for data loaded by a {Langchain::Loader}
#
# source://langchainrb//lib/langchain/data.rb#5
class Langchain::Data
  # @option options
  # @param data [String] data that was loaded
  # @param options [Hash] a customizable set of options
  # @return [Data] a new instance of Data
  #
  # source://langchainrb//lib/langchain/data.rb#12
  def initialize(data, source: T.unsafe(nil), chunker: T.unsafe(nil)); end

  # @param opts [Hash] options passed to the chunker
  # @return [Array<String>]
  #
  # source://langchainrb//lib/langchain/data.rb#25
  def chunks(opts = T.unsafe(nil)); end

  # URL or Path of the data source
  #
  # @return [String]
  #
  # source://langchainrb//lib/langchain/data.rb#8
  def source; end

  # @return [String]
  #
  # source://langchainrb//lib/langchain/data.rb#19
  def value; end
end

# source://langchainrb//lib/langchain/dependency_helper.rb#4
module Langchain::DependencyHelper
  # This method requires and loads the given gem, and then checks to see if the version of the gem meets the requirements listed in `langchain.gemspec`
  # This solution was built to avoid auto-loading every single gem in the Gemfile when the developer will mostly likely be only using a few of them.
  #
  # @param gem_name [String] The name of the gem to load
  # @raise [LoadError] If the gem is not installed
  # @raise [VersionError] If the gem is installed, but the version does not meet the requirements
  # @return [Boolean] Whether or not the gem was loaded successfully
  #
  # source://langchainrb//lib/langchain/dependency_helper.rb#17
  def depends_on(gem_name, req: T.unsafe(nil)); end
end

# source://langchainrb//lib/langchain/dependency_helper.rb#5
class Langchain::DependencyHelper::LoadError < ::LoadError; end

# source://langchainrb//lib/langchain/dependency_helper.rb#7
class Langchain::DependencyHelper::VersionError < ::ScriptError; end

# source://langchainrb//lib/langchain.rb#108
module Langchain::Errors; end

# source://langchainrb//lib/langchain.rb#109
class Langchain::Errors::BaseError < ::StandardError; end

# source://langchainrb//lib/langchain/evals/ragas/answer_relevance.rb#6
module Langchain::Evals; end

# The RAGAS (Retrieval Augmented Generative Assessment) is a framework for evaluating RAG (Retrieval Augmented Generation) pipelines.
# Based on the following research: https://arxiv.org/pdf/2309.15217.pdf
#
# source://langchainrb//lib/langchain/evals/ragas/answer_relevance.rb#7
module Langchain::Evals::Ragas; end

# Answer Relevance refers to the idea that the generated answer should address the actual question that was provided.
# This metric evaluates how closely the generated answer aligns with the initial question or instruction.
#
# source://langchainrb//lib/langchain/evals/ragas/answer_relevance.rb#10
class Langchain::Evals::Ragas::AnswerRelevance
  # @param llm [Langchain::LLM::*] Langchain::LLM::* object
  # @param batch_size [Integer] Batch size, i.e., number of generated questions to compare to the original question
  # @return [AnswerRelevance] a new instance of AnswerRelevance
  #
  # source://langchainrb//lib/langchain/evals/ragas/answer_relevance.rb#15
  def initialize(llm:, batch_size: T.unsafe(nil)); end

  # Returns the value of attribute batch_size.
  #
  # source://langchainrb//lib/langchain/evals/ragas/answer_relevance.rb#11
  def batch_size; end

  # Returns the value of attribute llm.
  #
  # source://langchainrb//lib/langchain/evals/ragas/answer_relevance.rb#11
  def llm; end

  # @param question [String] Question
  # @param answer [String] Answer
  # @return [Float] Answer Relevance score
  #
  # source://langchainrb//lib/langchain/evals/ragas/answer_relevance.rb#23
  def score(question:, answer:); end

  private

  # @return [PromptTemplate] PromptTemplate instance
  #
  # source://langchainrb//lib/langchain/evals/ragas/answer_relevance.rb#63
  def answer_relevance_prompt_template; end

  # @param question_1 [String] Question 1
  # @param question_2 [String] Question 2
  # @return [Float] Dot product similarity between the two questions
  #
  # source://langchainrb//lib/langchain/evals/ragas/answer_relevance.rb#47
  def calculate_similarity(original_question:, generated_question:); end

  # @param text [String] Text to generate an embedding for
  # @return [Array<Float>] Embedding
  #
  # source://langchainrb//lib/langchain/evals/ragas/answer_relevance.rb#58
  def generate_embedding(text); end
end

# Context Relevance refers to the idea that the retrieved context should be focused, containing as little irrelevant information as possible.
#
# source://langchainrb//lib/langchain/evals/ragas/context_relevance.rb#9
class Langchain::Evals::Ragas::ContextRelevance
  # @param llm [Langchain::LLM::*] Langchain::LLM::* object
  # @return [ContextRelevance] a new instance of ContextRelevance
  #
  # source://langchainrb//lib/langchain/evals/ragas/context_relevance.rb#13
  def initialize(llm:); end

  # Returns the value of attribute llm.
  #
  # source://langchainrb//lib/langchain/evals/ragas/context_relevance.rb#10
  def llm; end

  # @param question [String] Question
  # @param context [String] Context
  # @return [Float] Context Relevance score
  #
  # source://langchainrb//lib/langchain/evals/ragas/context_relevance.rb#20
  def score(question:, context:); end

  private

  # @return [PromptTemplate] PromptTemplate instance
  #
  # source://langchainrb//lib/langchain/evals/ragas/context_relevance.rb#38
  def context_relevance_prompt_template; end

  # source://langchainrb//lib/langchain/evals/ragas/context_relevance.rb#32
  def sentence_count(context); end
end

# Faithfulness refers to the idea that the answer should be grounded in the given context,
# ensuring that the retrieved context can act as a justification for the generated answer.
# The answer is faithful to the context if the claims that are made in the answer can be inferred from the context.
#
# Score calculation:
# F = |V| / |S|
#
# F = Faithfulness
# |V| = Number of statements that were supported according to the LLM
# |S| = Total number of statements extracted.
#
# source://langchainrb//lib/langchain/evals/ragas/faithfulness.rb#17
class Langchain::Evals::Ragas::Faithfulness
  # @param llm [Langchain::LLM::*] Langchain::LLM::* object
  # @return [Faithfulness] a new instance of Faithfulness
  #
  # source://langchainrb//lib/langchain/evals/ragas/faithfulness.rb#21
  def initialize(llm:); end

  # Returns the value of attribute llm.
  #
  # source://langchainrb//lib/langchain/evals/ragas/faithfulness.rb#18
  def llm; end

  # @param question [String] Question
  # @param answer [String] Answer
  # @param context [String] Context
  # @return [Float] Faithfulness score
  #
  # source://langchainrb//lib/langchain/evals/ragas/faithfulness.rb#29
  def score(question:, answer:, context:); end

  private

  # source://langchainrb//lib/langchain/evals/ragas/faithfulness.rb#43
  def count_verified_statements(verifications); end

  # source://langchainrb//lib/langchain/evals/ragas/faithfulness.rb#61
  def statements_extraction(question:, answer:); end

  # @return [PromptTemplate] PromptTemplate instance
  #
  # source://langchainrb//lib/langchain/evals/ragas/faithfulness.rb#77
  def statements_extraction_prompt_template; end

  # source://langchainrb//lib/langchain/evals/ragas/faithfulness.rb#53
  def statements_verification(statements:, context:); end

  # @return [PromptTemplate] PromptTemplate instance
  #
  # source://langchainrb//lib/langchain/evals/ragas/faithfulness.rb#70
  def statements_verification_prompt_template; end
end

# source://langchainrb//lib/langchain/evals/ragas/main.rb#8
class Langchain::Evals::Ragas::Main
  # @return [Main] a new instance of Main
  #
  # source://langchainrb//lib/langchain/evals/ragas/main.rb#11
  def initialize(llm:); end

  # Returns the value of attribute llm.
  #
  # source://langchainrb//lib/langchain/evals/ragas/main.rb#9
  def llm; end

  # Returns the RAGAS scores, e.g.:
  # {
  #   ragas_score: 0.6601257446503674,
  #   answer_relevance_score: 0.9573145866787608,
  #   context_relevance_score: 0.6666666666666666,
  #   faithfulness_score: 0.5
  # }
  #
  # @param question [String] Question
  # @param answer [String] Answer
  # @param context [String] Context
  # @return [Hash] RAGAS scores
  #
  # source://langchainrb//lib/langchain/evals/ragas/main.rb#27
  def score(question:, answer:, context:); end

  private

  # @return [Langchain::Evals::Ragas::AnswerRelevance] Class instance
  #
  # source://langchainrb//lib/langchain/evals/ragas/main.rb#54
  def answer_relevance; end

  # @return [Langchain::Evals::Ragas::ContextRelevance] Class instance
  #
  # source://langchainrb//lib/langchain/evals/ragas/main.rb#59
  def context_relevance; end

  # @return [Langchain::Evals::Ragas::Faithfulness] Class instance
  #
  # source://langchainrb//lib/langchain/evals/ragas/main.rb#64
  def faithfulness; end

  # Overall RAGAS score (harmonic mean): https://github.com/explodinggradients/ragas/blob/1dd363e3e54744e67b0be85962a0258d8121500a/src/ragas/evaluation.py#L140-L143
  #
  # @param answer_relevance_score [Float] Answer Relevance score
  # @param context_relevance_score [Float] Context Relevance score
  # @param faithfulness_score [Float] Faithfulness score
  # @return [Float] RAGAS score
  #
  # source://langchainrb//lib/langchain/evals/ragas/main.rb#48
  def ragas_score(answer_relevance_score, context_relevance_score, faithfulness_score); end
end

# source://langchainrb//lib/langchain/assistants/assistant.rb#0
module Langchain::LLM; end

# Wrapper around AI21 Studio APIs.
#
# Gem requirements:
#   gem "ai21", "~> 0.2.1"
#
# Usage:
#     ai21 = Langchain::LLM::AI21.new(api_key: ENV["AI21_API_KEY"])
#
# source://langchainrb//lib/langchain/llm/ai21.rb#13
class Langchain::LLM::AI21 < ::Langchain::LLM::Base
  # @return [AI21] a new instance of AI21
  #
  # source://langchainrb//lib/langchain/llm/ai21.rb#21
  def initialize(api_key:, default_options: T.unsafe(nil)); end

  # Generate a completion for a given prompt
  #
  # @param prompt [String] The prompt to generate a completion for
  # @param params [Hash] The parameters to pass to the API
  # @return [Langchain::LLM::AI21Response] The completion
  #
  # source://langchainrb//lib/langchain/llm/ai21.rb#35
  def complete(prompt:, **params); end

  # Generate a summary for a given text
  #
  # @param text [String] The text to generate a summary for
  # @param params [Hash] The parameters to pass to the API
  # @return [String] The summary
  #
  # source://langchainrb//lib/langchain/llm/ai21.rb#51
  def summarize(text:, **params); end

  private

  # source://langchainrb//lib/langchain/llm/ai21.rb#59
  def complete_parameters(params); end
end

# source://langchainrb//lib/langchain/llm/ai21.rb#14
Langchain::LLM::AI21::DEFAULTS = T.let(T.unsafe(nil), Hash)

# source://langchainrb//lib/langchain/llm/ai21.rb#19
Langchain::LLM::AI21::LENGTH_VALIDATOR = Langchain::Utils::TokenLength::AI21Validator

# source://langchainrb//lib/langchain/llm/response/ai21_response.rb#4
class Langchain::LLM::AI21Response < ::Langchain::LLM::BaseResponse
  # source://langchainrb//lib/langchain/llm/response/ai21_response.rb#9
  def completion; end

  # source://langchainrb//lib/langchain/llm/response/ai21_response.rb#5
  def completions; end
end

# Wrapper around Anthropic APIs.
#
# Gem requirements:
#   gem "anthropic", "~> 0.1.0"
#
# Usage:
#     anthorpic = Langchain::LLM::Anthropic.new(api_key: ENV["ANTHROPIC_API_KEY"])
#
# source://langchainrb//lib/langchain/llm/anthropic.rb#13
class Langchain::LLM::Anthropic < ::Langchain::LLM::Base
  # Initialize an Anthropic LLM instance
  #
  # @param api_key [String] The API key to use
  # @param llm_options [Hash] Options to pass to the Anthropic client
  # @param default_options [Hash] Default options to use on every call to LLM, e.g.: { temperature:, completion_model_name:, chat_completion_model_name:, max_tokens_to_sample: }
  # @return [Langchain::LLM::Anthropic] Langchain::LLM::Anthropic instance
  #
  # source://langchainrb//lib/langchain/llm/anthropic.rb#30
  def initialize(api_key:, llm_options: T.unsafe(nil), default_options: T.unsafe(nil)); end

  # Generate a chat completion for given messages
  #
  # @option params
  # @option params
  # @option params
  # @option params
  # @option params
  # @option params
  # @option params
  # @option params
  # @option params
  # @option params
  # @option params
  # @param params [Hash] unified chat parmeters from [Langchain::LLM::Parameters::Chat::SCHEMA]
  # @raise [ArgumentError]
  # @return [Langchain::LLM::AnthropicResponse] The chat completion
  #
  # source://langchainrb//lib/langchain/llm/anthropic.rb#103
  def chat(params = T.unsafe(nil)); end

  # Generate a completion for a given prompt
  #
  # @param prompt [String] Prompt to generate a completion for
  # @param model [String] The model to use
  # @param max_tokens_to_sample [Integer] The maximum number of tokens to sample
  # @param stop_sequences [Array<String>] The stop sequences to use
  # @param temperature [Float] The temperature to use
  # @param top_p [Float] The top p value to use
  # @param top_k [Integer] The top k value to use
  # @param metadata [Hash] The metadata to use
  # @param stream [Boolean] Whether to stream the response
  # @raise [ArgumentError]
  # @return [Langchain::LLM::AnthropicResponse] The completion
  #
  # source://langchainrb//lib/langchain/llm/anthropic.rb#58
  def complete(prompt:, model: T.unsafe(nil), max_tokens_to_sample: T.unsafe(nil), stop_sequences: T.unsafe(nil), temperature: T.unsafe(nil), top_p: T.unsafe(nil), top_k: T.unsafe(nil), metadata: T.unsafe(nil), stream: T.unsafe(nil)); end

  private

  # source://langchainrb//lib/langchain/llm/anthropic.rb#119
  def set_extra_headers!; end
end

# source://langchainrb//lib/langchain/llm/anthropic.rb#14
Langchain::LLM::Anthropic::DEFAULTS = T.let(T.unsafe(nil), Hash)

# source://langchainrb//lib/langchain/llm/response/anthropic_response.rb#4
class Langchain::LLM::AnthropicResponse < ::Langchain::LLM::BaseResponse
  # source://langchainrb//lib/langchain/llm/response/anthropic_response.rb#13
  def chat_completion; end

  # source://langchainrb//lib/langchain/llm/response/anthropic_response.rb#23
  def chat_completions; end

  # source://langchainrb//lib/langchain/llm/response/anthropic_response.rb#9
  def completion; end

  # source://langchainrb//lib/langchain/llm/response/anthropic_response.rb#47
  def completion_tokens; end

  # source://langchainrb//lib/langchain/llm/response/anthropic_response.rb#27
  def completions; end

  # source://langchainrb//lib/langchain/llm/response/anthropic_response.rb#39
  def log_id; end

  # source://langchainrb//lib/langchain/llm/response/anthropic_response.rb#5
  def model; end

  # source://langchainrb//lib/langchain/llm/response/anthropic_response.rb#43
  def prompt_tokens; end

  # source://langchainrb//lib/langchain/llm/response/anthropic_response.rb#55
  def role; end

  # source://langchainrb//lib/langchain/llm/response/anthropic_response.rb#35
  def stop; end

  # source://langchainrb//lib/langchain/llm/response/anthropic_response.rb#31
  def stop_reason; end

  # source://langchainrb//lib/langchain/llm/response/anthropic_response.rb#18
  def tool_calls; end

  # source://langchainrb//lib/langchain/llm/response/anthropic_response.rb#51
  def total_tokens; end
end

# source://langchainrb//lib/langchain/llm/base.rb#4
class Langchain::LLM::ApiError < ::StandardError; end

# LLM interface for Aws Bedrock APIs: https://docs.aws.amazon.com/bedrock/
#
# Gem requirements:
#    gem 'aws-sdk-bedrockruntime', '~> 1.1'
#
# Usage:
#    bedrock = Langchain::LLM::AwsBedrock.new(llm_options: {})
#
# source://langchainrb//lib/langchain/llm/aws_bedrock.rb#12
class Langchain::LLM::AwsBedrock < ::Langchain::LLM::Base
  # @return [AwsBedrock] a new instance of AwsBedrock
  #
  # source://langchainrb//lib/langchain/llm/aws_bedrock.rb#55
  def initialize(completion_model: T.unsafe(nil), embedding_model: T.unsafe(nil), aws_client_options: T.unsafe(nil), default_options: T.unsafe(nil)); end

  # Generate a chat completion for a given prompt
  # Currently only configured to work with the Anthropic provider and
  # the claude-3 model family
  #
  # @option params
  # @option params
  # @option params
  # @option params
  # @option params
  # @option params
  # @option params
  # @option params
  # @option params
  # @param params [Hash] unified chat parmeters from [Langchain::LLM::Parameters::Chat::SCHEMA]
  # @raise [ArgumentError]
  # @return [Langchain::LLM::AnthropicResponse] Response object
  # @yield [Hash] Provides chunks of the response as they are received
  #
  # source://langchainrb//lib/langchain/llm/aws_bedrock.rb#140
  def chat(params = T.unsafe(nil), &block); end

  # Returns the value of attribute client.
  #
  # source://langchainrb//lib/langchain/llm/aws_bedrock.rb#49
  def client; end

  # Generate a completion for a given prompt
  #
  # @param prompt [String] The prompt to generate a completion for
  # @param params extra parameters passed to Aws::BedrockRuntime::Client#invoke_model
  # @return [Langchain::LLM::AnthropicResponse] , [Langchain::LLM::CohereResponse] or [Langchain::LLM::AI21Response] Response object
  #
  # source://langchainrb//lib/langchain/llm/aws_bedrock.rb#105
  def complete(prompt:, **params); end

  # Returns the value of attribute defaults.
  #
  # source://langchainrb//lib/langchain/llm/aws_bedrock.rb#49
  def defaults; end

  # Generate an embedding for a given text
  #
  # @param text [String] The text to generate an embedding for
  # @param params extra parameters passed to Aws::BedrockRuntime::Client#invoke_model
  # @return [Langchain::LLM::AwsTitanResponse] Response object
  #
  # source://langchainrb//lib/langchain/llm/aws_bedrock.rb#82
  def embed(text:, **params); end

  private

  # source://langchainrb//lib/langchain/llm/aws_bedrock.rb#179
  def completion_provider; end

  # source://langchainrb//lib/langchain/llm/aws_bedrock.rb#205
  def compose_parameters(params); end

  # source://langchainrb//lib/langchain/llm/aws_bedrock.rb#250
  def compose_parameters_ai21(params); end

  # source://langchainrb//lib/langchain/llm/aws_bedrock.rb#237
  def compose_parameters_anthropic(params); end

  # source://langchainrb//lib/langchain/llm/aws_bedrock.rb#225
  def compose_parameters_cohere(params); end

  # source://langchainrb//lib/langchain/llm/aws_bedrock.rb#183
  def embedding_provider; end

  # source://langchainrb//lib/langchain/llm/aws_bedrock.rb#195
  def max_tokens_key; end

  # source://langchainrb//lib/langchain/llm/aws_bedrock.rb#215
  def parse_response(response); end

  # source://langchainrb//lib/langchain/llm/aws_bedrock.rb#285
  def response_from_chunks(chunks); end

  # source://langchainrb//lib/langchain/llm/aws_bedrock.rb#187
  def wrap_prompt(prompt); end
end

# source://langchainrb//lib/langchain/llm/aws_bedrock.rb#13
Langchain::LLM::AwsBedrock::DEFAULTS = T.let(T.unsafe(nil), Hash)

# source://langchainrb//lib/langchain/llm/aws_bedrock.rb#52
Langchain::LLM::AwsBedrock::SUPPORTED_CHAT_COMPLETION_PROVIDERS = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/llm/aws_bedrock.rb#51
Langchain::LLM::AwsBedrock::SUPPORTED_COMPLETION_PROVIDERS = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/llm/aws_bedrock.rb#53
Langchain::LLM::AwsBedrock::SUPPORTED_EMBEDDING_PROVIDERS = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/llm/response/aws_titan_response.rb#4
class Langchain::LLM::AwsTitanResponse < ::Langchain::LLM::BaseResponse
  # source://langchainrb//lib/langchain/llm/response/aws_titan_response.rb#5
  def embedding; end

  # source://langchainrb//lib/langchain/llm/response/aws_titan_response.rb#9
  def embeddings; end

  # source://langchainrb//lib/langchain/llm/response/aws_titan_response.rb#13
  def prompt_tokens; end
end

# LLM interface for Azure OpenAI Service APIs: https://learn.microsoft.com/en-us/azure/ai-services/openai/
#
# Gem requirements:
#    gem "ruby-openai", "~> 6.3.0"
#
# Usage:
#    openai = Langchain::LLM::Azure.new(api_key:, llm_options: {}, embedding_deployment_url: chat_deployment_url:)
#
# source://langchainrb//lib/langchain/llm/azure.rb#12
class Langchain::LLM::Azure < ::Langchain::LLM::OpenAI
  # @return [Azure] a new instance of Azure
  #
  # source://langchainrb//lib/langchain/llm/azure.rb#16
  def initialize(api_key:, llm_options: T.unsafe(nil), default_options: T.unsafe(nil), embedding_deployment_url: T.unsafe(nil), chat_deployment_url: T.unsafe(nil)); end

  # source://langchainrb//lib/langchain/llm/azure.rb#53
  def chat(*_arg0, **_arg1, &_arg2); end

  # Returns the value of attribute chat_client.
  #
  # source://langchainrb//lib/langchain/llm/azure.rb#14
  def chat_client; end

  # source://langchainrb//lib/langchain/llm/azure.rb#48
  def complete(*_arg0, **_arg1, &_arg2); end

  # source://langchainrb//lib/langchain/llm/azure.rb#43
  def embed(*_arg0, **_arg1, &_arg2); end

  # Returns the value of attribute embed_client.
  #
  # source://langchainrb//lib/langchain/llm/azure.rb#13
  def embed_client; end
end

# A LLM is a language model consisting of a neural network with many parameters (typically billions of weights or more), trained on large quantities of unlabeled text using self-supervised learning or semi-supervised learning.
#
# Langchain.rb provides a common interface to interact with all supported LLMs:
#
# - {Langchain::LLM::AI21}
# - {Langchain::LLM::Azure}
# - {Langchain::LLM::Cohere}
# - {Langchain::LLM::GooglePalm}
# - {Langchain::LLM::GoogleVertexAI}
# - {Langchain::LLM::GoogleGemini}
# - {Langchain::LLM::HuggingFace}
# - {Langchain::LLM::LlamaCpp}
# - {Langchain::LLM::OpenAI}
# - {Langchain::LLM::Replicate}
#
# @abstract
#
# source://langchainrb//lib/langchain/llm/base.rb#22
class Langchain::LLM::Base
  include ::Langchain::DependencyHelper

  # Generate a chat completion for a given prompt. Parameters will depend on the LLM
  #
  # @raise NotImplementedError if not supported by the LLM
  #
  # source://langchainrb//lib/langchain/llm/base.rb#45
  def chat(*_arg0, **_arg1, &_arg2); end

  # Returns an instance of Langchain::LLM::Parameters::Chat
  #
  # source://langchainrb//lib/langchain/llm/base.rb#78
  def chat_parameters(params = T.unsafe(nil)); end

  # A client for communicating with the LLM
  #
  # source://langchainrb//lib/langchain/llm/base.rb#26
  def client; end

  # Generate a completion for a given prompt. Parameters will depend on the LLM.
  #
  # @raise NotImplementedError if not supported by the LLM
  #
  # source://langchainrb//lib/langchain/llm/base.rb#53
  def complete(*_arg0, **_arg1, &_arg2); end

  # Ensuring backward compatibility after https://github.com/patterns-ai-core/langchainrb/pull/586
  # TODO: Delete this method later
  #
  # source://langchainrb//lib/langchain/llm/base.rb#30
  def default_dimension; end

  # Returns the number of vector dimensions used by DEFAULTS[:chat_completion_model_name]
  #
  # @return [Integer] Vector dimensions
  #
  # source://langchainrb//lib/langchain/llm/base.rb#37
  def default_dimensions; end

  # Generate an embedding for a given text. Parameters depends on the LLM.
  #
  # @raise NotImplementedError if not supported by the LLM
  #
  # source://langchainrb//lib/langchain/llm/base.rb#62
  def embed(*_arg0, **_arg1, &_arg2); end

  # Generate a summary for a given text. Parameters depends on the LLM.
  #
  # @raise NotImplementedError if not supported by the LLM
  #
  # source://langchainrb//lib/langchain/llm/base.rb#71
  def summarize(*_arg0, **_arg1, &_arg2); end
end

# source://langchainrb//lib/langchain/llm/response/base_response.rb#5
class Langchain::LLM::BaseResponse
  # @return [BaseResponse] a new instance of BaseResponse
  #
  # source://langchainrb//lib/langchain/llm/response/base_response.rb#11
  def initialize(raw_response, model: T.unsafe(nil)); end

  # Returns the chat completion text
  #
  # @raise [NotImplementedError]
  # @return [String]
  #
  # source://langchainrb//lib/langchain/llm/response/base_response.rb#35
  def chat_completion; end

  # Return the chat completion candidates
  #
  # @raise [NotImplementedError]
  # @return [Array<String>]
  #
  # source://langchainrb//lib/langchain/llm/response/base_response.rb#56
  def chat_completions; end

  # Returns the completion text
  #
  # @raise [NotImplementedError]
  # @return [String]
  #
  # source://langchainrb//lib/langchain/llm/response/base_response.rb#27
  def completion; end

  # Number of tokens utilized to generate the completion
  #
  # @raise [NotImplementedError]
  # @return [Integer]
  #
  # source://langchainrb//lib/langchain/llm/response/base_response.rb#77
  def completion_tokens; end

  # Return the completion candidates
  #
  # @raise [NotImplementedError]
  # @return [Array<String>]
  #
  # source://langchainrb//lib/langchain/llm/response/base_response.rb#49
  def completions; end

  # Save context in the response when doing RAG workflow vectorsearch#ask()
  #
  # source://langchainrb//lib/langchain/llm/response/base_response.rb#9
  def context; end

  # Save context in the response when doing RAG workflow vectorsearch#ask()
  #
  # source://langchainrb//lib/langchain/llm/response/base_response.rb#9
  def context=(_arg0); end

  # Returns the timestamp when the response was created
  #
  # @raise [NotImplementedError]
  # @return [Time]
  #
  # source://langchainrb//lib/langchain/llm/response/base_response.rb#19
  def created_at; end

  # Return the first embedding
  #
  # @raise [NotImplementedError]
  # @return [Array<Float>]
  #
  # source://langchainrb//lib/langchain/llm/response/base_response.rb#42
  def embedding; end

  # Return the embeddings
  #
  # @raise [NotImplementedError]
  # @return [Array<Array>]
  #
  # source://langchainrb//lib/langchain/llm/response/base_response.rb#63
  def embeddings; end

  # Returns the value of attribute model.
  #
  # source://langchainrb//lib/langchain/llm/response/base_response.rb#6
  def model; end

  # Number of tokens utilized in the prompt
  #
  # @raise [NotImplementedError]
  # @return [Integer]
  #
  # source://langchainrb//lib/langchain/llm/response/base_response.rb#70
  def prompt_tokens; end

  # Returns the value of attribute raw_response.
  #
  # source://langchainrb//lib/langchain/llm/response/base_response.rb#6
  def raw_response; end

  # Total number of tokens utilized
  #
  # @raise [NotImplementedError]
  # @return [Integer]
  #
  # source://langchainrb//lib/langchain/llm/response/base_response.rb#84
  def total_tokens; end
end

# Wrapper around the Cohere API.
#
# Gem requirements:
#     gem "cohere-ruby", "~> 0.9.6"
#
# Usage:
#     llm = Langchain::LLM::Cohere.new(api_key: ENV["COHERE_API_KEY"])
#
# source://langchainrb//lib/langchain/llm/cohere.rb#13
class Langchain::LLM::Cohere < ::Langchain::LLM::Base
  # @return [Cohere] a new instance of Cohere
  #
  # source://langchainrb//lib/langchain/llm/cohere.rb#23
  def initialize(api_key:, default_options: T.unsafe(nil)); end

  # Generate a chat completion for given messages
  #
  # @option params
  # @option params
  # @option params
  # @option params
  # @option params
  # @option params
  # @option params
  # @option params
  # @option params
  # @option params
  # @param params [Hash] unified chat parmeters from [Langchain::LLM::Parameters::Chat::SCHEMA]
  # @raise [ArgumentError]
  # @return [Langchain::LLM::CohereResponse] The chat completion
  #
  # source://langchainrb//lib/langchain/llm/cohere.rb#97
  def chat(params = T.unsafe(nil)); end

  # Generate a completion for a given prompt
  #
  # @param prompt [String] The prompt to generate a completion for
  # @param params [:stop_sequences]
  # @return [Langchain::LLM::CohereResponse] Response object
  #
  # source://langchainrb//lib/langchain/llm/cohere.rb#63
  def complete(prompt:, **params); end

  # Generate an embedding for a given text
  #
  # @param text [String] The text to generate an embedding for
  # @return [Langchain::LLM::CohereResponse] Response object
  #
  # source://langchainrb//lib/langchain/llm/cohere.rb#47
  def embed(text:); end

  # Generate a summary in English for a given text
  #
  # More parameters available to extend this method with: https://github.com/andreibondarev/cohere-ruby/blob/0.9.4/lib/cohere/client.rb#L107-L115
  #
  # @param text [String] The text to generate a summary for
  # @return [String] The summary
  #
  # source://langchainrb//lib/langchain/llm/cohere.rb#113
  def summarize(text:); end
end

# source://langchainrb//lib/langchain/llm/cohere.rb#14
Langchain::LLM::Cohere::DEFAULTS = T.let(T.unsafe(nil), Hash)

# source://langchainrb//lib/langchain/llm/response/cohere_response.rb#4
class Langchain::LLM::CohereResponse < ::Langchain::LLM::BaseResponse
  # source://langchainrb//lib/langchain/llm/response/cohere_response.rb#21
  def chat_completion; end

  # source://langchainrb//lib/langchain/llm/response/cohere_response.rb#17
  def completion; end

  # source://langchainrb//lib/langchain/llm/response/cohere_response.rb#33
  def completion_tokens; end

  # source://langchainrb//lib/langchain/llm/response/cohere_response.rb#13
  def completions; end

  # source://langchainrb//lib/langchain/llm/response/cohere_response.rb#5
  def embedding; end

  # source://langchainrb//lib/langchain/llm/response/cohere_response.rb#9
  def embeddings; end

  # source://langchainrb//lib/langchain/llm/response/cohere_response.rb#29
  def prompt_tokens; end

  # source://langchainrb//lib/langchain/llm/response/cohere_response.rb#25
  def role; end
end

# Usage:
#     llm = Langchain::LLM::GoogleGemini.new(api_key: ENV['GOOGLE_GEMINI_API_KEY'])
#
# source://langchainrb//lib/langchain/llm/google_gemini.rb#6
class Langchain::LLM::GoogleGemini < ::Langchain::LLM::Base
  # @return [GoogleGemini] a new instance of GoogleGemini
  #
  # source://langchainrb//lib/langchain/llm/google_gemini.rb#15
  def initialize(api_key:, default_options: T.unsafe(nil)); end

  # Returns the value of attribute api_key.
  #
  # source://langchainrb//lib/langchain/llm/google_gemini.rb#13
  def api_key; end

  # Generate a chat completion for a given prompt
  #
  # @param messages [Array<Hash>] List of messages comprising the conversation so far
  # @param model [String] The model to use
  # @param tools [Array<Hash>] A list of Tools the model may use to generate the next response
  # @param tool_choice [String] Specifies the mode in which function calling should execute. If unspecified, the default value will be set to AUTO. Possible values: AUTO, ANY, NONE
  # @param system [String] Developer set system instruction
  # @raise [ArgumentError]
  #
  # source://langchainrb//lib/langchain/llm/google_gemini.rb#37
  def chat(params = T.unsafe(nil)); end

  # Returns the value of attribute defaults.
  #
  # source://langchainrb//lib/langchain/llm/google_gemini.rb#13
  def defaults; end

  # source://langchainrb//lib/langchain/llm/google_gemini.rb#68
  def embed(text:, model: T.unsafe(nil)); end
end

# source://langchainrb//lib/langchain/llm/google_gemini.rb#7
Langchain::LLM::GoogleGemini::DEFAULTS = T.let(T.unsafe(nil), Hash)

# source://langchainrb//lib/langchain/llm/response/google_gemini_response.rb#4
class Langchain::LLM::GoogleGeminiResponse < ::Langchain::LLM::BaseResponse
  # @return [GoogleGeminiResponse] a new instance of GoogleGeminiResponse
  #
  # source://langchainrb//lib/langchain/llm/response/google_gemini_response.rb#5
  def initialize(raw_response, model: T.unsafe(nil)); end

  # source://langchainrb//lib/langchain/llm/response/google_gemini_response.rb#9
  def chat_completion; end

  # source://langchainrb//lib/langchain/llm/response/google_gemini_response.rb#41
  def completion_tokens; end

  # source://langchainrb//lib/langchain/llm/response/google_gemini_response.rb#25
  def embedding; end

  # source://langchainrb//lib/langchain/llm/response/google_gemini_response.rb#29
  def embeddings; end

  # source://langchainrb//lib/langchain/llm/response/google_gemini_response.rb#37
  def prompt_tokens; end

  # source://langchainrb//lib/langchain/llm/response/google_gemini_response.rb#13
  def role; end

  # source://langchainrb//lib/langchain/llm/response/google_gemini_response.rb#17
  def tool_calls; end

  # source://langchainrb//lib/langchain/llm/response/google_gemini_response.rb#45
  def total_tokens; end
end

# Wrapper around the Google PaLM (Pathways Language Model) APIs: https://ai.google/build/machine-learning/
#
# Gem requirements:
#     gem "google_palm_api", "~> 0.1.3"
#
# Usage:
#     google_palm = Langchain::LLM::GooglePalm.new(api_key: ENV["GOOGLE_PALM_API_KEY"])
#
# source://langchainrb//lib/langchain/llm/google_palm.rb#13
class Langchain::LLM::GooglePalm < ::Langchain::LLM::Base
  # @return [GooglePalm] a new instance of GooglePalm
  #
  # source://langchainrb//lib/langchain/llm/google_palm.rb#28
  def initialize(api_key:, default_options: T.unsafe(nil)); end

  # Generate a chat completion for a given prompt
  #
  # @param prompt [String] The prompt to generate a chat completion for
  # @param messages [Array<Hash>] The messages that have been sent in the conversation
  # @param context [String] An initial context to provide as a system message, ie "You are RubyGPT, a helpful chat bot for helping people learn Ruby"
  # @param examples [Array<Hash>] Examples of messages to provide to the model. Useful for Few-Shot Prompting
  # @param options [Hash] extra parameters passed to GooglePalmAPI::Client#generate_chat_message
  # @raise [ArgumentError]
  # @return [Langchain::LLM::GooglePalmResponse] Response object
  #
  # source://langchainrb//lib/langchain/llm/google_palm.rb#88
  def chat(prompt: T.unsafe(nil), messages: T.unsafe(nil), context: T.unsafe(nil), examples: T.unsafe(nil), **options); end

  # Generate a completion for a given prompt
  #
  # @param prompt [String] The prompt to generate a completion for
  # @param params extra parameters passed to GooglePalmAPI::Client#generate_text
  # @return [Langchain::LLM::GooglePalmResponse] Response object
  #
  # source://langchainrb//lib/langchain/llm/google_palm.rb#55
  def complete(prompt:, **params); end

  # Returns the value of attribute defaults.
  #
  # source://langchainrb//lib/langchain/llm/google_palm.rb#26
  def defaults; end

  # Generate an embedding for a given text
  #
  # @param text [String] The text to generate an embedding for
  # @return [Langchain::LLM::GooglePalmResponse] Response object
  #
  # source://langchainrb//lib/langchain/llm/google_palm.rb#41
  def embed(text:); end

  # Generate a summarization for a given text
  #
  # @param text [String] The text to generate a summarization for
  # @return [String] The summarization
  #
  # source://langchainrb//lib/langchain/llm/google_palm.rb#126
  def summarize(text:); end

  private

  # source://langchainrb//lib/langchain/llm/google_palm.rb#142
  def compose_chat_messages(prompt:, messages:); end

  # source://langchainrb//lib/langchain/llm/google_palm.rb#156
  def compose_examples(examples); end

  # source://langchainrb//lib/langchain/llm/google_palm.rb#165
  def transform_messages(messages); end
end

# source://langchainrb//lib/langchain/llm/google_palm.rb#14
Langchain::LLM::GooglePalm::DEFAULTS = T.let(T.unsafe(nil), Hash)

# source://langchainrb//lib/langchain/llm/google_palm.rb#21
Langchain::LLM::GooglePalm::LENGTH_VALIDATOR = Langchain::Utils::TokenLength::GooglePalmValidator

# source://langchainrb//lib/langchain/llm/google_palm.rb#22
Langchain::LLM::GooglePalm::ROLE_MAPPING = T.let(T.unsafe(nil), Hash)

# source://langchainrb//lib/langchain/llm/response/google_palm_response.rb#4
class Langchain::LLM::GooglePalmResponse < ::Langchain::LLM::BaseResponse
  # @return [GooglePalmResponse] a new instance of GooglePalmResponse
  #
  # source://langchainrb//lib/langchain/llm/response/google_palm_response.rb#7
  def initialize(raw_response, model: T.unsafe(nil), prompt_tokens: T.unsafe(nil)); end

  # source://langchainrb//lib/langchain/llm/response/google_palm_response.rb#24
  def chat_completion; end

  # source://langchainrb//lib/langchain/llm/response/google_palm_response.rb#28
  def chat_completions; end

  # source://langchainrb//lib/langchain/llm/response/google_palm_response.rb#12
  def completion; end

  # source://langchainrb//lib/langchain/llm/response/google_palm_response.rb#20
  def completions; end

  # source://langchainrb//lib/langchain/llm/response/google_palm_response.rb#16
  def embedding; end

  # source://langchainrb//lib/langchain/llm/response/google_palm_response.rb#32
  def embeddings; end

  # Returns the value of attribute prompt_tokens.
  #
  # source://langchainrb//lib/langchain/llm/response/google_palm_response.rb#5
  def prompt_tokens; end

  # source://langchainrb//lib/langchain/llm/response/google_palm_response.rb#36
  def role; end
end

# Wrapper around the Google Vertex AI APIs: https://cloud.google.com/vertex-ai
#
# Gem requirements:
#     gem "googleauth"
#
# Usage:
#     llm = Langchain::LLM::GoogleVertexAI.new(project_id: ENV["GOOGLE_VERTEX_AI_PROJECT_ID"], region: "us-central1")
#
# source://langchainrb//lib/langchain/llm/google_vertex_ai.rb#13
class Langchain::LLM::GoogleVertexAI < ::Langchain::LLM::Base
  # @return [GoogleVertexAI] a new instance of GoogleVertexAI
  #
  # source://langchainrb//lib/langchain/llm/google_vertex_ai.rb#28
  def initialize(project_id:, region:, default_options: T.unsafe(nil)); end

  # Google Cloud has a project id and a specific region of deployment.
  # For GenAI-related things, a safe choice is us-central1.
  #
  # source://langchainrb//lib/langchain/llm/google_vertex_ai.rb#26
  def authorizer; end

  # Generate a chat completion for given messages
  #
  # @param messages [Array<Hash>] Input messages
  # @param model [String] The model that will complete your prompt
  # @param tools [Array<Hash>] The tools to use
  # @param tool_choice [String] The tool choice to use
  # @param system [String] The system instruction to use
  # @raise [ArgumentError]
  # @return [Langchain::LLM::GoogleGeminiResponse] Response object
  #
  # source://langchainrb//lib/langchain/llm/google_vertex_ai.rb#88
  def chat(params = T.unsafe(nil)); end

  # Google Cloud has a project id and a specific region of deployment.
  # For GenAI-related things, a safe choice is us-central1.
  #
  # source://langchainrb//lib/langchain/llm/google_vertex_ai.rb#26
  def defaults; end

  # Generate an embedding for a given text
  #
  # @param text [String] The text to generate an embedding for
  # @param model [String] ID of the model to use
  # @return [Langchain::LLM::GoogleGeminiResponse] Response object
  #
  # source://langchainrb//lib/langchain/llm/google_vertex_ai.rb#58
  def embed(text:, model: T.unsafe(nil)); end

  # Google Cloud has a project id and a specific region of deployment.
  # For GenAI-related things, a safe choice is us-central1.
  #
  # source://langchainrb//lib/langchain/llm/google_vertex_ai.rb#26
  def url; end
end

# source://langchainrb//lib/langchain/llm/google_vertex_ai.rb#14
Langchain::LLM::GoogleVertexAI::DEFAULTS = T.let(T.unsafe(nil), Hash)

# Wrapper around the HuggingFace Inference API: https://huggingface.co/inference-api
#
# Gem requirements:
#     gem "hugging-face", "~> 0.3.4"
#
# Usage:
#     hf = Langchain::LLM::HuggingFace.new(api_key: ENV["HUGGING_FACE_API_KEY"])
#
# source://langchainrb//lib/langchain/llm/hugging_face.rb#13
class Langchain::LLM::HuggingFace < ::Langchain::LLM::Base
  # Intialize the HuggingFace LLM
  #
  # @param api_key [String] The API key to use
  # @return [HuggingFace] a new instance of HuggingFace
  #
  # source://langchainrb//lib/langchain/llm/hugging_face.rb#27
  def initialize(api_key:, default_options: T.unsafe(nil)); end

  # Returns the # of vector dimensions for the embeddings
  #
  # @return [Integer] The # of vector dimensions
  #
  # source://langchainrb//lib/langchain/llm/hugging_face.rb#36
  def default_dimensions; end

  # Generate an embedding for a given text
  #
  # @param text [String] The text to embed
  # @return [Langchain::LLM::HuggingFaceResponse] Response object
  #
  # source://langchainrb//lib/langchain/llm/hugging_face.rb#50
  def embed(text:); end
end

# source://langchainrb//lib/langchain/llm/hugging_face.rb#14
Langchain::LLM::HuggingFace::DEFAULTS = T.let(T.unsafe(nil), Hash)

# source://langchainrb//lib/langchain/llm/hugging_face.rb#18
Langchain::LLM::HuggingFace::EMBEDDING_SIZES = T.let(T.unsafe(nil), Hash)

# source://langchainrb//lib/langchain/llm/response/hugging_face_response.rb#4
class Langchain::LLM::HuggingFaceResponse < ::Langchain::LLM::BaseResponse
  # source://langchainrb//lib/langchain/llm/response/hugging_face_response.rb#9
  def embedding; end

  # source://langchainrb//lib/langchain/llm/response/hugging_face_response.rb#5
  def embeddings; end
end

# A wrapper around the LlamaCpp.rb library
#
# Gem requirements:
#     gem "llama_cpp"
#
# Usage:
#     llama = Langchain::LLM::LlamaCpp.new(
#       model_path: ENV["LLAMACPP_MODEL_PATH"],
#       n_gpu_layers: Integer(ENV["LLAMACPP_N_GPU_LAYERS"]),
#       n_threads: Integer(ENV["LLAMACPP_N_THREADS"])
#     )
#
# source://langchainrb//lib/langchain/llm/llama_cpp.rb#16
class Langchain::LLM::LlamaCpp < ::Langchain::LLM::Base
  # @param model_path [String] The path to the model to use
  # @param n_gpu_layers [Integer] The number of GPU layers to use
  # @param n_ctx [Integer] The number of context tokens to use
  # @param n_threads [Integer] The CPU number of threads to use
  # @param seed [Integer] The seed to use
  # @return [LlamaCpp] a new instance of LlamaCpp
  #
  # source://langchainrb//lib/langchain/llm/llama_cpp.rb#25
  def initialize(model_path:, n_gpu_layers: T.unsafe(nil), n_ctx: T.unsafe(nil), n_threads: T.unsafe(nil), seed: T.unsafe(nil)); end

  # @param prompt [String] The prompt to complete
  # @param n_predict [Integer] The number of tokens to predict
  # @return [String] The completed prompt
  #
  # source://langchainrb//lib/langchain/llm/llama_cpp.rb#51
  def complete(prompt:, n_predict: T.unsafe(nil)); end

  # @param text [String] The text to embed
  # @return [Array<Float>] The embedding
  #
  # source://langchainrb//lib/langchain/llm/llama_cpp.rb#37
  def embed(text:); end

  # Returns the value of attribute model_path.
  #
  # source://langchainrb//lib/langchain/llm/llama_cpp.rb#17
  def model_path; end

  # Sets the attribute model_path
  #
  # @param value the value to set the attribute model_path to.
  #
  # source://langchainrb//lib/langchain/llm/llama_cpp.rb#17
  def model_path=(_arg0); end

  # Returns the value of attribute n_ctx.
  #
  # source://langchainrb//lib/langchain/llm/llama_cpp.rb#17
  def n_ctx; end

  # Sets the attribute n_ctx
  #
  # @param value the value to set the attribute n_ctx to.
  #
  # source://langchainrb//lib/langchain/llm/llama_cpp.rb#17
  def n_ctx=(_arg0); end

  # Returns the value of attribute n_gpu_layers.
  #
  # source://langchainrb//lib/langchain/llm/llama_cpp.rb#17
  def n_gpu_layers; end

  # Sets the attribute n_gpu_layers
  #
  # @param value the value to set the attribute n_gpu_layers to.
  #
  # source://langchainrb//lib/langchain/llm/llama_cpp.rb#17
  def n_gpu_layers=(_arg0); end

  # Sets the attribute n_threads
  #
  # @param value the value to set the attribute n_threads to.
  #
  # source://langchainrb//lib/langchain/llm/llama_cpp.rb#18
  def n_threads=(_arg0); end

  # Returns the value of attribute seed.
  #
  # source://langchainrb//lib/langchain/llm/llama_cpp.rb#17
  def seed; end

  # Sets the attribute seed
  #
  # @param value the value to set the attribute seed to.
  #
  # source://langchainrb//lib/langchain/llm/llama_cpp.rb#17
  def seed=(_arg0); end

  private

  # source://langchainrb//lib/langchain/llm/llama_cpp.rb#87
  def build_completion_context; end

  # source://langchainrb//lib/langchain/llm/llama_cpp.rb#64
  def build_context_params(embeddings: T.unsafe(nil)); end

  # source://langchainrb//lib/langchain/llm/llama_cpp.rb#91
  def build_embedding_context; end

  # source://langchainrb//lib/langchain/llm/llama_cpp.rb#82
  def build_model(embeddings: T.unsafe(nil)); end

  # source://langchainrb//lib/langchain/llm/llama_cpp.rb#75
  def build_model_params; end

  # source://langchainrb//lib/langchain/llm/llama_cpp.rb#95
  def completion_context; end

  # source://langchainrb//lib/langchain/llm/llama_cpp.rb#99
  def embedding_context; end

  # source://langchainrb//lib/langchain/llm/llama_cpp.rb#59
  def n_threads; end
end

# source://langchainrb//lib/langchain/llm/response/llama_cpp_response.rb#4
class Langchain::LLM::LlamaCppResponse < ::Langchain::LLM::BaseResponse
  # source://langchainrb//lib/langchain/llm/response/llama_cpp_response.rb#5
  def embedding; end

  # source://langchainrb//lib/langchain/llm/response/llama_cpp_response.rb#9
  def embeddings; end
end

# Gem requirements:
#    gem "mistral-ai"
#
# Usage:
#    llm = Langchain::LLM::MistralAI.new(api_key: ENV["MISTRAL_AI_API_KEY"])
#
# source://langchainrb//lib/langchain/llm/mistral_ai.rb#9
class Langchain::LLM::MistralAI < ::Langchain::LLM::Base
  # @return [MistralAI] a new instance of MistralAI
  #
  # source://langchainrb//lib/langchain/llm/mistral_ai.rb#17
  def initialize(api_key:, default_options: T.unsafe(nil)); end

  # source://langchainrb//lib/langchain/llm/mistral_ai.rb#35
  def chat(params = T.unsafe(nil)); end

  # Returns the value of attribute defaults.
  #
  # source://langchainrb//lib/langchain/llm/mistral_ai.rb#15
  def defaults; end

  # source://langchainrb//lib/langchain/llm/mistral_ai.rb#43
  def embed(text:, model: T.unsafe(nil), encoding_format: T.unsafe(nil)); end
end

# source://langchainrb//lib/langchain/llm/mistral_ai.rb#10
Langchain::LLM::MistralAI::DEFAULTS = T.let(T.unsafe(nil), Hash)

# source://langchainrb//lib/langchain/llm/response/mistral_ai_response.rb#4
class Langchain::LLM::MistralAIResponse < ::Langchain::LLM::BaseResponse
  # source://langchainrb//lib/langchain/llm/response/mistral_ai_response.rb#9
  def chat_completion; end

  # source://langchainrb//lib/langchain/llm/response/mistral_ai_response.rb#29
  def completion_tokens; end

  # source://langchainrb//lib/langchain/llm/response/mistral_ai_response.rb#33
  def created_at; end

  # source://langchainrb//lib/langchain/llm/response/mistral_ai_response.rb#17
  def embedding; end

  # source://langchainrb//lib/langchain/llm/response/mistral_ai_response.rb#5
  def model; end

  # source://langchainrb//lib/langchain/llm/response/mistral_ai_response.rb#21
  def prompt_tokens; end

  # source://langchainrb//lib/langchain/llm/response/mistral_ai_response.rb#13
  def role; end

  # source://langchainrb//lib/langchain/llm/response/mistral_ai_response.rb#25
  def total_tokens; end
end

# Interface to Ollama API.
# Available models: https://ollama.ai/library
#
# Usage:
#    llm = Langchain::LLM::Ollama.new
#    llm = Langchain::LLM::Ollama.new(url: ENV["OLLAMA_URL"], default_options: {})
#
# source://langchainrb//lib/langchain/llm/ollama.rb#13
class Langchain::LLM::Ollama < ::Langchain::LLM::Base
  # Initialize the Ollama client
  #
  # @param url [String] The URL of the Ollama instance
  # @param default_options [Hash] The default options to use
  # @return [Ollama] a new instance of Ollama
  #
  # source://langchainrb//lib/langchain/llm/ollama.rb#38
  def initialize(url: T.unsafe(nil), default_options: T.unsafe(nil)); end

  # Generate a chat completion
  #
  # Example:
  #
  #  final_resp = ollama.chat(messages:) { |resp| print resp.chat_completion }
  #  final_resp.total_tokens
  #
  # The message object has the following fields:
  #   role: the role of the message, either system, user or assistant
  #   content: the content of the message
  #   images (optional): a list of images to include in the message (for multimodal models such as llava)
  #
  # @option params
  # @option params
  # @option params
  # @option params
  # @option params
  # @option block
  # @param messages [Array] The chat messages
  # @param model [String] The model to use
  # @param params [Hash] Unified chat parmeters from [Langchain::LLM::Parameters::Chat::SCHEMA]
  # @param block [Hash] a customizable set of options
  # @return [Langchain::LLM::OllamaResponse] Response object
  #
  # source://langchainrb//lib/langchain/llm/ollama.rb#175
  def chat(messages:, model: T.unsafe(nil), **params, &block); end

  # Generate the completion for a given prompt
  #
  # Example:
  #
  #  final_resp = ollama.complete(prompt:) { |resp| print resp.completion }
  #  final_resp.total_tokens
  #
  # @option block
  # @param prompt [String] The prompt to complete
  # @param model [String] The model to use
  #   For a list of valid parameters and values, see:
  #   https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values
  # @param block [Hash] a customizable set of options
  # @return [Langchain::LLM::OllamaResponse] Response object
  #
  # source://langchainrb//lib/langchain/llm/ollama.rb#76
  def complete(prompt:, model: T.unsafe(nil), images: T.unsafe(nil), format: T.unsafe(nil), system: T.unsafe(nil), template: T.unsafe(nil), context: T.unsafe(nil), raw: T.unsafe(nil), mirostat: T.unsafe(nil), mirostat_eta: T.unsafe(nil), mirostat_tau: T.unsafe(nil), num_ctx: T.unsafe(nil), num_gqa: T.unsafe(nil), num_gpu: T.unsafe(nil), num_thread: T.unsafe(nil), repeat_last_n: T.unsafe(nil), repeat_penalty: T.unsafe(nil), temperature: T.unsafe(nil), seed: T.unsafe(nil), stop: T.unsafe(nil), tfs_z: T.unsafe(nil), num_predict: T.unsafe(nil), top_k: T.unsafe(nil), top_p: T.unsafe(nil), stop_sequences: T.unsafe(nil), &block); end

  # Returns the # of vector dimensions for the embeddings
  #
  # @return [Integer] The # of vector dimensions
  #
  # source://langchainrb//lib/langchain/llm/ollama.rb#53
  def default_dimensions; end

  # Returns the value of attribute defaults.
  #
  # source://langchainrb//lib/langchain/llm/ollama.rb#14
  def defaults; end

  # Generate an embedding for a given text
  #
  # @param text [String] The text to generate an embedding for
  # @param model [String] The model to use
  # @param options [Hash] The options to use
  # @return [Langchain::LLM::OllamaResponse] Response object
  #
  # source://langchainrb//lib/langchain/llm/ollama.rb#198
  def embed(text:, model: T.unsafe(nil), mirostat: T.unsafe(nil), mirostat_eta: T.unsafe(nil), mirostat_tau: T.unsafe(nil), num_ctx: T.unsafe(nil), num_gqa: T.unsafe(nil), num_gpu: T.unsafe(nil), num_thread: T.unsafe(nil), repeat_last_n: T.unsafe(nil), repeat_penalty: T.unsafe(nil), temperature: T.unsafe(nil), seed: T.unsafe(nil), stop: T.unsafe(nil), tfs_z: T.unsafe(nil), num_predict: T.unsafe(nil), top_k: T.unsafe(nil), top_p: T.unsafe(nil)); end

  # Generate a summary for a given text
  #
  # @param text [String] The text to generate a summary for
  # @return [String] The summary
  #
  # source://langchainrb//lib/langchain/llm/ollama.rb#255
  def summarize(text:); end

  # Returns the value of attribute url.
  #
  # source://langchainrb//lib/langchain/llm/ollama.rb#14
  def url; end

  private

  # source://langchainrb//lib/langchain/llm/ollama.rb#266
  def client; end

  # source://langchainrb//lib/langchain/llm/ollama.rb#291
  def generate_final_chat_completion_response(responses_stream, parameters); end

  # source://langchainrb//lib/langchain/llm/ollama.rb#283
  def generate_final_completion_response(responses_stream, parameters); end

  # source://langchainrb//lib/langchain/llm/ollama.rb#274
  def json_responses_chunk_handler(&block); end
end

# source://langchainrb//lib/langchain/llm/ollama.rb#16
Langchain::LLM::Ollama::DEFAULTS = T.let(T.unsafe(nil), Hash)

# source://langchainrb//lib/langchain/llm/ollama.rb#23
Langchain::LLM::Ollama::EMBEDDING_SIZES = T.let(T.unsafe(nil), Hash)

# source://langchainrb//lib/langchain/llm/response/ollama_response.rb#4
class Langchain::LLM::OllamaResponse < ::Langchain::LLM::BaseResponse
  # @return [OllamaResponse] a new instance of OllamaResponse
  #
  # source://langchainrb//lib/langchain/llm/response/ollama_response.rb#5
  def initialize(raw_response, model: T.unsafe(nil), prompt_tokens: T.unsafe(nil)); end

  # source://langchainrb//lib/langchain/llm/response/ollama_response.rb#14
  def chat_completion; end

  # source://langchainrb//lib/langchain/llm/response/ollama_response.rb#18
  def completion; end

  # source://langchainrb//lib/langchain/llm/response/ollama_response.rb#42
  def completion_tokens; end

  # source://langchainrb//lib/langchain/llm/response/ollama_response.rb#22
  def completions; end

  # source://langchainrb//lib/langchain/llm/response/ollama_response.rb#10
  def created_at; end

  # source://langchainrb//lib/langchain/llm/response/ollama_response.rb#26
  def embedding; end

  # source://langchainrb//lib/langchain/llm/response/ollama_response.rb#30
  def embeddings; end

  # source://langchainrb//lib/langchain/llm/response/ollama_response.rb#38
  def prompt_tokens; end

  # source://langchainrb//lib/langchain/llm/response/ollama_response.rb#34
  def role; end

  # source://langchainrb//lib/langchain/llm/response/ollama_response.rb#46
  def total_tokens; end

  private

  # @return [Boolean]
  #
  # source://langchainrb//lib/langchain/llm/response/ollama_response.rb#52
  def done?; end
end

# LLM interface for OpenAI APIs: https://platform.openai.com/overview
#
# Gem requirements:
#    gem "ruby-openai", "~> 6.3.0"
#
# Usage:
#    openai = Langchain::LLM::OpenAI.new(
#      api_key: ENV["OPENAI_API_KEY"],
#      llm_options: {}, # Available options: https://github.com/alexrudall/ruby-openai/blob/main/lib/openai/client.rb#L5-L13
#      default_options: {}
#    )
#
# source://langchainrb//lib/langchain/llm/openai.rb#15
class Langchain::LLM::OpenAI < ::Langchain::LLM::Base
  # Initialize an OpenAI LLM instance
  #
  # @param api_key [String] The API key to use
  # @param client_options [Hash] Options to pass to the OpenAI::Client constructor
  # @return [OpenAI] a new instance of OpenAI
  #
  # source://langchainrb//lib/langchain/llm/openai.rb#35
  def initialize(api_key:, llm_options: T.unsafe(nil), default_options: T.unsafe(nil)); end

  # Generate a chat completion for given messages.
  #
  # @option params
  # @option params
  # @param params [Hash] unified chat parmeters from [Langchain::LLM::Parameters::Chat::SCHEMA]
  # @raise [ArgumentError]
  #
  # source://langchainrb//lib/langchain/llm/openai.rb#113
  def chat(params = T.unsafe(nil), &block); end

  # Generate a completion for a given prompt
  #
  # @param prompt [String] The prompt to generate a completion for
  # @param params [Hash] The parameters to pass to the `chat()` method
  # @return [Langchain::LLM::OpenAIResponse] Response object
  #
  # source://langchainrb//lib/langchain/llm/openai.rb#96
  def complete(prompt:, **params); end

  # source://langchainrb//lib/langchain/llm/openai.rb#155
  def default_dimensions; end

  # Returns the value of attribute defaults.
  #
  # source://langchainrb//lib/langchain/llm/openai.rb#29
  def defaults; end

  # Generate an embedding for a given text
  #
  # @param text [String] The text to generate an embedding for
  # @param model [String] ID of the model to use
  # @param encoding_format [String] The format to return the embeddings in. Can be either float or base64.
  # @param user [String] A unique identifier representing your end-user
  # @raise [ArgumentError]
  # @return [Langchain::LLM::OpenAIResponse] Response object
  #
  # source://langchainrb//lib/langchain/llm/openai.rb#59
  def embed(text:, model: T.unsafe(nil), encoding_format: T.unsafe(nil), user: T.unsafe(nil), dimensions: T.unsafe(nil)); end

  # Generate a summary for a given text
  #
  # @param text [String] The text to generate a summary for
  # @return [String] The summary
  #
  # source://langchainrb//lib/langchain/llm/openai.rb#146
  def summarize(text:); end

  private

  # source://langchainrb//lib/langchain/llm/openai.rb#163
  def reset_response_chunks; end

  # Returns the value of attribute response_chunks.
  #
  # source://langchainrb//lib/langchain/llm/openai.rb#161
  def response_chunks; end

  # source://langchainrb//lib/langchain/llm/openai.rb#176
  def response_from_chunks; end

  # source://langchainrb//lib/langchain/llm/openai.rb#192
  def tool_calls_from_choice_chunks(choice_chunks); end

  # source://langchainrb//lib/langchain/llm/openai.rb#167
  def with_api_error_handling; end
end

# source://langchainrb//lib/langchain/llm/openai.rb#16
Langchain::LLM::OpenAI::DEFAULTS = T.let(T.unsafe(nil), Hash)

# source://langchainrb//lib/langchain/llm/openai.rb#23
Langchain::LLM::OpenAI::EMBEDDING_SIZES = T.let(T.unsafe(nil), Hash)

# source://langchainrb//lib/langchain/llm/response/openai_response.rb#4
class Langchain::LLM::OpenAIResponse < ::Langchain::LLM::BaseResponse
  # source://langchainrb//lib/langchain/llm/response/openai_response.rb#23
  def chat_completion; end

  # source://langchainrb//lib/langchain/llm/response/openai_response.rb#43
  def chat_completions; end

  # source://langchainrb//lib/langchain/llm/response/openai_response.rb#15
  def completion; end

  # source://langchainrb//lib/langchain/llm/response/openai_response.rb#55
  def completion_tokens; end

  # source://langchainrb//lib/langchain/llm/response/openai_response.rb#39
  def completions; end

  # source://langchainrb//lib/langchain/llm/response/openai_response.rb#9
  def created_at; end

  # source://langchainrb//lib/langchain/llm/response/openai_response.rb#35
  def embedding; end

  # source://langchainrb//lib/langchain/llm/response/openai_response.rb#47
  def embeddings; end

  # source://langchainrb//lib/langchain/llm/response/openai_response.rb#5
  def model; end

  # source://langchainrb//lib/langchain/llm/response/openai_response.rb#51
  def prompt_tokens; end

  # source://langchainrb//lib/langchain/llm/response/openai_response.rb#19
  def role; end

  # source://langchainrb//lib/langchain/llm/response/openai_response.rb#27
  def tool_calls; end

  # source://langchainrb//lib/langchain/llm/response/openai_response.rb#59
  def total_tokens; end
end

# source://langchainrb//lib/langchain/llm/parameters/chat.rb#5
module Langchain::LLM::Parameters; end

# source://langchainrb//lib/langchain/llm/parameters/chat.rb#6
class Langchain::LLM::Parameters::Chat < ::SimpleDelegator
  # @return [Chat] a new instance of Chat
  #
  # source://langchainrb//lib/langchain/llm/parameters/chat.rb#42
  def initialize(parameters: T.unsafe(nil)); end
end

# TODO: At the moment, the UnifiedParamters only considers keys.  In the
# future, we may consider ActiveModel-style validations and further typed
# options here.
#
# source://langchainrb//lib/langchain/llm/parameters/chat.rb#10
Langchain::LLM::Parameters::Chat::SCHEMA = T.let(T.unsafe(nil), Hash)

# Wrapper around Replicate.com LLM provider
#
# Gem requirements:
#     gem "replicate-ruby", "~> 0.2.2"
#
# Use it directly:
#     replicate = Langchain::LLM::Replicate.new(api_key: ENV["REPLICATE_API_KEY"])
#
# Or pass it to be used by a vector search DB:
#     chroma = Langchain::Vectorsearch::Chroma.new(
#       url: ENV["CHROMA_URL"],
#       index_name: "...",
#       llm: replicate
#     )
#
# source://langchainrb//lib/langchain/llm/replicate.rb#20
class Langchain::LLM::Replicate < ::Langchain::LLM::Base
  # Intialize the Replicate LLM
  #
  # @param api_key [String] The API key to use
  # @return [Replicate] a new instance of Replicate
  #
  # source://langchainrb//lib/langchain/llm/replicate.rb#35
  def initialize(api_key:, default_options: T.unsafe(nil)); end

  # Generate a completion for a given prompt
  #
  # @param prompt [String] The prompt to generate a completion for
  # @return [Langchain::LLM::ReplicateResponse] Reponse object
  #
  # source://langchainrb//lib/langchain/llm/replicate.rb#69
  def complete(prompt:, **params); end

  # Generate an embedding for a given text
  #
  # @param text [String] The text to generate an embedding for
  # @return [Langchain::LLM::ReplicateResponse] Response object
  #
  # source://langchainrb//lib/langchain/llm/replicate.rb#52
  def embed(text:); end

  # Generate an embedding for a given text
  #
  # @param text [String] The text to generate an embedding for
  # @return [Langchain::LLM::ReplicateResponse] Response object
  #
  # source://langchainrb//lib/langchain/llm/replicate.rb#52
  def generate_embedding(text:); end

  # Generate a summary for a given text
  #
  # @param text [String] The text to generate a summary for
  # @return [String] The summary
  #
  # source://langchainrb//lib/langchain/llm/replicate.rb#86
  def summarize(text:); end

  private

  # source://langchainrb//lib/langchain/llm/replicate.rb#104
  def completion_model; end

  # source://langchainrb//lib/langchain/llm/replicate.rb#108
  def embeddings_model; end
end

# source://langchainrb//lib/langchain/llm/replicate.rb#21
Langchain::LLM::Replicate::DEFAULTS = T.let(T.unsafe(nil), Hash)

# source://langchainrb//lib/langchain/llm/response/replicate_response.rb#4
class Langchain::LLM::ReplicateResponse < ::Langchain::LLM::BaseResponse
  # source://langchainrb//lib/langchain/llm/response/replicate_response.rb#12
  def completion; end

  # source://langchainrb//lib/langchain/llm/response/replicate_response.rb#5
  def completions; end

  # source://langchainrb//lib/langchain/llm/response/replicate_response.rb#16
  def created_at; end

  # source://langchainrb//lib/langchain/llm/response/replicate_response.rb#20
  def embedding; end

  # source://langchainrb//lib/langchain/llm/response/replicate_response.rb#24
  def embeddings; end
end

# source://langchainrb//lib/langchain/llm/unified_parameters.rb#4
class Langchain::LLM::UnifiedParameters
  include ::Enumerable

  # @return [UnifiedParameters] a new instance of UnifiedParameters
  #
  # source://langchainrb//lib/langchain/llm/unified_parameters.rb#15
  def initialize(schema:, parameters: T.unsafe(nil)); end

  # source://langchainrb//lib/langchain/llm/unified_parameters.rb#84
  def <=>(other); end

  # source://langchainrb//lib/langchain/llm/unified_parameters.rb#88
  def [](key); end

  # source://langchainrb//lib/langchain/llm/unified_parameters.rb#71
  def alias_field(field_name, as:); end

  # Returns the value of attribute aliases.
  #
  # source://langchainrb//lib/langchain/llm/unified_parameters.rb#7
  def aliases; end

  # source://langchainrb//lib/langchain/llm/unified_parameters.rb#80
  def each(&_arg0); end

  # source://langchainrb//lib/langchain/llm/unified_parameters.rb#67
  def ignore(*field_names); end

  # Returns the value of attribute ignored.
  #
  # source://langchainrb//lib/langchain/llm/unified_parameters.rb#7
  def ignored; end

  # Returns the value of attribute parameters.
  #
  # source://langchainrb//lib/langchain/llm/unified_parameters.rb#7
  def parameters; end

  # source://langchainrb//lib/langchain/llm/unified_parameters.rb#48
  def remap(field_map); end

  # Returns the value of attribute remapped.
  #
  # source://langchainrb//lib/langchain/llm/unified_parameters.rb#7
  def remapped; end

  # Returns the value of attribute schema.
  #
  # source://langchainrb//lib/langchain/llm/unified_parameters.rb#7
  def schema; end

  # source://langchainrb//lib/langchain/llm/unified_parameters.rb#76
  def to_h; end

  # source://langchainrb//lib/langchain/llm/unified_parameters.rb#26
  def to_params(params = T.unsafe(nil)); end

  # source://langchainrb//lib/langchain/llm/unified_parameters.rb#56
  def update(schema = T.unsafe(nil)); end

  private

  # @return [Boolean]
  #
  # source://langchainrb//lib/langchain/llm/unified_parameters.rb#94
  def value_present?(value); end
end

# source://langchainrb//lib/langchain/llm/unified_parameters.rb#9
class Langchain::LLM::UnifiedParameters::Null < ::Langchain::LLM::UnifiedParameters
  # @return [Null] a new instance of Null
  #
  # source://langchainrb//lib/langchain/llm/unified_parameters.rb#10
  def initialize(parameters: T.unsafe(nil)); end
end

# source://langchainrb//lib/langchain/loader.rb#6
class Langchain::Loader
  # Initialize Langchain::Loader
  #
  # @param path [String | Pathname] path to file or URL
  # @param options [Hash] options passed to the processor class used to process the data
  # @return [Langchain::Loader] loader instance
  #
  # source://langchainrb//lib/langchain/loader.rb#42
  def initialize(path, options = T.unsafe(nil), chunker: T.unsafe(nil)); end

  # Is the path a directory
  #
  # @return [Boolean] true if path is a directory
  #
  # source://langchainrb//lib/langchain/loader.rb#60
  def directory?; end

  # Load data from a file or URL
  #
  #    loader = Langchain::Loader.new("README.md")
  #    # Load data using default processor for the file
  #    loader.load
  #
  #    # Load data using a custom processor
  #    loader.load do |raw_data, options|
  #      # your processing code goes here
  #      # return data at the end here
  #    end
  #
  #
  # @return [Data] data that was loaded
  # @yield [String, Hash] handle parsing raw output into string directly
  # @yieldparam raw_data [String] from the loaded URL or file
  # @yieldreturn [String] parsed data, as a String
  #
  # source://langchainrb//lib/langchain/loader.rb#82
  def load(&block); end

  # Is the path a URL?
  #
  # @return [Boolean] true if path is URL
  #
  # source://langchainrb//lib/langchain/loader.rb#51
  def url?; end

  private

  # source://langchainrb//lib/langchain/loader.rb#131
  def find_processor; end

  # source://langchainrb//lib/langchain/loader.rb#103
  def load_from_directory(&block); end

  # @raise [FileNotFound]
  #
  # source://langchainrb//lib/langchain/loader.rb#96
  def load_from_path; end

  # source://langchainrb//lib/langchain/loader.rb#92
  def load_from_url; end

  # source://langchainrb//lib/langchain/loader.rb#147
  def lookup_constant; end

  # source://langchainrb//lib/langchain/loader.rb#113
  def process_data(data, &block); end

  # @raise [UnknownFormatError]
  #
  # source://langchainrb//lib/langchain/loader.rb#125
  def processor_klass; end

  # @return [Boolean]
  #
  # source://langchainrb//lib/langchain/loader.rb#135
  def processor_matches?(constant, value); end

  # source://langchainrb//lib/langchain/loader.rb#139
  def processors; end

  # source://langchainrb//lib/langchain/loader.rb#143
  def source_type; end

  class << self
    # Load data from a file or URL. Shorthand for  `Langchain::Loader.new(path).load`
    #
    # == Examples
    #
    #     # load a URL
    #     data = Langchain::Loader.load("https://example.com/docs/README.md")
    #
    #     # load a file
    #     data = Langchain::Loader.load("README.md")
    #
    #    # Load data using a custom processor
    #    data = Langchain::Loader.load("README.md") do |raw_data, options|
    #      # your processing code goes here
    #      # return data at the end here
    #    end
    #
    #
    # @param path [String | Pathname] path to file or URL
    # @param options [Hash] options passed to the processor class used to process the data
    # @return [Data] data loaded from path
    #
    # source://langchainrb//lib/langchain/loader.rb#33
    def load(path, options = T.unsafe(nil), &block); end
  end
end

# source://langchainrb//lib/langchain/loader.rb#7
class Langchain::Loader::FileNotFound < ::StandardError; end

# source://langchainrb//lib/langchain/loader.rb#11
Langchain::Loader::URI_REGEX = T.let(T.unsafe(nil), Regexp)

# source://langchainrb//lib/langchain/loader.rb#9
class Langchain::Loader::UnknownFormatError < ::StandardError; end

# source://langchainrb//lib/langchain/assistants/messages/anthropic_message.rb#4
module Langchain::Messages; end

# source://langchainrb//lib/langchain/assistants/messages/anthropic_message.rb#5
class Langchain::Messages::AnthropicMessage < ::Langchain::Messages::Base
  # @raise [ArgumentError]
  # @return [AnthropicMessage] a new instance of AnthropicMessage
  #
  # source://langchainrb//lib/langchain/assistants/messages/anthropic_message.rb#14
  def initialize(role:, content: T.unsafe(nil), tool_calls: T.unsafe(nil), tool_call_id: T.unsafe(nil)); end

  # Check if the message came from an LLM
  #
  # @return [Boolean] true/false whether this message was produced by an LLM
  #
  # source://langchainrb//lib/langchain/assistants/messages/anthropic_message.rb#63
  def assistant?; end

  # Check if the message came from an LLM
  #
  # @return [Boolean] true/false whether this message was produced by an LLM
  #
  # source://langchainrb//lib/langchain/assistants/messages/anthropic_message.rb#70
  def llm?; end

  # Anthropic does not implement system prompts
  #
  # @return [Boolean]
  #
  # source://langchainrb//lib/langchain/assistants/messages/anthropic_message.rb#56
  def system?; end

  # Convert the message to an Anthropic API-compatible hash
  #
  # @return [Hash] The message as an Anthropic API-compatible hash
  #
  # source://langchainrb//lib/langchain/assistants/messages/anthropic_message.rb#28
  def to_hash; end

  # Check if the message is a tool call
  #
  # @return [Boolean] true/false whether this message is a tool call
  #
  # source://langchainrb//lib/langchain/assistants/messages/anthropic_message.rb#51
  def tool?; end
end

# source://langchainrb//lib/langchain/assistants/messages/anthropic_message.rb#6
Langchain::Messages::AnthropicMessage::ROLES = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/assistants/messages/anthropic_message.rb#12
Langchain::Messages::AnthropicMessage::TOOL_ROLE = T.let(T.unsafe(nil), String)

# source://langchainrb//lib/langchain/assistants/messages/base.rb#5
class Langchain::Messages::Base
  # Returns the value of attribute content.
  #
  # source://langchainrb//lib/langchain/assistants/messages/base.rb#6
  def content; end

  # Returns the value of attribute role.
  #
  # source://langchainrb//lib/langchain/assistants/messages/base.rb#6
  def role; end

  # Returns the value of attribute tool_call_id.
  #
  # source://langchainrb//lib/langchain/assistants/messages/base.rb#6
  def tool_call_id; end

  # Returns the value of attribute tool_calls.
  #
  # source://langchainrb//lib/langchain/assistants/messages/base.rb#6
  def tool_calls; end

  # Check if the message came from a user
  #
  # @param true/false [Boolean] whether the message came from a user
  # @return [Boolean]
  #
  # source://langchainrb//lib/langchain/assistants/messages/base.rb#11
  def user?; end
end

# source://langchainrb//lib/langchain/assistants/messages/google_gemini_message.rb#5
class Langchain::Messages::GoogleGeminiMessage < ::Langchain::Messages::Base
  # Initialize a new Google Gemini message
  #
  # @param The [String] role of the message
  # @param The [String] content of the message
  # @param The [Array<Hash>] tool calls made in the message
  # @param The [String] ID of the tool call
  # @raise [ArgumentError]
  # @return [GoogleGeminiMessage] a new instance of GoogleGeminiMessage
  #
  # source://langchainrb//lib/langchain/assistants/messages/google_gemini_message.rb#21
  def initialize(role:, content: T.unsafe(nil), tool_calls: T.unsafe(nil), tool_call_id: T.unsafe(nil)); end

  # Check if the message is a tool call
  #
  # @return [Boolean] true/false whether this message is a tool call
  #
  # source://langchainrb//lib/langchain/assistants/messages/google_gemini_message.rb#78
  def function?; end

  # Check if the message came from an LLM
  #
  # @return [Boolean] true/false whether this message was produced by an LLM
  #
  # source://langchainrb//lib/langchain/assistants/messages/google_gemini_message.rb#35
  def llm?; end

  # Check if the message came from an LLM
  #
  # @return [Boolean] true/false whether this message was produced by an LLM
  #
  # source://langchainrb//lib/langchain/assistants/messages/google_gemini_message.rb#85
  def model?; end

  # Google Gemini does not implement system prompts
  #
  # @return [Boolean]
  #
  # source://langchainrb//lib/langchain/assistants/messages/google_gemini_message.rb#64
  def system?; end

  # Convert the message to a Google Gemini API-compatible hash
  #
  # @return [Hash] The message as a Google Gemini API-compatible hash
  #
  # source://langchainrb//lib/langchain/assistants/messages/google_gemini_message.rb#42
  def to_hash; end

  # Check if the message is a tool call
  #
  # @return [Boolean] true/false whether this message is a tool call
  #
  # source://langchainrb//lib/langchain/assistants/messages/google_gemini_message.rb#71
  def tool?; end
end

# Google Gemini uses the following roles:
#
# source://langchainrb//lib/langchain/assistants/messages/google_gemini_message.rb#7
Langchain::Messages::GoogleGeminiMessage::ROLES = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/assistants/messages/google_gemini_message.rb#13
Langchain::Messages::GoogleGeminiMessage::TOOL_ROLE = T.let(T.unsafe(nil), String)

# source://langchainrb//lib/langchain/assistants/messages/openai_message.rb#5
class Langchain::Messages::OpenAIMessage < ::Langchain::Messages::Base
  # Initialize a new OpenAI message
  #
  # @param The [String] role of the message
  # @param The [String] content of the message
  # @param The [Array<Hash>] tool calls made in the message
  # @param The [String] ID of the tool call
  # @raise [ArgumentError]
  # @return [OpenAIMessage] a new instance of OpenAIMessage
  #
  # source://langchainrb//lib/langchain/assistants/messages/openai_message.rb#22
  def initialize(role:, content: T.unsafe(nil), tool_calls: T.unsafe(nil), tool_call_id: T.unsafe(nil)); end

  # Check if the message came from an LLM
  #
  # @return [Boolean] true/false whether this message was produced by an LLM
  #
  # source://langchainrb//lib/langchain/assistants/messages/openai_message.rb#55
  def assistant?; end

  # Check if the message came from an LLM
  #
  # @return [Boolean] true/false whether this message was produced by an LLM
  #
  # source://langchainrb//lib/langchain/assistants/messages/openai_message.rb#36
  def llm?; end

  # Check if the message are system instructions
  #
  # @return [Boolean] true/false whether this message are system instructions
  #
  # source://langchainrb//lib/langchain/assistants/messages/openai_message.rb#62
  def system?; end

  # Convert the message to an OpenAI API-compatible hash
  #
  # @return [Hash] The message as an OpenAI API-compatible hash
  #
  # source://langchainrb//lib/langchain/assistants/messages/openai_message.rb#43
  def to_hash; end

  # Check if the message is a tool call
  #
  # @return [Boolean] true/false whether this message is a tool call
  #
  # source://langchainrb//lib/langchain/assistants/messages/openai_message.rb#69
  def tool?; end
end

# OpenAI uses the following roles:
#
# source://langchainrb//lib/langchain/assistants/messages/openai_message.rb#7
Langchain::Messages::OpenAIMessage::ROLES = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/assistants/messages/openai_message.rb#14
Langchain::Messages::OpenAIMessage::TOOL_ROLE = T.let(T.unsafe(nil), String)

# source://langchainrb//lib/langchain/output_parsers/base.rb#3
module Langchain::OutputParsers; end

# Structured output parsers from the LLM.
#
# @abstract
#
# source://langchainrb//lib/langchain/output_parsers/base.rb#7
class Langchain::OutputParsers::Base
  # Return a string describing the format of the output.
  #
  # ```json
  # {
  #  "foo": "bar"
  # }
  # ```
  #
  # @example returns the format instructions
  # @raise [NotImplementedError]
  # @return [String] Format instructions.
  #
  # source://langchainrb//lib/langchain/output_parsers/base.rb#27
  def get_format_instructions; end

  # Parse the output of an LLM call.
  #
  # @param text - LLM output to parse.
  # @raise [NotImplementedError]
  # @return [Object] Parsed output.
  #
  # source://langchainrb//lib/langchain/output_parsers/base.rb#13
  def parse(text:); end
end

# = Output Fixing Parser
#
# source://langchainrb//lib/langchain/output_parsers/output_fixing_parser.rb#6
class Langchain::OutputParsers::OutputFixingParser < ::Langchain::OutputParsers::Base
  # Initializes a new instance of the class.
  #
  # @param llm [Langchain::LLM] The LLM used in the fixing process
  # @param parser [Langchain::OutputParsers] The parser originally used which resulted in parsing error
  # @param prompt [Langchain::Prompt::PromptTemplate]
  # @raise [ArgumentError]
  # @return [OutputFixingParser] a new instance of OutputFixingParser
  #
  # source://langchainrb//lib/langchain/output_parsers/output_fixing_parser.rb#14
  def initialize(llm:, parser:, prompt:); end

  # calls get_format_instructions on the @parser
  #
  # according to the @schema.
  #
  # @return [String] Instructions for how the output of a language model should be formatted
  #
  # source://langchainrb//lib/langchain/output_parsers/output_fixing_parser.rb#35
  def get_format_instructions; end

  # Returns the value of attribute llm.
  #
  # source://langchainrb//lib/langchain/output_parsers/output_fixing_parser.rb#7
  def llm; end

  # Parse the output of an LLM call, if fails with OutputParserException
  # then call the LLM with a fix prompt in an attempt to get the correctly
  # formatted response
  #
  # @param completion [String] Text output from the LLM call
  # @return [Object] object that is succesfully parsed by @parser.parse
  #
  # source://langchainrb//lib/langchain/output_parsers/output_fixing_parser.rb#46
  def parse(completion); end

  # Returns the value of attribute parser.
  #
  # source://langchainrb//lib/langchain/output_parsers/output_fixing_parser.rb#7
  def parser; end

  # Returns the value of attribute prompt.
  #
  # source://langchainrb//lib/langchain/output_parsers/output_fixing_parser.rb#7
  def prompt; end

  # source://langchainrb//lib/langchain/output_parsers/output_fixing_parser.rb#23
  def to_h; end

  class << self
    # Creates a new instance of the class using the given JSON::Schema.
    #
    # @param llm [Langchain::LLM] The LLM used in the fixing process
    # @param parser [Langchain::OutputParsers] The parser originally used which resulted in parsing error
    # @param prompt [Langchain::Prompt::PromptTemplate]
    # @return [Object] A new instance of the class
    #
    # source://langchainrb//lib/langchain/output_parsers/output_fixing_parser.rb#67
    def from_llm(llm:, parser:, prompt: T.unsafe(nil)); end

    private

    # source://langchainrb//lib/langchain/output_parsers/output_fixing_parser.rb#73
    def naive_fix_prompt; end
  end
end

# source://langchainrb//lib/langchain/output_parsers/base.rb#32
class Langchain::OutputParsers::OutputParserException < ::StandardError
  # @return [OutputParserException] a new instance of OutputParserException
  #
  # source://langchainrb//lib/langchain/output_parsers/base.rb#33
  def initialize(message, text); end

  # source://langchainrb//lib/langchain/output_parsers/base.rb#38
  def to_s; end
end

# = Structured Output Parser
#
# source://langchainrb//lib/langchain/output_parsers/structured_output_parser.rb#8
class Langchain::OutputParsers::StructuredOutputParser < ::Langchain::OutputParsers::Base
  # Initializes a new instance of the class.
  #
  # @param schema [JSON::Schema] The json schema
  # @return [StructuredOutputParser] a new instance of StructuredOutputParser
  #
  # source://langchainrb//lib/langchain/output_parsers/structured_output_parser.rb#14
  def initialize(schema:); end

  # Returns a string containing instructions for how the output of a language model should be formatted
  # according to the @schema.
  #
  # according to the @schema.
  #
  # @return [String] Instructions for how the output of a language model should be formatted
  #
  # source://langchainrb//lib/langchain/output_parsers/structured_output_parser.rb#39
  def get_format_instructions; end

  # Parse the output of an LLM call extracting an object that abides by the @schema
  #
  # @param text [String] Text output from the LLM call
  # @return [Object] object that abides by the @schema
  #
  # source://langchainrb//lib/langchain/output_parsers/structured_output_parser.rb#62
  def parse(text); end

  # Returns the value of attribute schema.
  #
  # source://langchainrb//lib/langchain/output_parsers/structured_output_parser.rb#9
  def schema; end

  # source://langchainrb//lib/langchain/output_parsers/structured_output_parser.rb#18
  def to_h; end

  private

  # source://langchainrb//lib/langchain/output_parsers/structured_output_parser.rb#73
  def validate_schema!(schema); end

  class << self
    # Creates a new instance of the class using the given JSON::Schema.
    #
    # @param schema [JSON::Schema] The JSON::Schema to use
    # @return [Object] A new instance of the class
    #
    # source://langchainrb//lib/langchain/output_parsers/structured_output_parser.rb#30
    def from_json_schema(schema); end
  end
end

# source://langchainrb//lib/langchain/processors/base.rb#4
module Langchain::Processors; end

# Processors load and parse/process various data types such as CSVs, PDFs, Word documents, HTML pages, and others.
#
# source://langchainrb//lib/langchain/processors/base.rb#6
class Langchain::Processors::Base
  include ::Langchain::DependencyHelper

  # @return [Base] a new instance of Base
  #
  # source://langchainrb//lib/langchain/processors/base.rb#12
  def initialize(options = T.unsafe(nil)); end

  # @raise [NotImplementedError]
  #
  # source://langchainrb//lib/langchain/processors/base.rb#16
  def parse(data); end
end

# source://langchainrb//lib/langchain/processors/base.rb#10
Langchain::Processors::Base::CONTENT_TYPES = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/processors/base.rb#9
Langchain::Processors::Base::EXTENSIONS = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/processors/csv.rb#7
class Langchain::Processors::CSV < ::Langchain::Processors::Base
  # Parse the document and return the text
  #
  # @param data [File]
  # @return [String]
  #
  # source://langchainrb//lib/langchain/processors/csv.rb#20
  def parse(data); end

  private

  # source://langchainrb//lib/langchain/processors/csv.rb#56
  def chunk_file(data); end

  # source://langchainrb//lib/langchain/processors/csv.rb#37
  def chunk_mode; end

  # source://langchainrb//lib/langchain/processors/csv.rb#47
  def chunk_row(data); end

  # source://langchainrb//lib/langchain/processors/csv.rb#33
  def separator; end
end

# source://langchainrb//lib/langchain/processors/csv.rb#12
Langchain::Processors::CSV::CHUNK_MODE = T.let(T.unsafe(nil), Hash)

# source://langchainrb//lib/langchain/processors/csv.rb#11
Langchain::Processors::CSV::CONTENT_TYPES = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/processors/csv.rb#10
Langchain::Processors::CSV::EXTENSIONS = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/processors/csv.rb#8
class Langchain::Processors::CSV::InvalidChunkMode < ::StandardError; end

# source://langchainrb//lib/langchain/processors/docx.rb#5
class Langchain::Processors::Docx < ::Langchain::Processors::Base
  # @return [Docx] a new instance of Docx
  #
  # source://langchainrb//lib/langchain/processors/docx.rb#9
  def initialize(*_arg0); end

  # Parse the document and return the text
  #
  # @param data [File]
  # @return [String]
  #
  # source://langchainrb//lib/langchain/processors/docx.rb#16
  def parse(data); end
end

# source://langchainrb//lib/langchain/processors/docx.rb#7
Langchain::Processors::Docx::CONTENT_TYPES = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/processors/docx.rb#6
Langchain::Processors::Docx::EXTENSIONS = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/processors/eml.rb#5
class Langchain::Processors::Eml < ::Langchain::Processors::Base
  # @return [Eml] a new instance of Eml
  #
  # source://langchainrb//lib/langchain/processors/eml.rb#9
  def initialize(*_arg0); end

  # Parse the document and return the cleaned text
  #
  # @param data [File]
  # @return [String]
  #
  # source://langchainrb//lib/langchain/processors/eml.rb#16
  def parse(data); end

  private

  # Clean and format the extracted content
  #
  # source://langchainrb//lib/langchain/processors/eml.rb#54
  def clean_content(content); end

  # Extract text content from the email, preferring plaintext over HTML
  #
  # source://langchainrb//lib/langchain/processors/eml.rb#25
  def extract_text_content(mail); end
end

# source://langchainrb//lib/langchain/processors/eml.rb#7
Langchain::Processors::Eml::CONTENT_TYPES = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/processors/eml.rb#6
Langchain::Processors::Eml::EXTENSIONS = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/processors/html.rb#5
class Langchain::Processors::HTML < ::Langchain::Processors::Base
  # @return [HTML] a new instance of HTML
  #
  # source://langchainrb//lib/langchain/processors/html.rb#12
  def initialize(*_arg0); end

  # Parse the document and return the text
  #
  # @param data [File]
  # @return [String]
  #
  # source://langchainrb//lib/langchain/processors/html.rb#19
  def parse(data); end
end

# source://langchainrb//lib/langchain/processors/html.rb#7
Langchain::Processors::HTML::CONTENT_TYPES = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/processors/html.rb#6
Langchain::Processors::HTML::EXTENSIONS = T.let(T.unsafe(nil), Array)

# We only look for headings and paragraphs
#
# source://langchainrb//lib/langchain/processors/html.rb#10
Langchain::Processors::HTML::TEXT_CONTENT_TAGS = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/processors/json.rb#5
class Langchain::Processors::JSON < ::Langchain::Processors::Base
  # Parse the document and return the text
  #
  # @param data [File]
  # @return [Hash]
  #
  # source://langchainrb//lib/langchain/processors/json.rb#12
  def parse(data); end
end

# source://langchainrb//lib/langchain/processors/json.rb#7
Langchain::Processors::JSON::CONTENT_TYPES = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/processors/json.rb#6
Langchain::Processors::JSON::EXTENSIONS = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/processors/jsonl.rb#5
class Langchain::Processors::JSONL < ::Langchain::Processors::Base
  # Parse the document and return the text
  #
  # @param data [File]
  # @return [Array of Hash]
  #
  # source://langchainrb//lib/langchain/processors/jsonl.rb#12
  def parse(data); end
end

# source://langchainrb//lib/langchain/processors/jsonl.rb#7
Langchain::Processors::JSONL::CONTENT_TYPES = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/processors/jsonl.rb#6
Langchain::Processors::JSONL::EXTENSIONS = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/processors/markdown.rb#5
class Langchain::Processors::Markdown < ::Langchain::Processors::Base
  # Parse the document and return the text
  #
  # @param data [File]
  # @return [String]
  #
  # source://langchainrb//lib/langchain/processors/markdown.rb#12
  def parse(data); end
end

# source://langchainrb//lib/langchain/processors/markdown.rb#7
Langchain::Processors::Markdown::CONTENT_TYPES = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/processors/markdown.rb#6
Langchain::Processors::Markdown::EXTENSIONS = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/processors/pdf.rb#5
class Langchain::Processors::PDF < ::Langchain::Processors::Base
  # @return [PDF] a new instance of PDF
  #
  # source://langchainrb//lib/langchain/processors/pdf.rb#9
  def initialize(*_arg0); end

  # Parse the document and return the text
  #
  # @param data [File]
  # @return [String]
  #
  # source://langchainrb//lib/langchain/processors/pdf.rb#16
  def parse(data); end
end

# source://langchainrb//lib/langchain/processors/pdf.rb#7
Langchain::Processors::PDF::CONTENT_TYPES = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/processors/pdf.rb#6
Langchain::Processors::PDF::EXTENSIONS = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/processors/pptx.rb#5
class Langchain::Processors::Pptx < ::Langchain::Processors::Base
  # @return [Pptx] a new instance of Pptx
  #
  # source://langchainrb//lib/langchain/processors/pptx.rb#9
  def initialize(*_arg0); end

  # Parse the document and return the text
  #
  # @param data [File]
  # @return [String]
  #
  # source://langchainrb//lib/langchain/processors/pptx.rb#16
  def parse(data); end
end

# source://langchainrb//lib/langchain/processors/pptx.rb#7
Langchain::Processors::Pptx::CONTENT_TYPES = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/processors/pptx.rb#6
Langchain::Processors::Pptx::EXTENSIONS = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/processors/text.rb#5
class Langchain::Processors::Text < ::Langchain::Processors::Base
  # Parse the document and return the text
  #
  # @param data [File]
  # @return [String]
  #
  # source://langchainrb//lib/langchain/processors/text.rb#12
  def parse(data); end
end

# source://langchainrb//lib/langchain/processors/text.rb#7
Langchain::Processors::Text::CONTENT_TYPES = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/processors/text.rb#6
Langchain::Processors::Text::EXTENSIONS = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/processors/xls.rb#5
class Langchain::Processors::Xls < ::Langchain::Processors::Base
  # @return [Xls] a new instance of Xls
  #
  # source://langchainrb//lib/langchain/processors/xls.rb#9
  def initialize(*_arg0); end

  # Parse the document and return the text
  #
  # @param data [File]
  # @return [Array<Array<String>>] Array of rows, each row is an array of cells
  #
  # source://langchainrb//lib/langchain/processors/xls.rb#17
  def parse(data); end
end

# source://langchainrb//lib/langchain/processors/xls.rb#7
Langchain::Processors::Xls::CONTENT_TYPES = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/processors/xls.rb#6
Langchain::Processors::Xls::EXTENSIONS = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/processors/xlsx.rb#5
class Langchain::Processors::Xlsx < ::Langchain::Processors::Base
  # @return [Xlsx] a new instance of Xlsx
  #
  # source://langchainrb//lib/langchain/processors/xlsx.rb#9
  def initialize(*_arg0); end

  # Parse the document and return the text
  #
  # @param data [File]
  # @return [Array<Array<String>>] Array of rows, each row is an array of cells
  #
  # source://langchainrb//lib/langchain/processors/xlsx.rb#16
  def parse(data); end
end

# source://langchainrb//lib/langchain/processors/xlsx.rb#7
Langchain::Processors::Xlsx::CONTENT_TYPES = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/processors/xlsx.rb#6
Langchain::Processors::Xlsx::EXTENSIONS = T.let(T.unsafe(nil), Array)

# source://langchainrb//lib/langchain/prompt.rb#2
module Langchain::Prompt
  include ::Langchain::Prompt::Loading
  extend ::Langchain::Prompt::Loading::ClassMethods
end

# Prompts are structured inputs to the LLMs. Prompts provide instructions, context and other user input that LLMs use to generate responses.
#
# @abstract
#
# source://langchainrb//lib/langchain/prompt/base.rb#11
class Langchain::Prompt::Base
  # @raise [NotImplementedError]
  #
  # source://langchainrb//lib/langchain/prompt/base.rb#12
  def format(**kwargs); end

  # @raise [NotImplementedError]
  # @return [String] the type of the prompt
  #
  # source://langchainrb//lib/langchain/prompt/base.rb#17
  def prompt_type; end

  # Save the object to a file in JSON or YAML format.
  #
  # @param file_path [String, Pathname] The path to the file to save the object to
  # @raise [ArgumentError] If file_path doesn't end with .json or .yaml or .yml
  # @return [void]
  #
  # source://langchainrb//lib/langchain/prompt/base.rb#56
  def save(file_path:); end

  # @raise [NotImplementedError]
  # @return [Hash] a hash representation of the prompt
  #
  # source://langchainrb//lib/langchain/prompt/base.rb#22
  def to_h; end

  # Validate the input variables against the template.
  #
  # @param template [String] The template to validate against.
  # @param input_variables [Array<String>] The input variables to validate.
  # @raise [ArgumentError] If there are missing or extra variables.
  # @return [void]
  #
  # source://langchainrb//lib/langchain/prompt/base.rb#36
  def validate(template:, input_variables:); end

  class << self
    # Extracts variables from a template string.
    #
    # This method takes a template string and returns an array of input variable names
    # contained within the template. Input variables are defined as text enclosed in
    # curly braces (e.g. <code>\{variable_name\}</code>).
    #
    # Content within two consecutive curly braces (e.g. <code>\{\{ignore_me}}</code>) are ignored.
    #
    # @param template [String] The template string to extract variables from.
    # @return [Array<String>] An array of input variable names.
    #
    # source://langchainrb//lib/langchain/prompt/base.rb#84
    def extract_variables_from_template(template); end
  end
end

# = Few Shot Prompt Templates
#
# Create a prompt with a few shot examples:
#
#     prompt = Langchain::Prompt::FewShotPromptTemplate.new(
#       prefix: "Write antonyms for the following words.",
#       suffix: "Input: <code>{adjective}</code>\nOutput:",
#       example_prompt: Langchain::Prompt::PromptTemplate.new(
#         input_variables: ["input", "output"],
#         template: "Input: {input}\nOutput: {output}"
#       ),
#       examples: [
#         { "input": "happy", "output": "sad" },
#         { "input": "tall", "output": "short" }
#       ],
#        input_variables: ["adjective"]
#     )
#
#     prompt.format(adjective: "good")
#
#     # Write antonyms for the following words.
#     #
#     # Input: happy
#     # Output: sad
#     #
#     # Input: tall
#     # Output: short
#     #
#     # Input: good
#     # Output:
#
# Save prompt template to JSON file:
#
#     prompt.save(file_path: "spec/fixtures/prompt/few_shot_prompt_template.json")
#
# Loading a new prompt template using a JSON file:
#
#     prompt = Langchain::Prompt.load_from_path(file_path: "spec/fixtures/prompt/few_shot_prompt_template.json")
#     prompt.prefix # "Write antonyms for the following words."
#
# Loading a new prompt template using a YAML file:
#
#     prompt = Langchain::Prompt.load_from_path(file_path: "spec/fixtures/prompt/prompt_template.yaml")
#     prompt.input_variables #=> ["adjective", "content"]
#
# source://langchainrb//lib/langchain/prompt/few_shot_prompt_template.rb#49
class Langchain::Prompt::FewShotPromptTemplate < ::Langchain::Prompt::Base
  # Initializes a new instance of the class.
  #
  # @param examples [Array<Hash>] Examples to format into the prompt.
  # @param example_prompt [PromptTemplate] PromptTemplate used to format an individual example.
  # @param suffix [String] A prompt template string to put after the examples.
  # @param input_variables [Array<String>] A list of the names of the variables the prompt template expects.
  # @param example_separator [String] String separator used to join the prefix, the examples, and suffix.
  # @param prefix [String] A prompt template string to put before the examples.
  # @param validate_template [Boolean] Whether or not to try validating the template.
  # @return [FewShotPromptTemplate] a new instance of FewShotPromptTemplate
  #
  # source://langchainrb//lib/langchain/prompt/few_shot_prompt_template.rb#63
  def initialize(examples:, example_prompt:, input_variables:, suffix:, prefix: T.unsafe(nil), example_separator: T.unsafe(nil), validate_template: T.unsafe(nil)); end

  # Returns the value of attribute example_prompt.
  #
  # source://langchainrb//lib/langchain/prompt/few_shot_prompt_template.rb#50
  def example_prompt; end

  # Returns the value of attribute example_separator.
  #
  # source://langchainrb//lib/langchain/prompt/few_shot_prompt_template.rb#50
  def example_separator; end

  # Returns the value of attribute examples.
  #
  # source://langchainrb//lib/langchain/prompt/few_shot_prompt_template.rb#50
  def examples; end

  # Format the prompt with the inputs.
  #
  # @param kwargs [Hash] Any arguments to be passed to the prompt template.
  # @return [String] A formatted string.
  #
  # source://langchainrb//lib/langchain/prompt/few_shot_prompt_template.rb#90
  def format(**kwargs); end

  # Returns the value of attribute input_variables.
  #
  # source://langchainrb//lib/langchain/prompt/few_shot_prompt_template.rb#50
  def input_variables; end

  # Returns the value of attribute prefix.
  #
  # source://langchainrb//lib/langchain/prompt/few_shot_prompt_template.rb#50
  def prefix; end

  # Returns the key type of prompt as a string.
  #
  # @return [String] the prompt type key
  #
  # source://langchainrb//lib/langchain/prompt/few_shot_prompt_template.rb#104
  def prompt_type; end

  # Returns the value of attribute suffix.
  #
  # source://langchainrb//lib/langchain/prompt/few_shot_prompt_template.rb#50
  def suffix; end

  # source://langchainrb//lib/langchain/prompt/few_shot_prompt_template.rb#108
  def to_h; end
end

# source://langchainrb//lib/langchain/prompt/loading.rb#14
module Langchain::Prompt::Loading
  mixes_in_class_methods ::Langchain::Prompt::Loading::ClassMethods

  class << self
    # @private
    #
    # source://langchainrb//lib/langchain/prompt/loading.rb#15
    def included(base); end
  end
end

# source://langchainrb//lib/langchain/prompt/loading.rb#19
module Langchain::Prompt::Loading::ClassMethods
  # Loads a prompt template with the given configuration.
  #
  # @param config [Hash] A hash containing the configuration for the prompt.
  # @return [FewShotPromptTemplate] The loaded prompt loaded.
  #
  # source://langchainrb//lib/langchain/prompt/loading.rb#63
  def load_few_shot_prompt(config); end

  # Load prompt from file.
  #
  # @param file_path [String, Pathname] The path of the file to read the configuration data from.
  # @raise [ArgumentError] If the file type of the specified file path is not supported.
  # @return [Object] The loaded prompt loaded.
  #
  # source://langchainrb//lib/langchain/prompt/loading.rb#29
  def load_from_path(file_path:); end

  # Loads a prompt template with the given configuration.
  #
  # @param config [Hash] A hash containing the configuration for the prompt.
  # @return [PromptTemplate] The loaded prompt loaded.
  #
  # source://langchainrb//lib/langchain/prompt/loading.rb#51
  def load_prompt(config); end

  private

  # Loads the prompt from the given configuration hash
  #
  # @param config [Hash] the configuration hash to load from
  # @raise [ArgumentError] if the prompt type specified in the config is not supported
  # @return [Object] the loaded prompt
  #
  # source://langchainrb//lib/langchain/prompt/loading.rb#80
  def load_from_config(config); end
end

# = Prompt Templates
#
# Create a prompt with one input variable:
#
#     prompt = Langchain::Prompt::PromptTemplate.new(template: "Tell me a {adjective} joke.", input_variables: ["adjective"])
#     prompt.format(adjective: "funny") # "Tell me a funny joke."
#
# Create a prompt with multiple input variables:
#
#     prompt = Langchain::Prompt::PromptTemplate.new(template: "Tell me a {adjective} joke about {content}.", input_variables: ["adjective", "content"])
#     prompt.format(adjective: "funny", content: "chickens") # "Tell me a funny joke about chickens."
#
# Creating a PromptTemplate using just a prompt and no input_variables:
#
#     prompt = Langchain::Prompt::PromptTemplate.from_template("Tell me a {adjective} joke about {content}.")
#     prompt.input_variables # ["adjective", "content"]
#     prompt.format(adjective: "funny", content: "chickens") # "Tell me a funny joke about chickens."
#
# Save prompt template to JSON file:
#
#     prompt.save(file_path: "spec/fixtures/prompt/prompt_template.json")
#
# Loading a new prompt template using a JSON file:
#
#     prompt = Langchain::Prompt.load_from_path(file_path: "spec/fixtures/prompt/prompt_template.json")
#     prompt.input_variables # ["adjective", "content"]
#
# Loading a new prompt template using a YAML file:
#     prompt = Langchain::Prompt.load_from_path(file_path: "spec/fixtures/prompt/prompt_template.yaml")
#     prompt.input_variables #=> ["adjective", "content"]
#
# source://langchainrb//lib/langchain/prompt/prompt_template.rb#35
class Langchain::Prompt::PromptTemplate < ::Langchain::Prompt::Base
  # Initializes a new instance of the class.
  #
  # @param template [String] The prompt template.
  # @param input_variables [Array<String>] A list of the names of the variables the prompt template expects.
  # @param validate_template [Boolean] Whether or not to try validating the template.
  # @return [PromptTemplate] a new instance of PromptTemplate
  #
  # source://langchainrb//lib/langchain/prompt/prompt_template.rb#45
  def initialize(template:, input_variables:, validate_template: T.unsafe(nil)); end

  # Format the prompt with the inputs. Double <code>{{}}</code> replaced with single <code>{}</code> to adhere to f-string spec.
  #
  # @param kwargs [Hash] Any arguments to be passed to the prompt template.
  # @return [String] A formatted string.
  #
  # source://langchainrb//lib/langchain/prompt/prompt_template.rb#59
  def format(**kwargs); end

  # Returns the value of attribute input_variables.
  #
  # source://langchainrb//lib/langchain/prompt/prompt_template.rb#36
  def input_variables; end

  # Returns the key type of prompt as a string.
  #
  # @return [String] the prompt type key
  #
  # source://langchainrb//lib/langchain/prompt/prompt_template.rb#70
  def prompt_type; end

  # Returns the value of attribute template.
  #
  # source://langchainrb//lib/langchain/prompt/prompt_template.rb#36
  def template; end

  # source://langchainrb//lib/langchain/prompt/prompt_template.rb#74
  def to_h; end

  # Returns the value of attribute validate_template.
  #
  # source://langchainrb//lib/langchain/prompt/prompt_template.rb#36
  def validate_template; end

  class << self
    # Creates a new instance of the class using the given template.
    #
    # @param template [String] The template to use
    # @return [Object] A new instance of the class
    #
    # source://langchainrb//lib/langchain/prompt/prompt_template.rb#89
    def from_template(template); end
  end
end

# source://langchainrb//lib/langchain/prompt/loading.rb#9
Langchain::Prompt::TYPE_TO_LOADER = T.let(T.unsafe(nil), Hash)

# Langchain::Thread keeps track of messages in a conversation.
# TODO: Add functionality to persist to the thread to disk, DB, storage, etc.
#
# source://langchainrb//lib/langchain/assistants/thread.rb#6
class Langchain::Thread
  # @param messages [Array<Langchain::Message>]
  # @raise [ArgumentError]
  # @return [Thread] a new instance of Thread
  #
  # source://langchainrb//lib/langchain/assistants/thread.rb#10
  def initialize(messages: T.unsafe(nil)); end

  # Add a message to the thread
  #
  # @param message [Langchain::Message] The message to add
  # @raise [ArgumentError]
  # @return [Array<Langchain::Message>] The updated messages array
  #
  # source://langchainrb//lib/langchain/assistants/thread.rb#27
  def add_message(message); end

  # Convert the thread to an LLM APIs-compatible array of hashes
  #
  # @return [Array<Hash>] The thread as an OpenAI API-compatible array of hashes
  #
  # source://langchainrb//lib/langchain/assistants/thread.rb#19
  def array_of_message_hashes; end

  # Returns the value of attribute messages.
  #
  # source://langchainrb//lib/langchain/assistants/thread.rb#7
  def messages; end

  # Sets the attribute messages
  #
  # @param value the value to set the attribute messages to.
  #
  # source://langchainrb//lib/langchain/assistants/thread.rb#7
  def messages=(_arg0); end
end

# source://langchainrb//lib/langchain/tool/base.rb#3
module Langchain::Tool; end

# = Tools
#
# Tools are used by Agents to perform specific tasks. A 'Tool' is a collection of functions ("methods").
#
# == Available Tools
#
# - {Langchain::Tool::Calculator}: calculate the result of a math expression
# - {Langchain::Tool::Database}: executes SQL queries
# - {Langchain::Tool::FileSystem}: interacts with the file system
# - {Langchain::Tool::GoogleSearch}: search on Google (via SerpAPI)
# - {Langchain::Tool::RubyCodeInterpreter}: runs ruby code
# - {Langchain::Tool::Weather}: gets current weather data
# - {Langchain::Tool::Wikipedia}: search on Wikipedia
#
# == Usage
#
# 1. Pick the tools you'd like to pass to an Agent and install the gems listed under **Gem Requirements**
#
#     # For example to use the Calculator, GoogleSearch, and Wikipedia:
#     gem install eqn
#     gem install google_search_results
#     gem install wikipedia-client
#
# 2. Set the environment variables listed under **ENV Requirements**
#
#     export SERPAPI_API_KEY=paste-your-serpapi-api-key-here
#
# 3. Pass the tools when Agent is instantiated.
#
#     agent = Langchain::Assistant.new(
#       llm: Langchain::LLM::OpenAI.new(api_key: "YOUR_API_KEY"), # or other LLM that supports function calling (coming soon)
#       thread: Langchain::Thread.new,
#       tools: [
#         Langchain::Tool::GoogleSearch.new(api_key: "YOUR_API_KEY"),
#         Langchain::Tool::Calculator.new,
#         Langchain::Tool::Wikipedia.new
#       ]
#     )
#
# == Adding Tools
#
# 1. Create a new folder in lib/langchain/tool/your_tool_name/
# 2. Inside of this folder create a file with a class YourToolName that inherits from {Langchain::Tool::Base}
# 3. Add `NAME=` and `ANNOTATIONS_PATH=` constants in your Tool class
# 4. Implement various public methods in your tool class
# 5. Create a sidecar .json file in the same directory as your tool file annotating the methods in the Open API format
# 6. Add your tool to the {file:README.md}
#
# source://langchainrb//lib/langchain/tool/base.rb#51
class Langchain::Tool::Base
  include ::Langchain::DependencyHelper

  # Return tool's method annotations as JSON
  #
  # @return [Hash] Tool's method annotations
  #
  # source://langchainrb//lib/langchain/tool/base.rb#99
  def method_annotations; end

  # Returns the NAME constant of the tool
  #
  # @return [String] tool name
  #
  # source://langchainrb//lib/langchain/tool/base.rb#57
  def name; end

  # Returns the tool as a list of Anthropic formatted functions
  #
  # @return [Array<Hash>] List of hashes representing the tool as Anthropic formatted functions
  #
  # source://langchainrb//lib/langchain/tool/base.rb#77
  def to_anthropic_tools; end

  # Returns the tool as a list of Google Gemini formatted functions
  #
  # @return [Array<Hash>] List of hashes representing the tool as Google Gemini formatted functions
  #
  # source://langchainrb//lib/langchain/tool/base.rb#89
  def to_google_gemini_tools; end

  # Returns the tool as a list of OpenAI formatted functions
  #
  # @return [Array<Hash>] List of hashes representing the tool as OpenAI formatted functions
  #
  # source://langchainrb//lib/langchain/tool/base.rb#70
  def to_openai_tools; end

  class << self
    # source://langchainrb//lib/langchain/tool/base.rb#61
    def logger_options; end
  end
end

# source://langchainrb//lib/langchain/tool/calculator/calculator.rb#4
class Langchain::Tool::Calculator < ::Langchain::Tool::Base
  # @return [Calculator] a new instance of Calculator
  #
  # source://langchainrb//lib/langchain/tool/calculator/calculator.rb#18
  def initialize; end

  # Evaluates a pure math expression or if equation contains non-math characters (e.g.: "12F in Celsius") then it uses the google search calculator to evaluate the expression
  #
  # @param input [String] math expression
  # @return [String] Answer
  #
  # source://langchainrb//lib/langchain/tool/calculator/calculator.rb#26
  def execute(input:); end
end

# source://langchainrb//lib/langchain/tool/calculator/calculator.rb#16
Langchain::Tool::Calculator::ANNOTATIONS_PATH = T.let(T.unsafe(nil), String)

# A calculator tool that falls back to the Google calculator widget
#
# Gem requirements:
#     gem "eqn", "~> 1.6.5"
#     gem "google_search_results", "~> 2.0.0"
#
# Usage:
#     calculator = Langchain::Tool::Calculator.new
#
# source://langchainrb//lib/langchain/tool/calculator/calculator.rb#15
Langchain::Tool::Calculator::NAME = T.let(T.unsafe(nil), String)

# source://langchainrb//lib/langchain/tool/database/database.rb#2
class Langchain::Tool::Database < ::Langchain::Tool::Base
  # Establish a database connection
  #
  # @param connection_string [String] Database connection info, e.g. 'postgres://user:password@localhost:5432/db_name'
  # @param tables [Array<Symbol>] The tables to use. Will use all if empty.
  # @param except_tables [Array<Symbol>] The tables to exclude. Will exclude none if empty.
  # @raise [StandardError]
  # @return [Database] Database object
  #
  # source://langchainrb//lib/langchain/tool/database/database.rb#23
  def initialize(connection_string:, tables: T.unsafe(nil), exclude_tables: T.unsafe(nil)); end

  # Returns the value of attribute db.
  #
  # source://langchainrb//lib/langchain/tool/database/database.rb#15
  def db; end

  # source://langchainrb//lib/langchain/tool/database/database.rb#62
  def describe_table(table, schema); end

  # Database Tool: Returns the schema for a list of tables
  #
  # @param tables [String] The tables to describe.
  # @return [String] Database schema for the tables
  #
  # source://langchainrb//lib/langchain/tool/database/database.rb#42
  def describe_tables(tables:); end

  # Database Tool: Returns the database schema
  #
  # @return [String] Database schema
  #
  # source://langchainrb//lib/langchain/tool/database/database.rb#53
  def dump_schema; end

  # Returns the value of attribute excluded_tables.
  #
  # source://langchainrb//lib/langchain/tool/database/database.rb#15
  def excluded_tables; end

  # Database Tool: Executes a SQL query and returns the results
  #
  # @param input [String] SQL query to be executed
  # @return [Array] Results from the SQL query
  #
  # source://langchainrb//lib/langchain/tool/database/database.rb#91
  def execute(input:); end

  # Database Tool: Returns a list of tables in the database
  #
  # source://langchainrb//lib/langchain/tool/database/database.rb#34
  def list_tables; end

  # Returns the value of attribute requested_tables.
  #
  # source://langchainrb//lib/langchain/tool/database/database.rb#15
  def requested_tables; end
end

# source://langchainrb//lib/langchain/tool/database/database.rb#13
Langchain::Tool::Database::ANNOTATIONS_PATH = T.let(T.unsafe(nil), String)

# Connects to a database, executes SQL queries, and outputs DB schema for Agents to use
#
# Gem requirements:
#     gem "sequel", "~> 5.68.0"
#
# Usage:
#     database = Langchain::Tool::Database.new(connection_string: "postgres://user:password@localhost:5432/db_name")
#
# source://langchainrb//lib/langchain/tool/database/database.rb#12
Langchain::Tool::Database::NAME = T.let(T.unsafe(nil), String)

# source://langchainrb//lib/langchain/tool/file_system/file_system.rb#4
class Langchain::Tool::FileSystem < ::Langchain::Tool::Base
  # source://langchainrb//lib/langchain/tool/file_system/file_system.rb#14
  def list_directory(directory_path:); end

  # source://langchainrb//lib/langchain/tool/file_system/file_system.rb#20
  def read_file(file_path:); end

  # source://langchainrb//lib/langchain/tool/file_system/file_system.rb#26
  def write_to_file(file_path:, content:); end
end

# source://langchainrb//lib/langchain/tool/file_system/file_system.rb#12
Langchain::Tool::FileSystem::ANNOTATIONS_PATH = T.let(T.unsafe(nil), String)

# A tool that wraps the Ruby file system classes.
#
# Usage:
#    file_system = Langchain::Tool::FileSystem.new
#
# source://langchainrb//lib/langchain/tool/file_system/file_system.rb#11
Langchain::Tool::FileSystem::NAME = T.let(T.unsafe(nil), String)

# source://langchainrb//lib/langchain/tool/google_search/google_search.rb#4
class Langchain::Tool::GoogleSearch < ::Langchain::Tool::Base
  # Initializes the Google Search tool
  #
  # @param api_key [String] Search API key
  # @return [Langchain::Tool::GoogleSearch] Google search tool
  #
  # source://langchainrb//lib/langchain/tool/google_search/google_search.rb#26
  def initialize(api_key:); end

  # Returns the value of attribute api_key.
  #
  # source://langchainrb//lib/langchain/tool/google_search/google_search.rb#18
  def api_key; end

  # Executes Google Search and returns the result
  #
  # @param input [String] search query
  # @return [String] Answer
  #
  # source://langchainrb//lib/langchain/tool/google_search/google_search.rb#46
  def execute(input:); end

  # Executes Google Search and returns hash_results JSON
  #
  # @param input [String] search query
  # @return [Hash] hash_results JSON
  #
  # source://langchainrb//lib/langchain/tool/google_search/google_search.rb#129
  def execute_search(input:); end

  class << self
    # Executes Google Search and returns hash_results JSON
    #
    # @param input [String] search query
    # @return [Hash] hash_results JSON
    #
    # source://langchainrb//lib/langchain/tool/google_search/google_search.rb#38
    def execute_search(input:); end
  end
end

# source://langchainrb//lib/langchain/tool/google_search/google_search.rb#16
Langchain::Tool::GoogleSearch::ANNOTATIONS_PATH = T.let(T.unsafe(nil), String)

# Wrapper around SerpApi's Google Search API
#
# Gem requirements:
#     gem "google_search_results", "~> 2.0.0"
#
# Usage:
#     search = Langchain::Tool::GoogleSearch.new(api_key: "YOUR_API_KEY")
#     search.execute(input: "What is the capital of France?")
#
# source://langchainrb//lib/langchain/tool/google_search/google_search.rb#15
Langchain::Tool::GoogleSearch::NAME = T.let(T.unsafe(nil), String)

# source://langchainrb//lib/langchain/tool/news_retriever/news_retriever.rb#4
class Langchain::Tool::NewsRetriever < ::Langchain::Tool::Base
  # @return [NewsRetriever] a new instance of NewsRetriever
  #
  # source://langchainrb//lib/langchain/tool/news_retriever/news_retriever.rb#15
  def initialize(api_key: T.unsafe(nil)); end

  # Retrieve all news
  #
  # @param q [String] Keywords or phrases to search for in the article title and body.
  # @param search_in [String] The fields to restrict your q search to. The possible options are: title, description, content.
  # @param sources [String] A comma-seperated string of identifiers (maximum 20) for the news sources or blogs you want headlines from. Use the /sources endpoint to locate these programmatically or look at the sources index.
  # @param domains [String] A comma-seperated string of domains (eg bbc.co.uk, techcrunch.com, engadget.com) to restrict the search to.
  # @param exclude_domains [String] A comma-seperated string of domains (eg bbc.co.uk, techcrunch.com, engadget.com) to remove from the results.
  # @param from [String] A date and optional time for the oldest article allowed. This should be in ISO 8601 format.
  # @param to [String] A date and optional time for the newest article allowed. This should be in ISO 8601 format.
  # @param language [String] The 2-letter ISO-639-1 code of the language you want to get headlines for. Possible options: ar, de, en, es, fr, he, it, nl, no, pt, ru, se, ud, zh.
  # @param sort_by [String] The order to sort the articles in. Possible options: relevancy, popularity, publishedAt.
  # @param page_size [Integer] The number of results to return per page. 20 is the API's default, 100 is the maximum. Our default is 5.
  # @param page [Integer] Use this to page through the results.
  # @return [String] JSON response
  #
  # source://langchainrb//lib/langchain/tool/news_retriever/news_retriever.rb#34
  def get_everything(q: T.unsafe(nil), search_in: T.unsafe(nil), sources: T.unsafe(nil), domains: T.unsafe(nil), exclude_domains: T.unsafe(nil), from: T.unsafe(nil), to: T.unsafe(nil), language: T.unsafe(nil), sort_by: T.unsafe(nil), page_size: T.unsafe(nil), page: T.unsafe(nil)); end

  # Retrieve news sources
  #
  # @param category [String] The category you want to get headlines for. Possible options: business, entertainment, general, health, science, sports, technology.
  # @param language [String] The 2-letter ISO-639-1 code of the language you want to get headlines for. Possible options: ar, de, en, es, fr, he, it, nl, no, pt, ru, se, ud, zh.
  # @param country [String] The 2-letter ISO 3166-1 code of the country you want to get headlines for. Possible options: ae, ar, at, au, be, bg, br, ca, ch, cn, co, cu, cz, de, eg, fr, gb, gr, hk, hu, id, ie, il, in, it, jp, kr, lt, lv, ma, mx, my, ng, nl, no, nz, ph, pl, pt, ro, rs, ru, sa, se, sg, si, sk, th, tr, tw, ua, us, ve, za.
  # @return [String] JSON response
  #
  # source://langchainrb//lib/langchain/tool/news_retriever/news_retriever.rb#103
  def get_sources(category: T.unsafe(nil), language: T.unsafe(nil), country: T.unsafe(nil)); end

  # Retrieve top headlines
  #
  # @param country [String] The 2-letter ISO 3166-1 code of the country you want to get headlines for. Possible options: ae, ar, at, au, be, bg, br, ca, ch, cn, co, cu, cz, de, eg, fr, gb, gr, hk, hu, id, ie, il, in, it, jp, kr, lt, lv, ma, mx, my, ng, nl, no, nz, ph, pl, pt, ro, rs, ru, sa, se, sg, si, sk, th, tr, tw, ua, us, ve, za.
  # @param category [String] The category you want to get headlines for. Possible options: business, entertainment, general, health, science, sports, technology.
  # @param sources [String] A comma-seperated string of identifiers for the news sources or blogs you want headlines from. Use the /sources endpoint to locate these programmatically.
  # @param q [String] Keywords or a phrase to search for.
  # @param page_size [Integer] The number of results to return per page. 20 is the API's default, 100 is the maximum. Our default is 5.
  # @param page [Integer] Use this to page through the results.
  # @return [String] JSON response
  #
  # source://langchainrb//lib/langchain/tool/news_retriever/news_retriever.rb#75
  def get_top_headlines(country: T.unsafe(nil), category: T.unsafe(nil), sources: T.unsafe(nil), q: T.unsafe(nil), page_size: T.unsafe(nil), page: T.unsafe(nil)); end

  private

  # source://langchainrb//lib/langchain/tool/news_retriever/news_retriever.rb#120
  def send_request(path:, params:); end
end

# source://langchainrb//lib/langchain/tool/news_retriever/news_retriever.rb#13
Langchain::Tool::NewsRetriever::ANNOTATIONS_PATH = T.let(T.unsafe(nil), String)

# A tool that retrieves latest news from various sources via https://newsapi.org/.
# An API key needs to be obtained from https://newsapi.org/ to use this tool.
#
# Usage:
#    news_retriever = Langchain::Tool::NewsRetriever.new(api_key: ENV["NEWS_API_KEY"])
#
# source://langchainrb//lib/langchain/tool/news_retriever/news_retriever.rb#12
Langchain::Tool::NewsRetriever::NAME = T.let(T.unsafe(nil), String)

# source://langchainrb//lib/langchain/tool/ruby_code_interpreter/ruby_code_interpreter.rb#4
class Langchain::Tool::RubyCodeInterpreter < ::Langchain::Tool::Base
  # @return [RubyCodeInterpreter] a new instance of RubyCodeInterpreter
  #
  # source://langchainrb//lib/langchain/tool/ruby_code_interpreter/ruby_code_interpreter.rb#17
  def initialize(timeout: T.unsafe(nil)); end

  # Executes Ruby code in a sandboxes environment.
  #
  # @param input [String] ruby code expression
  # @return [String] Answer
  #
  # source://langchainrb//lib/langchain/tool/ruby_code_interpreter/ruby_code_interpreter.rb#27
  def execute(input:); end

  # source://langchainrb//lib/langchain/tool/ruby_code_interpreter/ruby_code_interpreter.rb#33
  def safe_eval(code); end
end

# source://langchainrb//lib/langchain/tool/ruby_code_interpreter/ruby_code_interpreter.rb#15
Langchain::Tool::RubyCodeInterpreter::ANNOTATIONS_PATH = T.let(T.unsafe(nil), String)

# A tool that execute Ruby code in a sandboxed environment.
#
# Gem requirements:
#     gem "safe_ruby", "~> 1.0.4"
#
# Usage:
#    interpreter = Langchain::Tool::RubyCodeInterpreter.new
#
# source://langchainrb//lib/langchain/tool/ruby_code_interpreter/ruby_code_interpreter.rb#14
Langchain::Tool::RubyCodeInterpreter::NAME = T.let(T.unsafe(nil), String)

# source://langchainrb//lib/langchain/tool/tavily/tavily.rb#4
class Langchain::Tool::Tavily < ::Langchain::Tool::Base
  # @return [Tavily] a new instance of Tavily
  #
  # source://langchainrb//lib/langchain/tool/tavily/tavily.rb#15
  def initialize(api_key:); end

  # Search for data based on a query.
  #
  # @param query [String] The search query string.
  # @param search_depth [String] The depth of the search. It can be basic or advanced. Default is basic for quick results and advanced for indepth high quality results but longer response time. Advanced calls equals 2 requests.
  # @param include_images [Boolean] Include a list of query related images in the response. Default is False.
  # @param include_answer [Boolean] Include answers in the search results. Default is False.
  # @param include_raw_content [Boolean] Include raw content in the search results. Default is False.
  # @param max_results [Integer] The number of maximum search results to return. Default is 5.
  # @param include_domains [Array<String>] A list of domains to specifically include in the search results. Default is None, which includes all domains.
  # @param exclude_domains [Array<String>] A list of domains to specifically exclude from the search results. Default is None, which doesn't exclude any domains.
  # @return [String] The search results in JSON format.
  #
  # source://langchainrb//lib/langchain/tool/tavily/tavily.rb#31
  def search(query:, search_depth: T.unsafe(nil), include_images: T.unsafe(nil), include_answer: T.unsafe(nil), include_raw_content: T.unsafe(nil), max_results: T.unsafe(nil), include_domains: T.unsafe(nil), exclude_domains: T.unsafe(nil)); end
end

# source://langchainrb//lib/langchain/tool/tavily/tavily.rb#13
Langchain::Tool::Tavily::ANNOTATIONS_PATH = T.let(T.unsafe(nil), String)

# Tavily Search is a robust search API tailored specifically for LLM Agents.
# It seamlessly integrates with diverse data sources to ensure a superior, relevant search experience.
#
# Usage:
#    tavily = Langchain::Tool::Tavily.new(api_key: ENV["TAVILY_API_KEY"])
#
# source://langchainrb//lib/langchain/tool/tavily/tavily.rb#12
Langchain::Tool::Tavily::NAME = T.let(T.unsafe(nil), String)

# source://langchainrb//lib/langchain/tool/vectorsearch/vectorsearch.rb#4
class Langchain::Tool::Vectorsearch < ::Langchain::Tool::Base
  # Initializes the Vectorsearch tool
  #
  # @param vectorsearch [Langchain::Vectorsearch::Base] Vectorsearch instance to use
  # @return [Vectorsearch] a new instance of Vectorsearch
  #
  # source://langchainrb//lib/langchain/tool/vectorsearch/vectorsearch.rb#24
  def initialize(vectorsearch:); end

  # Executes the vector search and returns the results
  #
  # @param query [String] The query to search for
  # @param k [Integer] The number of results to return
  #
  # source://langchainrb//lib/langchain/tool/vectorsearch/vectorsearch.rb#32
  def similarity_search(query:, k: T.unsafe(nil)); end

  # Returns the value of attribute vectorsearch.
  #
  # source://langchainrb//lib/langchain/tool/vectorsearch/vectorsearch.rb#19
  def vectorsearch; end
end

# source://langchainrb//lib/langchain/tool/vectorsearch/vectorsearch.rb#17
Langchain::Tool::Vectorsearch::ANNOTATIONS_PATH = T.let(T.unsafe(nil), String)

# A tool wraps vectorsearch classes
#
# Usage:
#    # Initialize the LLM that will be used to generate embeddings
#    ollama = Langchain::LLM::Ollama.new(url: ENV["OLLAMA_URL"]
#    chroma = Langchain::Vectorsearch::Chroma.new(url: ENV["CHROMA_URL"], index_name: "my_index", llm: ollama)
#
#    # This tool can now be used by the Assistant
#    vectorsearch_tool = Langchain::Tool::Vectorsearch.new(vectorsearch: chroma)
#
# source://langchainrb//lib/langchain/tool/vectorsearch/vectorsearch.rb#16
Langchain::Tool::Vectorsearch::NAME = T.let(T.unsafe(nil), String)

# source://langchainrb//lib/langchain/tool/weather/weather.rb#4
class Langchain::Tool::Weather < ::Langchain::Tool::Base
  # Initializes the Weather tool
  #
  # @param api_key [String] Open Weather API key
  # @return [Langchain::Tool::Weather] Weather tool
  #
  # source://langchainrb//lib/langchain/tool/weather/weather.rb#28
  def initialize(api_key:, units: T.unsafe(nil)); end

  # Returns the value of attribute client.
  #
  # source://langchainrb//lib/langchain/tool/weather/weather.rb#22
  def client; end

  # Returns current weather for a city
  #
  # @param input [String] comma separated city and unit (optional: imperial, metric, or standard)
  # @return [String] Answer
  #
  # source://langchainrb//lib/langchain/tool/weather/weather.rb#44
  def execute(input:); end

  # Returns the value of attribute units.
  #
  # source://langchainrb//lib/langchain/tool/weather/weather.rb#22
  def units; end
end

# source://langchainrb//lib/langchain/tool/weather/weather.rb#20
Langchain::Tool::Weather::ANNOTATIONS_PATH = T.let(T.unsafe(nil), String)

# A weather tool that gets current weather data
#
# Current weather data is free for 1000 calls per day (https://home.openweathermap.org/api_keys)
# Forecast and historical data require registration with credit card, so not supported yet.
#
# Gem requirements:
#     gem "open-weather-ruby-client", "~> 0.3.0"
#     api_key: https://home.openweathermap.org/api_keys
#
# Usage:
#     weather = Langchain::Tool::Weather.new(api_key: ENV["OPEN_WEATHER_API_KEY"])
#     weather.execute(input: "Boston, MA; imperial")
#
# source://langchainrb//lib/langchain/tool/weather/weather.rb#19
Langchain::Tool::Weather::NAME = T.let(T.unsafe(nil), String)

# source://langchainrb//lib/langchain/tool/wikipedia/wikipedia.rb#4
class Langchain::Tool::Wikipedia < ::Langchain::Tool::Base
  # Initializes the Wikipedia tool
  #
  # @return [Wikipedia] a new instance of Wikipedia
  #
  # source://langchainrb//lib/langchain/tool/wikipedia/wikipedia.rb#19
  def initialize; end

  # Executes Wikipedia API search and returns the answer
  #
  # @param input [String] search query
  # @return [String] Answer
  #
  # source://langchainrb//lib/langchain/tool/wikipedia/wikipedia.rb#27
  def execute(input:); end
end

# source://langchainrb//lib/langchain/tool/wikipedia/wikipedia.rb#16
Langchain::Tool::Wikipedia::ANNOTATIONS_PATH = T.let(T.unsafe(nil), String)

# Tool that adds the capability to search using the Wikipedia API
#
# Gem requirements:
#     gem "wikipedia-client", "~> 1.17.0"
#
# Usage:
#     wikipedia = Langchain::Tool::Wikipedia.new
#     wikipedia.execute(input: "The Roman Empire")
#
# source://langchainrb//lib/langchain/tool/wikipedia/wikipedia.rb#15
Langchain::Tool::Wikipedia::NAME = T.let(T.unsafe(nil), String)

# source://langchainrb//lib/langchain/utils/token_length/ai21_validator.rb#4
module Langchain::Utils; end

# source://langchainrb//lib/langchain/utils/cosine_similarity.rb#5
class Langchain::Utils::CosineSimilarity
  # @param vector_a [Array<Float>] First vector
  # @param vector_b [Array<Float>] Second vector
  # @return [CosineSimilarity] a new instance of CosineSimilarity
  #
  # source://langchainrb//lib/langchain/utils/cosine_similarity.rb#10
  def initialize(vector_a, vector_b); end

  # Calculate the cosine similarity between two vectors
  #
  # @return [Float] The cosine similarity between the two vectors
  #
  # source://langchainrb//lib/langchain/utils/cosine_similarity.rb#17
  def calculate_similarity; end

  # Returns the value of attribute vector_a.
  #
  # source://langchainrb//lib/langchain/utils/cosine_similarity.rb#6
  def vector_a; end

  # Returns the value of attribute vector_b.
  #
  # source://langchainrb//lib/langchain/utils/cosine_similarity.rb#6
  def vector_b; end
end

# source://langchainrb//lib/langchain/llm/ai21.rb#0
module Langchain::Utils::TokenLength; end

# This class is meant to validate the length of the text passed in to AI21's API.
# It is used to validate the token length before the API call is made
#
# source://langchainrb//lib/langchain/utils/token_length/ai21_validator.rb#11
class Langchain::Utils::TokenLength::AI21Validator < ::Langchain::Utils::TokenLength::BaseValidator
  class << self
    # source://langchainrb//lib/langchain/utils/token_length/ai21_validator.rb#30
    def completion_token_limit(model_name); end

    # Calculate token length for a given text and model name
    #
    # @param text [String] The text to calculate the token length for
    # @param model_name [String] The model name to validate against
    # @return [Integer] The token length of the text
    #
    # source://langchainrb//lib/langchain/utils/token_length/ai21_validator.rb#25
    def token_length(text, model_name, options = T.unsafe(nil)); end

    # source://langchainrb//lib/langchain/utils/token_length/ai21_validator.rb#35
    def token_length_from_messages(messages, model_name, options); end

    # source://langchainrb//lib/langchain/utils/token_length/ai21_validator.rb#30
    def token_limit(model_name); end
  end
end

# source://langchainrb//lib/langchain/utils/token_length/ai21_validator.rb#12
Langchain::Utils::TokenLength::AI21Validator::TOKEN_LIMITS = T.let(T.unsafe(nil), Hash)

# Calculate the `max_tokens:` parameter to be set by calculating the context length of the text minus the prompt length
#
# @param content [String | Array<String>] The text or array of texts to validate
# @param model_name [String] The model name to validate against
# @raise [TokenLimitExceeded] If the text is too long
# @return [Integer] Whether the text is valid or not
#
# source://langchainrb//lib/langchain/utils/token_length/base_validator.rb#14
class Langchain::Utils::TokenLength::BaseValidator
  class << self
    # source://langchainrb//lib/langchain/utils/token_length/base_validator.rb#36
    def limit_exceeded_exception(limit, length); end

    # source://langchainrb//lib/langchain/utils/token_length/base_validator.rb#15
    def validate_max_tokens!(content, model_name, options = T.unsafe(nil)); end
  end
end

# This class is meant to validate the length of the text passed in to Cohere's API.
# It is used to validate the token length before the API call is made
#
# source://langchainrb//lib/langchain/utils/token_length/cohere_validator.rb#11
class Langchain::Utils::TokenLength::CohereValidator < ::Langchain::Utils::TokenLength::BaseValidator
  class << self
    # source://langchainrb//lib/langchain/utils/token_length/cohere_validator.rb#38
    def completion_token_limit(model_name); end

    # Calculate token length for a given text and model name
    #
    # @param text [String] The text to calculate the token length for
    # @param model_name [String] The model name to validate against
    # @return [Integer] The token length of the text
    #
    # source://langchainrb//lib/langchain/utils/token_length/cohere_validator.rb#33
    def token_length(text, model_name, options = T.unsafe(nil)); end

    # source://langchainrb//lib/langchain/utils/token_length/cohere_validator.rb#43
    def token_length_from_messages(messages, model_name, options); end

    # source://langchainrb//lib/langchain/utils/token_length/cohere_validator.rb#38
    def token_limit(model_name); end
  end
end

# source://langchainrb//lib/langchain/utils/token_length/cohere_validator.rb#12
Langchain::Utils::TokenLength::CohereValidator::TOKEN_LIMITS = T.let(T.unsafe(nil), Hash)

# This class is meant to validate the length of the text passed in to Google Palm's API.
# It is used to validate the token length before the API call is made
#
# source://langchainrb//lib/langchain/utils/token_length/google_palm_validator.rb#10
class Langchain::Utils::TokenLength::GooglePalmValidator < ::Langchain::Utils::TokenLength::BaseValidator
  class << self
    # source://langchainrb//lib/langchain/utils/token_length/google_palm_validator.rb#50
    def completion_token_limit(model_name); end

    # Calculate token length for a given text and model name
    #
    # @option options
    # @param text [String] The text to calculate the token length for
    # @param model_name [String] The model name to validate against
    # @param options [Hash] the options to create a message with
    # @raise [Langchain::LLM::ApiError]
    # @return [Integer] The token length of the text
    #
    # source://langchainrb//lib/langchain/utils/token_length/google_palm_validator.rb#38
    def token_length(text, model_name = T.unsafe(nil), options = T.unsafe(nil)); end

    # source://langchainrb//lib/langchain/utils/token_length/google_palm_validator.rb#46
    def token_length_from_messages(messages, model_name, options = T.unsafe(nil)); end

    # source://langchainrb//lib/langchain/utils/token_length/google_palm_validator.rb#50
    def token_limit(model_name); end
  end
end

# source://langchainrb//lib/langchain/utils/token_length/google_palm_validator.rb#11
Langchain::Utils::TokenLength::GooglePalmValidator::TOKEN_LIMITS = T.let(T.unsafe(nil), Hash)

# This class is meant to validate the length of the text passed in to OpenAI's API.
# It is used to validate the token length before the API call is made
#
# source://langchainrb//lib/langchain/utils/token_length/openai_validator.rb#12
class Langchain::Utils::TokenLength::OpenAIValidator < ::Langchain::Utils::TokenLength::BaseValidator
  class << self
    # source://langchainrb//lib/langchain/utils/token_length/openai_validator.rb#85
    def completion_token_limit(model_name); end

    # Calculate token length for a given text and model name
    #
    # @param text [String] The text to calculate the token length for
    # @param model_name [String] The model name to validate against
    # @return [Integer] The token length of the text
    #
    # source://langchainrb//lib/langchain/utils/token_length/openai_validator.rb#71
    def token_length(text, model_name, options = T.unsafe(nil)); end

    # Copied from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb
    # Return the number of tokens used by a list of messages
    #
    # @param messages [Array<Hash>] The messages to calculate the token length for
    # @param model [String] The model name to validate against
    # @return [Integer] The token length of the messages
    #
    # source://langchainrb//lib/langchain/utils/token_length/openai_validator.rb#102
    def token_length_from_messages(messages, model_name, options = T.unsafe(nil)); end

    # source://langchainrb//lib/langchain/utils/token_length/openai_validator.rb#81
    def token_limit(model_name); end

    # If :max_tokens is passed in, take the lower of it and the calculated max_tokens
    #
    # source://langchainrb//lib/langchain/utils/token_length/openai_validator.rb#90
    def validate_max_tokens!(content, model_name, options = T.unsafe(nil)); end
  end
end

# source://langchainrb//lib/langchain/utils/token_length/openai_validator.rb#13
Langchain::Utils::TokenLength::OpenAIValidator::COMPLETION_TOKEN_LIMITS = T.let(T.unsafe(nil), Hash)

# NOTE: The gpt-4-turbo-preview is an alias that will always point to the latest GPT 4 Turbo preview
#   the future previews may have a different token limit!
#
# source://langchainrb//lib/langchain/utils/token_length/openai_validator.rb#24
Langchain::Utils::TokenLength::OpenAIValidator::TOKEN_LIMITS = T.let(T.unsafe(nil), Hash)

# source://langchainrb//lib/langchain/utils/token_length/token_limit_exceeded.rb#6
class Langchain::Utils::TokenLength::TokenLimitExceeded < ::StandardError
  # @return [TokenLimitExceeded] a new instance of TokenLimitExceeded
  #
  # source://langchainrb//lib/langchain/utils/token_length/token_limit_exceeded.rb#9
  def initialize(message = T.unsafe(nil), token_overflow = T.unsafe(nil)); end

  # Returns the value of attribute token_overflow.
  #
  # source://langchainrb//lib/langchain/utils/token_length/token_limit_exceeded.rb#7
  def token_overflow; end
end

# source://langchainrb//lib/langchain/version.rb#4
Langchain::VERSION = T.let(T.unsafe(nil), String)

# source://langchainrb//lib/langchain/vectorsearch/base.rb#3
module Langchain::Vectorsearch; end

# = Vector Databases
# A vector database a type of database that stores data as high-dimensional vectors, which are mathematical representations of features or attributes. Each vector has a certain number of dimensions, which can range from tens to thousands, depending on the complexity and granularity of the data.
#
# == Available vector databases
#
# - {Langchain::Vectorsearch::Chroma}
# - {Langchain::Vectorsearch::Epsilla}
# - {Langchain::Vectorsearch::Elasticsearch}
# - {Langchain::Vectorsearch::Hnswlib}
# - {Langchain::Vectorsearch::Milvus}
# - {Langchain::Vectorsearch::Pgvector}
# - {Langchain::Vectorsearch::Pinecone}
# - {Langchain::Vectorsearch::Qdrant}
# - {Langchain::Vectorsearch::Weaviate}
#
# == Usage
#
# 1. Pick a vector database from list.
# 2. Review its documentation to install the required gems, and create an account, get an API key, etc
# 3. Instantiate the vector database class:
#
#     weaviate = Langchain::Vectorsearch::Weaviate.new(
#       url:         ENV["WEAVIATE_URL"],
#       api_key:     ENV["WEAVIATE_API_KEY"],
#       index_name:  "Documents",
#       llm:         Langchain::LLM::OpenAI.new(api_key:)
#     )
#
#     # You can instantiate other supported vector databases the same way:
#     epsilla  = Langchain::Vectorsearch::Epsilla.new(...)
#     milvus   = Langchain::Vectorsearch::Milvus.new(...)
#     qdrant   = Langchain::Vectorsearch::Qdrant.new(...)
#     pinecone = Langchain::Vectorsearch::Pinecone.new(...)
#     chroma   = Langchain::Vectorsearch::Chroma.new(...)
#     pgvector = Langchain::Vectorsearch::Pgvector.new(...)
#
# == Schema Creation
#
# `create_default_schema()` creates default schema in your vector database.
#
#     search.create_default_schema
#
# (We plan on offering customizable schema creation shortly)
#
# == Adding Data
#
# You can add data with:
# 1. `add_data(path:, paths:)` to add any kind of data type
#
#     my_pdf = Langchain.root.join("path/to/my.pdf")
#     my_text = Langchain.root.join("path/to/my.txt")
#     my_docx = Langchain.root.join("path/to/my.docx")
#     my_csv = Langchain.root.join("path/to/my.csv")
#
#     search.add_data(paths: [my_pdf, my_text, my_docx, my_csv])
#
# 2. `add_texts(texts:)` to only add textual data
#
#     search.add_texts(
#       texts: [
#         "Lorem Ipsum is simply dummy text of the printing and typesetting industry.",
#         "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s"
#       ]
#     )
#
# == Retrieving Data
#
# `similarity_search_by_vector(embedding:, k:)` searches the vector database for the closest `k` number of embeddings.
#
#    search.similarity_search_by_vector(
#      embedding: ...,
#      k: # number of results to be retrieved
#    )
#
# `vector_store.similarity_search(query:, k:)` generates an embedding for the query and searches the vector database for the closest `k` number of embeddings.
#
# search.similarity_search_by_vector(
#   embedding: ...,
#   k: # number of results to be retrieved
# )
#
# `ask(question:)` generates an embedding for the passed-in question, searches the vector database for closest embeddings and then passes these as context to the LLM to generate an answer to the question.
#
#     search.ask(question: "What is lorem ipsum?")
#
# source://langchainrb//lib/langchain/vectorsearch/base.rb#89
class Langchain::Vectorsearch::Base
  include ::Langchain::DependencyHelper
  extend ::Forwardable

  # @param llm [Object] The LLM client to use
  # @return [Base] a new instance of Base
  #
  # source://langchainrb//lib/langchain/vectorsearch/base.rb#98
  def initialize(llm:); end

  # @raise [ArgumentError]
  #
  # source://langchainrb//lib/langchain/vectorsearch/base.rb#183
  def add_data(paths:, options: T.unsafe(nil), chunker: T.unsafe(nil)); end

  # Method supported by Vectorsearch DB to add a list of texts to the index
  #
  # @raise [NotImplementedError]
  #
  # source://langchainrb//lib/langchain/vectorsearch/base.rb#118
  def add_texts(*_arg0, **_arg1, &_arg2); end

  # Method supported by Vectorsearch DB to answer a question given a context (data) pulled from your Vectorsearch DB.
  #
  # @raise [NotImplementedError]
  #
  # source://langchainrb//lib/langchain/vectorsearch/base.rb#155
  def ask(*_arg0, **_arg1, &_arg2); end

  # Returns the value of attribute client.
  #
  # source://langchainrb//lib/langchain/vectorsearch/base.rb#93
  def client; end

  # Method supported by Vectorsearch DB to create a default schema
  #
  # @raise [NotImplementedError]
  #
  # source://langchainrb//lib/langchain/vectorsearch/base.rb#108
  def create_default_schema; end

  # Method supported by Vectorsearch DB to delete the default schema
  #
  # @raise [NotImplementedError]
  #
  # source://langchainrb//lib/langchain/vectorsearch/base.rb#113
  def destroy_default_schema; end

  # HyDE-style prompt
  #
  # @param User's [String] question
  # @return [String] Prompt
  #
  # source://langchainrb//lib/langchain/vectorsearch/base.rb#163
  def generate_hyde_prompt(question:); end

  # Retrieval Augmented Generation (RAG)
  #
  # @param question [String] User's question
  # @param context [String] The context to synthesize the answer from
  # @return [String] Prompt
  #
  # source://langchainrb//lib/langchain/vectorsearch/base.rb#176
  def generate_rag_prompt(question:, context:); end

  # Method supported by Vectorsearch DB to retrieve a default schema
  #
  # @raise [NotImplementedError]
  #
  # source://langchainrb//lib/langchain/vectorsearch/base.rb#103
  def get_default_schema; end

  # Returns the value of attribute index_name.
  #
  # source://langchainrb//lib/langchain/vectorsearch/base.rb#93
  def index_name; end

  # Returns the value of attribute llm.
  #
  # source://langchainrb//lib/langchain/vectorsearch/base.rb#93
  def llm; end

  # Method supported by Vectorsearch DB to delete a list of texts from the index
  #
  # @raise [NotImplementedError]
  #
  # source://langchainrb//lib/langchain/vectorsearch/base.rb#128
  def remove_texts(*_arg0, **_arg1, &_arg2); end

  # Method supported by Vectorsearch DB to search for similar texts in the index
  #
  # @raise [NotImplementedError]
  #
  # source://langchainrb//lib/langchain/vectorsearch/base.rb#133
  def similarity_search(*_arg0, **_arg1, &_arg2); end

  # Method supported by Vectorsearch DB to search for similar texts in the index by the passed in vector.
  # You must generate your own vector using the same LLM that generated the embeddings stored in the Vectorsearch DB.
  #
  # @raise [NotImplementedError]
  #
  # source://langchainrb//lib/langchain/vectorsearch/base.rb#150
  def similarity_search_by_vector(*_arg0, **_arg1, &_arg2); end

  # Paper: https://arxiv.org/abs/2212.10496
  # Hypothetical Document Embeddings (HyDE)-augmented similarity search
  #
  # @param query [String] The query to search for
  # @param k [Integer] The number of results to return
  # @return [String] Response
  #
  # source://langchainrb//lib/langchain/vectorsearch/base.rb#143
  def similarity_search_with_hyde(query:, k: T.unsafe(nil)); end

  # Method supported by Vectorsearch DB to update a list of texts to the index
  #
  # @raise [NotImplementedError]
  #
  # source://langchainrb//lib/langchain/vectorsearch/base.rb#123
  def update_texts(*_arg0, **_arg1, &_arg2); end

  class << self
    # source://langchainrb//lib/langchain/vectorsearch/base.rb#198
    def logger_options; end
  end
end

# source://langchainrb//lib/langchain/vectorsearch/base.rb#95
Langchain::Vectorsearch::Base::DEFAULT_METRIC = T.let(T.unsafe(nil), String)

# source://langchainrb//lib/langchain/vectorsearch/chroma.rb#4
class Langchain::Vectorsearch::Chroma < ::Langchain::Vectorsearch::Base
  # Initialize the Chroma client
  #
  # @param url [String] The URL of the Chroma server
  # @param index_name [String] The name of the index to use
  # @param llm [Object] The LLM client to use
  # @return [Chroma] a new instance of Chroma
  #
  # source://langchainrb//lib/langchain/vectorsearch/chroma.rb#19
  def initialize(url:, index_name:, llm:); end

  # Add a list of texts to the index
  #
  # @param texts [Array<String>] The list of texts to add
  # @param ids [Array<String>] The list of ids to use for the texts (optional)
  # @param metadatas [Array<Hash>] The list of metadata to use for the texts (optional)
  # @return [Hash] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/chroma.rb#36
  def add_texts(texts:, ids: T.unsafe(nil), metadatas: T.unsafe(nil)); end

  # Ask a question and return the answer
  #
  # @param question [String] The question to ask
  # @param k [Integer] The number of results to have in context
  # @return [String] The answer to the question
  # @yield [String] Stream responses back one String at a time
  #
  # source://langchainrb//lib/langchain/vectorsearch/chroma.rb#125
  def ask(question:, k: T.unsafe(nil), &block); end

  # Create the collection with the default schema
  #
  # @return [::Chroma::Resources::Collection] Created collection
  #
  # source://langchainrb//lib/langchain/vectorsearch/chroma.rb#72
  def create_default_schema; end

  # Delete the default schema
  #
  # @return [bool] Success or failure
  #
  # source://langchainrb//lib/langchain/vectorsearch/chroma.rb#84
  def destroy_default_schema; end

  # Get the default schema
  #
  # @return [::Chroma::Resources::Collection] Default schema
  #
  # source://langchainrb//lib/langchain/vectorsearch/chroma.rb#78
  def get_default_schema; end

  # Remove a list of texts from the index
  #
  # @param ids [Array<String>] The list of ids to remove
  # @return [Hash] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/chroma.rb#66
  def remove_texts(ids:); end

  # Search for similar texts
  #
  # @param query [String] The text to search for
  # @param k [Integer] The number of results to return
  # @return [Chroma::Resources::Embedding] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/chroma.rb#92
  def similarity_search(query:, k: T.unsafe(nil)); end

  # Search for similar texts by embedding
  #
  # @param embedding [Array<Float>] The embedding to search for
  # @param k [Integer] The number of results to return
  # @return [Chroma::Resources::Embedding] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/chroma.rb#108
  def similarity_search_by_vector(embedding:, k: T.unsafe(nil)); end

  # source://langchainrb//lib/langchain/vectorsearch/chroma.rb#50
  def update_texts(texts:, ids:, metadatas: T.unsafe(nil)); end

  private

  # @return [Chroma::Resources::Collection] The collection
  #
  # source://langchainrb//lib/langchain/vectorsearch/chroma.rb#146
  def collection; end
end

# source://langchainrb//lib/langchain/vectorsearch/elasticsearch.rb#4
class Langchain::Vectorsearch::Elasticsearch < ::Langchain::Vectorsearch::Base
  # @return [Elasticsearch] a new instance of Elasticsearch
  #
  # source://langchainrb//lib/langchain/vectorsearch/elasticsearch.rb#34
  def initialize(url:, index_name:, llm:, api_key: T.unsafe(nil), es_options: T.unsafe(nil)); end

  # Add a list of texts to the index
  #
  # @param texts [Array<String>] The list of texts to add
  # @return [Elasticsearch::Response] from the Elasticsearch server
  #
  # source://langchainrb//lib/langchain/vectorsearch/elasticsearch.rb#52
  def add_texts(texts: T.unsafe(nil)); end

  # Ask a question and return the answer
  #
  # @param question [String] The question to ask
  # @param k [Integer] The number of results to have in context
  # @return [String] The answer to the question
  # @yield [String] Stream responses back one String at a time
  #
  # source://langchainrb//lib/langchain/vectorsearch/elasticsearch.rb#146
  def ask(question:, k: T.unsafe(nil), &block); end

  # Create the index with the default schema
  #
  # @return [Elasticsearch::Response] Index creation
  #
  # source://langchainrb//lib/langchain/vectorsearch/elasticsearch.rb#91
  def create_default_schema; end

  # source://langchainrb//lib/langchain/vectorsearch/elasticsearch.rb#127
  def default_query(query_vector); end

  # source://langchainrb//lib/langchain/vectorsearch/elasticsearch.rb#114
  def default_schema; end

  # source://langchainrb//lib/langchain/vectorsearch/elasticsearch.rb#106
  def default_vector_settings; end

  # Deletes the default schema
  #
  # @return [Elasticsearch::Response] Index deletion
  #
  # source://langchainrb//lib/langchain/vectorsearch/elasticsearch.rb#100
  def delete_default_schema; end

  # Wrapper around Elasticsearch vector search capabilities.
  #
  # Setting up Elasticsearch:
  # 1. Get Elasticsearch up and running with Docker: https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html
  # 2. Copy the HTTP CA certificate SHA-256 fingerprint and set the ELASTICSEARCH_CA_FINGERPRINT environment variable
  # 3. Set the ELASTICSEARCH_URL environment variable
  #
  # Gem requirements:
  #     gem "elasticsearch", "~> 8.0.0"
  #
  # Usage:
  #     llm = Langchain::LLM::OpenAI.new(api_key: ENV["OPENAI_API_KEY"])
  #     es = Langchain::Vectorsearch::Elasticsearch.new(
  #       url: ENV["ELASTICSEARCH_URL"],
  #       index_name: "docs",
  #       llm: llm,
  #       es_options: {
  #         transport_options: {ssl: {verify: false}},
  #         ca_fingerprint: ENV["ELASTICSEARCH_CA_FINGERPRINT"]
  #       }
  #     )
  #
  #     es.create_default_schema
  #     es.add_texts(texts: ["..."])
  #     es.similarity_search(text: "...")
  #
  # source://langchainrb//lib/langchain/vectorsearch/elasticsearch.rb#32
  def es_client; end

  # Wrapper around Elasticsearch vector search capabilities.
  #
  # Setting up Elasticsearch:
  # 1. Get Elasticsearch up and running with Docker: https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html
  # 2. Copy the HTTP CA certificate SHA-256 fingerprint and set the ELASTICSEARCH_CA_FINGERPRINT environment variable
  # 3. Set the ELASTICSEARCH_URL environment variable
  #
  # Gem requirements:
  #     gem "elasticsearch", "~> 8.0.0"
  #
  # Usage:
  #     llm = Langchain::LLM::OpenAI.new(api_key: ENV["OPENAI_API_KEY"])
  #     es = Langchain::Vectorsearch::Elasticsearch.new(
  #       url: ENV["ELASTICSEARCH_URL"],
  #       index_name: "docs",
  #       llm: llm,
  #       es_options: {
  #         transport_options: {ssl: {verify: false}},
  #         ca_fingerprint: ENV["ELASTICSEARCH_CA_FINGERPRINT"]
  #       }
  #     )
  #
  #     es.create_default_schema
  #     es.add_texts(texts: ["..."])
  #     es.similarity_search(text: "...")
  #
  # source://langchainrb//lib/langchain/vectorsearch/elasticsearch.rb#32
  def es_client=(_arg0); end

  # Wrapper around Elasticsearch vector search capabilities.
  #
  # Setting up Elasticsearch:
  # 1. Get Elasticsearch up and running with Docker: https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html
  # 2. Copy the HTTP CA certificate SHA-256 fingerprint and set the ELASTICSEARCH_CA_FINGERPRINT environment variable
  # 3. Set the ELASTICSEARCH_URL environment variable
  #
  # Gem requirements:
  #     gem "elasticsearch", "~> 8.0.0"
  #
  # Usage:
  #     llm = Langchain::LLM::OpenAI.new(api_key: ENV["OPENAI_API_KEY"])
  #     es = Langchain::Vectorsearch::Elasticsearch.new(
  #       url: ENV["ELASTICSEARCH_URL"],
  #       index_name: "docs",
  #       llm: llm,
  #       es_options: {
  #         transport_options: {ssl: {verify: false}},
  #         ca_fingerprint: ENV["ELASTICSEARCH_CA_FINGERPRINT"]
  #       }
  #     )
  #
  #     es.create_default_schema
  #     es.add_texts(texts: ["..."])
  #     es.similarity_search(text: "...")
  #
  # source://langchainrb//lib/langchain/vectorsearch/elasticsearch.rb#32
  def index_name; end

  # Wrapper around Elasticsearch vector search capabilities.
  #
  # Setting up Elasticsearch:
  # 1. Get Elasticsearch up and running with Docker: https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html
  # 2. Copy the HTTP CA certificate SHA-256 fingerprint and set the ELASTICSEARCH_CA_FINGERPRINT environment variable
  # 3. Set the ELASTICSEARCH_URL environment variable
  #
  # Gem requirements:
  #     gem "elasticsearch", "~> 8.0.0"
  #
  # Usage:
  #     llm = Langchain::LLM::OpenAI.new(api_key: ENV["OPENAI_API_KEY"])
  #     es = Langchain::Vectorsearch::Elasticsearch.new(
  #       url: ENV["ELASTICSEARCH_URL"],
  #       index_name: "docs",
  #       llm: llm,
  #       es_options: {
  #         transport_options: {ssl: {verify: false}},
  #         ca_fingerprint: ENV["ELASTICSEARCH_CA_FINGERPRINT"]
  #       }
  #     )
  #
  #     es.create_default_schema
  #     es.add_texts(texts: ["..."])
  #     es.similarity_search(text: "...")
  #
  # source://langchainrb//lib/langchain/vectorsearch/elasticsearch.rb#32
  def index_name=(_arg0); end

  # Wrapper around Elasticsearch vector search capabilities.
  #
  # Setting up Elasticsearch:
  # 1. Get Elasticsearch up and running with Docker: https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html
  # 2. Copy the HTTP CA certificate SHA-256 fingerprint and set the ELASTICSEARCH_CA_FINGERPRINT environment variable
  # 3. Set the ELASTICSEARCH_URL environment variable
  #
  # Gem requirements:
  #     gem "elasticsearch", "~> 8.0.0"
  #
  # Usage:
  #     llm = Langchain::LLM::OpenAI.new(api_key: ENV["OPENAI_API_KEY"])
  #     es = Langchain::Vectorsearch::Elasticsearch.new(
  #       url: ENV["ELASTICSEARCH_URL"],
  #       index_name: "docs",
  #       llm: llm,
  #       es_options: {
  #         transport_options: {ssl: {verify: false}},
  #         ca_fingerprint: ENV["ELASTICSEARCH_CA_FINGERPRINT"]
  #       }
  #     )
  #
  #     es.create_default_schema
  #     es.add_texts(texts: ["..."])
  #     es.similarity_search(text: "...")
  #
  # source://langchainrb//lib/langchain/vectorsearch/elasticsearch.rb#32
  def options; end

  # Wrapper around Elasticsearch vector search capabilities.
  #
  # Setting up Elasticsearch:
  # 1. Get Elasticsearch up and running with Docker: https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html
  # 2. Copy the HTTP CA certificate SHA-256 fingerprint and set the ELASTICSEARCH_CA_FINGERPRINT environment variable
  # 3. Set the ELASTICSEARCH_URL environment variable
  #
  # Gem requirements:
  #     gem "elasticsearch", "~> 8.0.0"
  #
  # Usage:
  #     llm = Langchain::LLM::OpenAI.new(api_key: ENV["OPENAI_API_KEY"])
  #     es = Langchain::Vectorsearch::Elasticsearch.new(
  #       url: ENV["ELASTICSEARCH_URL"],
  #       index_name: "docs",
  #       llm: llm,
  #       es_options: {
  #         transport_options: {ssl: {verify: false}},
  #         ca_fingerprint: ENV["ELASTICSEARCH_CA_FINGERPRINT"]
  #       }
  #     )
  #
  #     es.create_default_schema
  #     es.add_texts(texts: ["..."])
  #     es.similarity_search(text: "...")
  #
  # source://langchainrb//lib/langchain/vectorsearch/elasticsearch.rb#32
  def options=(_arg0); end

  # Remove a list of texts from the index
  #
  # @param ids [Array<Integer>] The list of ids to delete
  # @return [Elasticsearch::Response] from the Elasticsearch server
  #
  # source://langchainrb//lib/langchain/vectorsearch/elasticsearch.rb#81
  def remove_texts(ids: T.unsafe(nil)); end

  # Search for similar texts
  #
  # @param text [String] The text to search for
  # @param k [Integer] The number of results to return
  # @param query [Hash] Elasticsearch query that needs to be used while searching (Optional)
  # @return [Elasticsearch::Response] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/elasticsearch.rb#167
  def similarity_search(text: T.unsafe(nil), k: T.unsafe(nil), query: T.unsafe(nil)); end

  # Search for similar texts by embedding
  #
  # @param embedding [Array<Float>] The embedding to search for
  # @param k [Integer] The number of results to return
  # @param query [Hash] Elasticsearch query that needs to be used while searching (Optional)
  # @return [Elasticsearch::Response] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/elasticsearch.rb#186
  def similarity_search_by_vector(embedding: T.unsafe(nil), k: T.unsafe(nil), query: T.unsafe(nil)); end

  # Add a list of texts to the index
  #
  # @param texts [Array<String>] The list of texts to update
  # @param texts [Array<Integer>] The list of texts to update
  # @return [Elasticsearch::Response] from the Elasticsearch server
  #
  # source://langchainrb//lib/langchain/vectorsearch/elasticsearch.rb#67
  def update_texts(texts: T.unsafe(nil), ids: T.unsafe(nil)); end

  # source://langchainrb//lib/langchain/vectorsearch/elasticsearch.rb#110
  def vector_settings; end
end

# source://langchainrb//lib/langchain/vectorsearch/epsilla.rb#9
class Langchain::Vectorsearch::Epsilla < ::Langchain::Vectorsearch::Base
  # Wrapper around Epsilla client library
  #
  # Gem requirements:
  #     gem "epsilla-ruby", "~> 0.0.3"
  #
  # Usage:
  #     epsilla = Langchain::Vectorsearch::Epsilla.new(url:, db_name:, db_path:, index_name:, llm:)
  #
  # Initialize Epsilla client
  #
  # @param url [String] URL to connect to the Epsilla db instance, protocol://host:port
  # @param db_name [String] The name of the database to use
  # @param db_path [String] The path to the database to use
  # @param index_name [String] The name of the Epsilla table to use
  # @param llm [Object] The LLM client to use
  # @return [Epsilla] a new instance of Epsilla
  #
  # source://langchainrb//lib/langchain/vectorsearch/epsilla.rb#25
  def initialize(url:, db_name:, db_path:, index_name:, llm:); end

  # Add a list of texts to the database
  #
  # @param texts [Array<String>] The list of texts to add
  # @param ids [Array<String>] The unique ids to add to the index, in the same order as the texts; if nil, it will be random uuids
  #
  # source://langchainrb//lib/langchain/vectorsearch/epsilla.rb#85
  def add_texts(texts:, ids: T.unsafe(nil)); end

  # Ask a question and return the answer
  #
  # @param question [String] The question to ask
  # @param k [Integer] The number of results to have in context
  # @return [String] The answer to the question
  # @yield [String] Stream responses back one String at a time
  #
  # source://langchainrb//lib/langchain/vectorsearch/epsilla.rb#132
  def ask(question:, k: T.unsafe(nil), &block); end

  # Create a table using the index_name passed in the constructor
  #
  # source://langchainrb//lib/langchain/vectorsearch/epsilla.rb#63
  def create_default_schema; end

  # Drop the table using the index_name passed in the constructor
  #
  # source://langchainrb//lib/langchain/vectorsearch/epsilla.rb#75
  def destroy_default_schema; end

  # Search for similar texts
  #
  # @param query [String] The text to search for
  # @param k [Integer] The number of results to return
  # @return [String] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/epsilla.rb#106
  def similarity_search(query:, k: T.unsafe(nil)); end

  # Search for entries by embedding
  #
  # @param embedding [Array<Float>] The embedding to search for
  # @param k [Integer] The number of results to return
  # @return [String] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/epsilla.rb#119
  def similarity_search_by_vector(embedding:, k: T.unsafe(nil)); end
end

# source://langchainrb//lib/langchain/vectorsearch/hnswlib.rb#4
class Langchain::Vectorsearch::Hnswlib < ::Langchain::Vectorsearch::Base
  # Initialize the HNSW vector search
  #
  # @param llm [Object] The LLM client to use
  # @param path_to_index [String] The local path to the index file, e.g.: "/storage/index.ann"
  # @return [Langchain::Vectorsearch::Hnswlib] Class instance
  #
  # source://langchainrb//lib/langchain/vectorsearch/hnswlib.rb#24
  def initialize(llm:, path_to_index:); end

  # Add a list of texts and corresponding IDs to the index
  #
  # @param texts [Array<String>] The list of texts to add
  # @param ids [Array<Integer>] The list of corresponding IDs (integers) to the texts
  # @return [Boolean] The response from the HNSW library
  #
  # source://langchainrb//lib/langchain/vectorsearch/hnswlib.rb#42
  def add_texts(texts:, ids:); end

  # Wrapper around HNSW (Hierarchical Navigable Small World) library.
  # HNSWLib is an in-memory vectorstore that can be saved to a file on disk.
  #
  # Gem requirements:
  #     gem "hnswlib", "~> 0.8.1"
  #
  # Usage:
  #     hnsw = Langchain::Vectorsearch::Hnswlib.new(llm:, path_to_index:)
  #
  # source://langchainrb//lib/langchain/vectorsearch/hnswlib.rb#15
  def client; end

  # Wrapper around HNSW (Hierarchical Navigable Small World) library.
  # HNSWLib is an in-memory vectorstore that can be saved to a file on disk.
  #
  # Gem requirements:
  #     gem "hnswlib", "~> 0.8.1"
  #
  # Usage:
  #     hnsw = Langchain::Vectorsearch::Hnswlib.new(llm:, path_to_index:)
  #
  # source://langchainrb//lib/langchain/vectorsearch/hnswlib.rb#15
  def path_to_index; end

  # Search for similar texts
  #
  # @param query [String] The text to search for
  # @param k [Integer] The number of results to return
  # @return [Array] Results in the format `[[id1, distance3], [id2, distance2]]`
  #
  # source://langchainrb//lib/langchain/vectorsearch/hnswlib.rb#63
  def similarity_search(query:, k: T.unsafe(nil)); end

  # Search for the K nearest neighbors of a given vector
  #
  # @param embedding [Array<Float>] The embedding to search for
  # @param k [Integer] The number of results to return
  # @return [Array] Results in the format `[[id1, distance3], [id2, distance2]]`
  #
  # source://langchainrb//lib/langchain/vectorsearch/hnswlib.rb#82
  def similarity_search_by_vector(embedding:, k: T.unsafe(nil)); end

  private

  # Loads or initializes the new index
  #
  # source://langchainrb//lib/langchain/vectorsearch/hnswlib.rb#113
  def initialize_index; end

  # Optionally resizes the index if there's no space for new data
  #
  # @param num_of_elements_to_add [Integer] The number of elements to add to the index
  #
  # source://langchainrb//lib/langchain/vectorsearch/hnswlib.rb#100
  def resize_index(num_of_elements_to_add); end
end

# source://langchainrb//lib/langchain/vectorsearch/milvus.rb#4
class Langchain::Vectorsearch::Milvus < ::Langchain::Vectorsearch::Base
  # Wrapper around Milvus REST APIs.
  #
  # Gem requirements:
  #     gem "milvus", "~> 0.9.2"
  #
  # Usage:
  # milvus = Langchain::Vectorsearch::Milvus.new(url:, index_name:, llm:, api_key:)
  #
  # @return [Milvus] a new instance of Milvus
  #
  # source://langchainrb//lib/langchain/vectorsearch/milvus.rb#15
  def initialize(url:, index_name:, llm:, api_key: T.unsafe(nil)); end

  # source://langchainrb//lib/langchain/vectorsearch/milvus.rb#24
  def add_texts(texts:); end

  # Ask a question and return the answer
  #
  # @param question [String] The question to ask
  # @param k [Integer] The number of results to have in context
  # @return [String] The answer to the question
  # @yield [String] Stream responses back one String at a time
  #
  # source://langchainrb//lib/langchain/vectorsearch/milvus.rb#144
  def ask(question:, k: T.unsafe(nil), &block); end

  # Create the default index
  #
  # @return [Boolean] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/milvus.rb#84
  def create_default_index; end

  # Create default schema
  #
  # @return [Hash] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/milvus.rb#46
  def create_default_schema; end

  # Delete default schema
  #
  # @return [Hash] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/milvus.rb#104
  def destroy_default_schema; end

  # Get the default schema
  #
  # @return [Hash] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/milvus.rb#98
  def get_default_schema; end

  # Load default schema into memory
  #
  # @return [Boolean] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/milvus.rb#110
  def load_default_schema; end

  # source://langchainrb//lib/langchain/vectorsearch/milvus.rb#114
  def similarity_search(query:, k: T.unsafe(nil)); end

  # source://langchainrb//lib/langchain/vectorsearch/milvus.rb#123
  def similarity_search_by_vector(embedding:, k: T.unsafe(nil)); end
end

# source://langchainrb//lib/langchain/vectorsearch/pgvector.rb#4
class Langchain::Vectorsearch::Pgvector < ::Langchain::Vectorsearch::Base
  # @param url [String] The URL of the PostgreSQL database
  # @param index_name [String] The name of the table to use for the index
  # @param llm [Object] The LLM client to use
  # @param namespace [String] The namespace to use for the index when inserting/querying
  # @return [Pgvector] a new instance of Pgvector
  #
  # source://langchainrb_rails/0.1.10/lib/langchainrb_overrides/vectorsearch/pgvector.rb#25
  def initialize(llm:); end

  # Add a list of texts to the index
  #
  # @param texts [Array<String>] The texts to add to the index
  # @param ids [Array<String>] The ids to add to the index, in the same order as the texts
  # @return [Array<Integer>] The the ids of the added texts.
  #
  # source://langchainrb_rails/0.1.10/lib/langchainrb_overrides/vectorsearch/pgvector.rb#40
  def add_texts(texts:, ids:); end

  # Ask a question and return the answer
  #
  # @param question [String] The question to ask
  # @param k [Integer] The number of results to have in context
  # @return [String] The answer to the question
  # @yield [String] Stream responses back one String at a time
  #
  # source://langchainrb_rails/0.1.10/lib/langchainrb_overrides/vectorsearch/pgvector.rb#100
  def ask(question:, k: T.unsafe(nil), &block); end

  # Create default schema
  #
  # source://langchainrb_rails/0.1.10/lib/langchainrb_overrides/vectorsearch/pgvector.rb#60
  def create_default_schema; end

  # Returns the value of attribute db.
  #
  # source://langchainrb//lib/langchain/vectorsearch/pgvector.rb#24
  def db; end

  # Destroy default schema
  #
  # source://langchainrb_rails/0.1.10/lib/langchainrb_overrides/vectorsearch/pgvector.rb#65
  def destroy_default_schema; end

  # source://langchainrb//lib/langchain/vectorsearch/pgvector.rb#45
  def documents_model; end

  # Returns the value of attribute documents_table.
  #
  # source://langchainrb//lib/langchain/vectorsearch/pgvector.rb#24
  def documents_table; end

  # source://langchainrb_rails/0.1.10/lib/langchainrb_overrides/vectorsearch/pgvector.rb#18
  def llm; end

  # source://langchainrb_rails/0.1.10/lib/langchainrb_overrides/vectorsearch/pgvector.rb#19
  def model; end

  # source://langchainrb_rails/0.1.10/lib/langchainrb_overrides/vectorsearch/pgvector.rb#19
  def model=(_arg0); end

  # Returns the value of attribute namespace.
  #
  # source://langchainrb//lib/langchain/vectorsearch/pgvector.rb#24
  def namespace; end

  # Returns the value of attribute namespace_column.
  #
  # source://langchainrb//lib/langchain/vectorsearch/pgvector.rb#24
  def namespace_column; end

  # Returns the value of attribute operator.
  #
  # source://langchainrb_rails/0.1.10/lib/langchainrb_overrides/vectorsearch/pgvector.rb#18
  def operator; end

  # Remove a list of texts from the index
  #
  # @param ids [Array<Integer>] The ids of the texts to remove from the index
  # @return [Integer] The number of texts removed from the index
  #
  # source://langchainrb//lib/langchain/vectorsearch/pgvector.rb#96
  def remove_texts(ids:); end

  # Search for similar texts in the index
  #
  # @param query [String] The text to search for
  # @param k [Integer] The number of top results to return
  # @return [Array<Hash>] The results of the search
  #
  # source://langchainrb_rails/0.1.10/lib/langchainrb_overrides/vectorsearch/pgvector.rb#74
  def similarity_search(query:, k: T.unsafe(nil)); end

  # Search for similar texts in the index by the passed in vector.
  # You must generate your own vector using the same LLM that generated the embeddings stored in the Vectorsearch DB.
  #
  # @param embedding [Array<Float>] The vector to search for
  # @param k [Integer] The number of top results to return
  # @return [Array<Hash>] The results of the search
  #
  # source://langchainrb_rails/0.1.10/lib/langchainrb_overrides/vectorsearch/pgvector.rb#89
  def similarity_search_by_vector(embedding:, k: T.unsafe(nil)); end

  # Returns the value of attribute table_name.
  #
  # source://langchainrb//lib/langchain/vectorsearch/pgvector.rb#24
  def table_name; end

  # Update a list of ids and corresponding texts to the index
  #
  # @param texts [Array<String>] The texts to add to the index
  # @param ids [Array<String>] The ids to add to the index, in the same order as the texts
  # @return [Array<Integer>] The ids of the updated texts.
  #
  # source://langchainrb_rails/0.1.10/lib/langchainrb_overrides/vectorsearch/pgvector.rb#54
  def update_texts(texts:, ids:); end

  # Upsert a list of texts to the index
  # the added or updated texts.
  #
  # @param texts [Array<String>] The texts to add to the index
  # @param ids [Array<Integer>] The ids of the objects to add to the index, in the same order as the texts
  # @return [PG::Result] The response from the database including the ids of
  #
  # source://langchainrb//lib/langchain/vectorsearch/pgvector.rb#56
  def upsert_texts(texts:, ids:); end
end

# source://langchainrb//lib/langchain/vectorsearch/pgvector.rb#22
Langchain::Vectorsearch::Pgvector::DEFAULT_OPERATOR = T.let(T.unsafe(nil), String)

# The operators supported by the PostgreSQL vector search adapter
#
# source://langchainrb//lib/langchain/vectorsearch/pgvector.rb#17
Langchain::Vectorsearch::Pgvector::OPERATORS = T.let(T.unsafe(nil), Hash)

# source://langchainrb//lib/langchain/vectorsearch/pinecone.rb#4
class Langchain::Vectorsearch::Pinecone < ::Langchain::Vectorsearch::Base
  # Initialize the Pinecone client
  #
  # @param environment [String] The environment to use
  # @param api_key [String] The API key to use
  # @param index_name [String] The name of the index to use
  # @param llm [Object] The LLM client to use
  # @return [Pinecone] a new instance of Pinecone
  #
  # source://langchainrb//lib/langchain/vectorsearch/pinecone.rb#20
  def initialize(environment:, api_key:, index_name:, llm:, base_uri: T.unsafe(nil)); end

  # @raise [ArgumentError]
  #
  # source://langchainrb//lib/langchain/vectorsearch/pinecone.rb#68
  def add_data(paths:, namespace: T.unsafe(nil), options: T.unsafe(nil), chunker: T.unsafe(nil)); end

  # Add a list of texts to the index
  #
  # @param texts [Array<String>] The list of texts to add
  # @param ids [Array<Integer>] The list of IDs to add
  # @param namespace [String] The namespace to add the texts to
  # @param metadata [Hash] The metadata to use for the texts
  # @return [Hash] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/pinecone.rb#54
  def add_texts(texts:, ids: T.unsafe(nil), namespace: T.unsafe(nil), metadata: T.unsafe(nil)); end

  # Ask a question and return the answer
  #
  # @param question [String] The question to ask
  # @param namespace [String] The namespace to search in
  # @param k [Integer] The number of results to have in context
  # @param filter [String] The filter to use
  # @return [String] The answer to the question
  # @yield [String] Stream responses back one String at a time
  #
  # source://langchainrb//lib/langchain/vectorsearch/pinecone.rb#174
  def ask(question:, namespace: T.unsafe(nil), filter: T.unsafe(nil), k: T.unsafe(nil), &block); end

  # Create the index with the default schema
  #
  # @return [Hash] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/pinecone.rb#103
  def create_default_schema; end

  # Delete the index
  #
  # @return [Hash] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/pinecone.rb#113
  def destroy_default_schema; end

  # Find records by ids
  #
  # @param ids [Array<Integer>] The ids to find
  # @param namespace String The namespace to search through
  # @raise [ArgumentError]
  # @return [Hash] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/pinecone.rb#39
  def find(ids: T.unsafe(nil), namespace: T.unsafe(nil)); end

  # Get the default schema
  #
  # @return [Pinecone::Vector] The default schema
  #
  # source://langchainrb//lib/langchain/vectorsearch/pinecone.rb#119
  def get_default_schema; end

  # Search for similar texts
  #
  # @param query [String] The text to search for
  # @param k [Integer] The number of results to return
  # @param namespace [String] The namespace to search in
  # @param filter [String] The filter to use
  # @return [Array] The list of results
  #
  # source://langchainrb//lib/langchain/vectorsearch/pinecone.rb#129
  def similarity_search(query:, k: T.unsafe(nil), namespace: T.unsafe(nil), filter: T.unsafe(nil)); end

  # Search for similar texts by embedding
  #
  # @param embedding [Array<Float>] The embedding to search for
  # @param k [Integer] The number of results to return
  # @param namespace [String] The namespace to search in
  # @param filter [String] The filter to use
  # @return [Array] The list of results
  #
  # source://langchainrb//lib/langchain/vectorsearch/pinecone.rb#151
  def similarity_search_by_vector(embedding:, k: T.unsafe(nil), namespace: T.unsafe(nil), filter: T.unsafe(nil)); end

  # Update a list of texts in the index
  #
  # @param texts [Array<String>] The list of texts to update
  # @param ids [Array<Integer>] The list of IDs to update
  # @param namespace [String] The namespace to update the texts in
  # @param metadata [Hash] The metadata to use for the texts
  # @return [Array] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/pinecone.rb#89
  def update_texts(texts:, ids:, namespace: T.unsafe(nil), metadata: T.unsafe(nil)); end

  private

  # Pinecone index
  #
  # @return [Object] The Pinecone index
  #
  # source://langchainrb//lib/langchain/vectorsearch/pinecone.rb#193
  def index; end
end

# source://langchainrb//lib/langchain/vectorsearch/qdrant.rb#4
class Langchain::Vectorsearch::Qdrant < ::Langchain::Vectorsearch::Base
  # Initialize the Qdrant client
  #
  # @param url [String] The URL of the Qdrant server
  # @param api_key [String] The API key to use
  # @param index_name [String] The name of the index to use
  # @param llm [Object] The LLM client to use
  # @return [Qdrant] a new instance of Qdrant
  #
  # source://langchainrb//lib/langchain/vectorsearch/qdrant.rb#20
  def initialize(url:, api_key:, index_name:, llm:); end

  # Add a list of texts to the index
  #
  # @param texts [Array<String>] The list of texts to add
  # @return [Hash] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/qdrant.rb#47
  def add_texts(texts:, ids: T.unsafe(nil), payload: T.unsafe(nil)); end

  # Ask a question and return the answer
  #
  # @param question [String] The question to ask
  # @param k [Integer] The number of results to have in context
  # @return [String] The answer to the question
  # @yield [String] Stream responses back one String at a time
  #
  # source://langchainrb//lib/langchain/vectorsearch/qdrant.rb#140
  def ask(question:, k: T.unsafe(nil), &block); end

  # Create the index with the default schema
  #
  # @return [Hash] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/qdrant.rb#91
  def create_default_schema; end

  # Deletes the default schema
  #
  # @return [Hash] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/qdrant.rb#85
  def destroy_default_schema; end

  # Find records by ids
  #
  # @param ids [Array<Integer>] The ids to find
  # @return [Hash] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/qdrant.rb#35
  def find(ids: T.unsafe(nil)); end

  # Get the default schema
  #
  # @return [Hash] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/qdrant.rb#79
  def get_default_schema; end

  # Remove a list of texts from the index
  #
  # @param ids [Array<Integer>] The ids to remove
  # @return [Hash] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/qdrant.rb#70
  def remove_texts(ids:); end

  # Search for similar texts
  #
  # @param query [String] The text to search for
  # @param k [Integer] The number of results to return
  # @return [Hash] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/qdrant.rb#105
  def similarity_search(query:, k: T.unsafe(nil)); end

  # Search for similar texts by embedding
  #
  # @param embedding [Array<Float>] The embedding to search for
  # @param k [Integer] The number of results to return
  # @return [Hash] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/qdrant.rb#121
  def similarity_search_by_vector(embedding:, k: T.unsafe(nil)); end

  # source://langchainrb//lib/langchain/vectorsearch/qdrant.rb#63
  def update_texts(texts:, ids:); end
end

# source://langchainrb//lib/langchain/vectorsearch/weaviate.rb#4
class Langchain::Vectorsearch::Weaviate < ::Langchain::Vectorsearch::Base
  # Initialize the Weaviate adapter
  #
  # @param url [String] The URL of the Weaviate instance
  # @param api_key [String] The API key to use
  # @param index_name [String] The capitalized name of the index to use
  # @param llm [Object] The LLM client to use
  # @return [Weaviate] a new instance of Weaviate
  #
  # source://langchainrb//lib/langchain/vectorsearch/weaviate.rb#20
  def initialize(url:, api_key:, index_name:, llm:); end

  # Add a list of texts to the index
  #
  # @param texts [Array<String>] The list of texts to add
  # @return [Hash] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/weaviate.rb#38
  def add_texts(texts:, ids: T.unsafe(nil)); end

  # Ask a question and return the answer
  #
  # @param question [String] The question to ask
  # @param k [Integer] The number of results to have in context
  # @return [Hash] The answer
  # @yield [String] Stream responses back one String at a time
  #
  # source://langchainrb//lib/langchain/vectorsearch/weaviate.rb#146
  def ask(question:, k: T.unsafe(nil), &block); end

  # Create default schema
  #
  # @return [Hash] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/weaviate.rb#92
  def create_default_schema; end

  # Delete the index
  #
  # @return [Boolean] Whether the index was deleted
  #
  # source://langchainrb//lib/langchain/vectorsearch/weaviate.rb#112
  def destroy_default_schema; end

  # Get default schema
  #
  # @return [Hash] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/weaviate.rb#106
  def get_default_schema; end

  # Deletes a list of texts in the index
  #
  # @param ids [Array] The ids of texts to delete
  # @raise [ArgumentError]
  # @return [Hash] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/weaviate.rb#77
  def remove_texts(ids:); end

  # Return documents similar to the query
  #
  # @param query [String] The query to search for
  # @param k [Integer|String] The number of results to return
  # @return [Hash] The search results
  #
  # source://langchainrb//lib/langchain/vectorsearch/weaviate.rb#120
  def similarity_search(query:, k: T.unsafe(nil)); end

  # Return documents similar to the vector
  #
  # @param embedding [Array<Float>] The vector to search for
  # @param k [Integer|String] The number of results to return
  # @return [Hash] The search results
  #
  # source://langchainrb//lib/langchain/vectorsearch/weaviate.rb#130
  def similarity_search_by_vector(embedding:, k: T.unsafe(nil)); end

  # Update a list of texts in the index
  #
  # @param texts [Array<String>] The list of texts to update
  # @return [Hash] The response from the server
  #
  # source://langchainrb//lib/langchain/vectorsearch/weaviate.rb#47
  def update_texts(texts:, ids:); end

  private

  # source://langchainrb//lib/langchain/vectorsearch/weaviate.rb#171
  def weaviate_object(text, id = T.unsafe(nil)); end

  # source://langchainrb//lib/langchain/vectorsearch/weaviate.rb#165
  def weaviate_objects(texts, ids = T.unsafe(nil)); end
end
